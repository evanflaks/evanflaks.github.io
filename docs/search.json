[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Evan Flaks and this is my Machine Learning blog."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/penguins/penguins.html",
    "href": "posts/penguins/penguins.html",
    "title": "Palmer Penguins Classification",
    "section": "",
    "text": "Abstract\nThis goal of this project was to develop a machine learning model to classify penguin species based on quantitative and qualititave characteristics using the Palmer Penguins dataset. After accessing the data and splitting it up into training and testing sets, I created visualizations on the training data to better understand which species exhibited particular traits. Then, I used a feature selection system to determine which features (two quantitative and one qualitiative) would be the best predictor of a penguin’s species. From there, I used these features to train a logistic regression model. The model’s performance is assessed using training and test accuracy and decision region visualizations. The goal of this assignment was to take a penguin with unknown species, analyze three of its given features, and with 100% accuracy, predict its species.\n\n\nData Preparation\nFirst we must access and read the data from the source.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nNow, I must prepare the dataset for machine learning by encoding the target variable (Species) into numerical values, dropping unnecessary columns, removing missing values and invalid entries, converting categorical columns into numerical representations via one-hot encoding, and splitting up training and testing sets\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\n# Initialize LabelEncoder for the target variable\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n# Function to preprocess data\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]  # Remove rows with invalid 'Sex' values\n    df = df.dropna()  # Remove any remaining missing values\n    y = le.transform(df[\"Species\"])  # Encode the target variable\n    df = df.drop([\"Species\"], axis=1)  # Remove the target column from features\n    df = pd.get_dummies(df)  # One-hot encode categorical features\n    return df, y\n\n# Prepare the dataset\nX, y = prepare_data(train)\n\n# Split the dataset into 80% training and 20% testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Convert training data back into a DataFrame for visualization purposes\ntrain_data = X_train.copy()\ntrain_data[\"Species\"] = le.inverse_transform(y_train)  # Convert encoded labels back to species names\n\n# Reconstruct the 'Island' column from one-hot encoded values\nisland_columns = ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\ntrain_data[\"Island\"] = train_data[island_columns].idxmax(axis=1).str.replace(\"Island_\", \"\")\n\n# Check the sizes of the splits\nprint(f\"Training Set Size: {X_train.shape[0]}\")\nprint(f\"Test Set Size: {X_test.shape[0]}\")\n\nTraining Set Size: 204\nTest Set Size: 52\n\n\n\n\nVisualizations\nNow, to visualize the dataset, I have created a scatter plot that plots flipper length vs. body mass of the three species and a bar chart showing the distribution of Penguins found on the three different islands. Finally, I created a summary table that shows each species’ average Culmen Length and Culmen Depth\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=train_data, x='Flipper Length (mm)', y='Body Mass (g)', hue='Species', style='Species')\n\n# Labels and title\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Body Mass (g)')\nplt.title('Flipper Length vs. Body Mass by Species (Training Data Only)')\nplt.legend(title='Species')\nplt.show()\n\n\n\n\n\n\n\n\nHere, we can see that the Adelie and Chinstrap penguins have pretty similar flipper lenght and body mass while the Gentoo penguins are much larger in both of those metrics.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create a count plot for species distribution by island (training data only)\nplt.figure(figsize=(8, 6))\nsns.countplot(data=train_data, x='Island', hue='Species')\n\n# Labels and title\nplt.xlabel('Island')\nplt.ylabel('Count')\nplt.title('Penguin Species Distribution by Island (Training Data Only)')\nplt.legend(title='Species')\nplt.show()\n\n\n\n\n\n\n\n\nHere, we can see that Gentoo penguins are only found on Biscoe Island and Chinstrap penguins are only found on Dream Island. Adelie penguins are found on all three islands.\n\nimport pandas as pd\n\n# Compute the average Culmen Length & Depth in the training set only\nsummary_table = train_data.groupby(\"Species\")[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].mean()\n\n# Display the summary table\nprint(summary_table)\n\n\n                                           Culmen Length (mm)  \\\nSpecies                                                         \nAdelie Penguin (Pygoscelis adeliae)                 39.048837   \nChinstrap penguin (Pygoscelis antarctica)           48.697778   \nGentoo penguin (Pygoscelis papua)                   47.031507   \n\n                                           Culmen Depth (mm)  \nSpecies                                                       \nAdelie Penguin (Pygoscelis adeliae)                18.412791  \nChinstrap penguin (Pygoscelis antarctica)          18.406667  \nGentoo penguin (Pygoscelis papua)                  14.957534  \n\n\nFrom this table we can see that Adelie penguins typically have a smaller culmen length than Gentoo and Chinstrap penguins and Gentoo penguins typically have a smaller culmen depth than the other two.\n\n\nFeature Selection\nNow we must choose which three features we want to use to predict the penguin species. To do this, I used a Random Forest Classifier by measuring how much each feature contributes to making accurate predictions. When splitting a node in a decision tree, features that provide better separation between classes are preferred. The model tracks how often a feature is used in important splits and how much it improves classification accuracy. The importance of each feature is calculated as the total reduction in prediction error (impurity) it provides across all trees. Features with higher scores contribute more to the model’s decision-making.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train a random forest to assess feature importance\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Get feature importance scores\nfeature_importance = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': model.feature_importances_\n})\n\n# Sort by importance\nfeature_importance = feature_importance.sort_values(by='Importance', ascending=False)\nprint(feature_importance)\n\n                     Feature  Importance\n0         Culmen Length (mm)    0.234636\n2        Flipper Length (mm)    0.179364\n5          Delta 13 C (o/oo)    0.161621\n1          Culmen Depth (mm)    0.148071\n4          Delta 15 N (o/oo)    0.075136\n7               Island_Dream    0.063007\n6              Island_Biscoe    0.059049\n3              Body Mass (g)    0.058736\n8           Island_Torgersen    0.012969\n10      Clutch Completion_No    0.002622\n12                Sex_FEMALE    0.002250\n13                  Sex_MALE    0.001822\n11     Clutch Completion_Yes    0.000717\n9   Stage_Adult, 1 Egg Stage    0.000000\n\n\nFrom this, we can conclude that the best qualitative category is Island and the best quantitative categories are Culmen Length and Flipper Length. So, these are the features we will train our model on.\n\n\nLogistic Regression Model Testing\nThe model below uses logistic regression to classify data based on flipper length, culmen length, and island. First, it standardizes the numerical features using StandardScaler() to ensure consistent scaling. Then, it trains a logistic regression model with increased iterations (1000) to ensure convergence. Finally, it evaluates performance using training and test accuracy, helping assess how well the model generalizes.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# Define selected features\ncols = ['Flipper Length (mm)', 'Culmen Length (mm)','Island_Biscoe', 'Island_Dream', 'Island_Torgersen',]\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[cols]), columns=cols)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test[cols]), columns=cols)\n\n\n# Initialize and train logistic regression with increased iterations\nLR = LogisticRegression(max_iter=1000)  # Increased max_iter to ensure convergence\nLR.fit(X_train_scaled, y_train)\n\n# Evaluate model\nprint(\"Training Accuracy:\", LR.score(X_train_scaled, y_train))\nprint(\"Test Accuracy:\", LR.score(X_test_scaled, y_test))\n\n\n\nTraining Accuracy: 0.9754901960784313\nTest Accuracy: 1.0\n\n\n\n\nPlotting Decision Regions\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, scaler):\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(7, 3))\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n\n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({X.columns[0]: XX, X.columns[1]: YY})\n\n        # Initialize categorical features as zeros\n        for j in qual_features:\n            XY[j] = 0\n\n        # Set the specific category feature to 1\n        XY[qual_features[i]] = 1\n\n        # Standardize XY to match model input\n        XY_scaled = pd.DataFrame(scaler.transform(XY), columns=X.columns)\n\n        # Predict decision boundary\n        p = model.predict(XY_scaled)\n        p = p.reshape(xx.shape)\n\n        # Use contour plot to visualize the predictions\n        decision_cmap = ListedColormap([\"blue\", \"green\", \"orange\"])  # Match species colors\n        axarr[i].contourf(xx, yy, p, cmap=decision_cmap, alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n\n        # Plot the actual training data points\n        species_cmap = ListedColormap([\"blue\", \"green\", \"orange\"])  # blue = Adelie, Green = Chinstrap, orange = Gentoo\n        axarr[i].scatter(x0[ix], x1[ix], c=y[ix], cmap=species_cmap, vmin=0, vmax=2)\n\n        axarr[i].set(\n            xlabel=X.columns[0],\n            ylabel=X.columns[1],\n            title=qual_features[i]\n        )\n\n        patches = []\n        for color, spec in zip([\"blue\", \"green\", \"orange\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color=color, label=spec))\n\n        axarr[i].legend(handles=patches, title=\"Species\", loc=\"best\")\n\n    plt.tight_layout()\n\n# Call the function with scaler applied\nplot_regions(LR, X_train[cols], y_train, scaler)\nplot_regions(LR, X_test[cols], y_test, scaler)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test_scaled)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[22,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 19]])\n\n\n\n\nDiscussion\nThroughout this project, I gained valuable insights into the entire machine learning process. First, it was essential to prepare the data into training and test sections, encode qualitative regions into quantitave values, drop unessecary columns, and simplify column names. Then, the data visualizations helped me uncover trends in the dataset. This allowed me to get a foundational understanding of the variance between the species’ characteristics. A key takeaway was the importance of feature selection. I initially just analyzed my data visualizations and chose features I thought would be strong predictors. Even though this was somewhat accurate, when I automated my feature selection with a RandomForestClassifier and chose the most statistically important feautures, I was able to bring my model’s prediction success up to 100% for the test set. I also gained experience with model selection and scaling. After trying out a few options, I chose the Logistic Regression model because it was consistently high performing. During this process, I also realized that I had to standardize the numerical features to ensure consistent scaling. Perhaps the most interesting discovery of the entire process was plotting the decision regions to see where my model numerically distinguished between the species’ and to see the entire regions where a particular species would fall."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Palmer Penguins Classification\n\n\n\n\n\nMachine Learning model to predict penguin species\n\n\n\n\n\nFeb 18, 2024\n\n\nEvan Flaks\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]