[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Evan Flaks and this is my Machine Learning blog."
  },
  {
    "objectID": "posts/bias/bias.html",
    "href": "posts/bias/bias.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\nFor this project, I will be creating a machine learning algorithm to predict the employment status of Maryland citizens on the basis of non-racial demographics, and then audit for racial bias.\n\n\nDownloading Data\nFirst we will download some PUMS data from the state of Maryland using folktables.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MD\"\n\ndata_source = ACSDataSource(survey_year='2023', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nSTATE\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2023GQ0000068\n5\n1\n201\n3\n24\n1019518\n27\n62\n...\n27\n29\n24\n25\n29\n27\n25\n29\n28\n28\n\n\n1\nP\n2023GQ0000079\n5\n1\n502\n3\n24\n1019518\n13\n21\n...\n13\n26\n17\n15\n13\n32\n0\n2\n2\n13\n\n\n2\nP\n2023GQ0000088\n5\n1\n1400\n3\n24\n1019518\n25\n35\n...\n32\n25\n70\n54\n42\n24\n2\n34\n49\n23\n\n\n3\nP\n2023GQ0000093\n5\n1\n1300\n3\n24\n1019518\n19\n61\n...\n14\n19\n23\n27\n2\n22\n27\n3\n20\n2\n\n\n4\nP\n2023GQ0000100\n5\n1\n802\n3\n24\n1019518\n106\n73\n...\n136\n206\n9\n15\n106\n209\n115\n210\n211\n15\n\n\n\n\n5 rows × 287 columns\n\n\n\n\n\nData Cleaning\nThe dataset has a lot of features. For our modeling task, we will only use the following possible features\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n62\n17.0\n5\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n1\n1.0\n1\n1\n6.0\n\n\n1\n21\n19.0\n5\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n2\n9\n6.0\n\n\n2\n35\n16.0\n5\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n1\n2\n6.0\n\n\n3\n61\n18.0\n3\n1\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n1.0\n1\n1\n6.0\n\n\n4\n73\n13.0\n5\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n\n\n\n\n\nFor documentation on what these features mean, please consult the appendix of this paper that introduced the package.\nThe feautures that I am going to train my model on are educational attainment (SCHL), employment status of parents (ESP), mobility status (MIG), and age (AGEP). I will use these features to predict employment status (ESR).\n\nfeatures_to_use = [\"SCHL\",\"ESP\",\"MIG\",\"AGEP\"]\n\nNow we can construct a BasicProblem that expresses our wish to use these feautures to predict employment status (ESR), using race (RAC1P) as the group label.\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nBefore we touch the data anymore, we should perform a train-test split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nBasic Descriptives\nNow we want to answer some basic questions about the data we are working with. We can answer these questions by turning our training data into a data from for easy analysis.\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\nHow many individuals are in the data?\n\n\nnum_rows = df.shape[0]\nprint(f\"Number of people in the data: {num_rows}\")\n\nNumber of people in the data: 49624\n\n\n\nOf these individuals, what proportion are employed?\nOf these individuals, how many are in each racial group?\n\n\nrace_counts = df['RAC1P'].value_counts()\nprint(race_counts)\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile /opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/pandas/core/indexes/base.py:3361, in Index.get_loc(self, key, method, tolerance)\n   3360 try:\n-&gt; 3361     return self._engine.get_loc(casted_key)\n   3362 except KeyError as err:\n\nFile /opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/pandas/_libs/index.pyx:76, in pandas._libs.index.IndexEngine.get_loc()\n\nFile /opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/pandas/_libs/index.pyx:108, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'RAC1P'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[43], line 1\n----&gt; 1 race_counts = df['RAC1P'].value_counts()\n      2 print(race_counts)\n\nFile /opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/pandas/core/frame.py:3458, in DataFrame.__getitem__(self, key)\n   3456 if self.columns.nlevels &gt; 1:\n   3457     return self._getitem_multilevel(key)\n-&gt; 3458 indexer = self.columns.get_loc(key)\n   3459 if is_integer(indexer):\n   3460     indexer = [indexer]\n\nFile /opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/pandas/core/indexes/base.py:3363, in Index.get_loc(self, key, method, tolerance)\n   3361         return self._engine.get_loc(casted_key)\n   3362     except KeyError as err:\n-&gt; 3363         raise KeyError(key) from err\n   3365 if is_scalar(key) and isna(key) and not self.hasnans:\n   3366     raise KeyError(key)\n\nKeyError: 'RAC1P'\n\n\n\n\nIn each group, what proportion of individuals have target label equal to 1?\nNow I will look for some intersectional trends."
  },
  {
    "objectID": "posts/loans/credit-risk.html",
    "href": "posts/loans/credit-risk.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Abstract\nIn this project, I developed a machine learning-based loan approval system designed to predict the likelihood of loan default and optimize lending decisions for a financial institution. Using a dataset containing borrower information such as age, income, employment length, homeownership status, and credit history, I first created data visualizations and summary tables to explore trends in the data. Then, I implemented a logistic regression model to classify applicants as either high-risk (default) or low-risk (non-default) and create a score function with a weight vector to weight each feature into the decision process. Then, to decide on a threshold value for my score function, I conducted a profit optimization analysis, adjusting approval thresholds to maximize expected returns. Through this study, I evaluated how loan approval rates varied across age groups, income levels, and loan purposes, ultimately assessing the fairness and efficiency of an automated credit decision system.\n\n\nData Extraction\nWe begin by downloading the data from the source. The columns in this data include extensive information about the prospective borrower – age, income, home owndership, loan intent, etc.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nData Exploration\nNow I will create some visualizations to explore patterns in the data.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='loan_intent', y='person_age', hue='person_home_ownership', data=df_train)\nplt.title(\"Age Distribution by Loan Intent and Homeownership Status\")\nplt.xlabel(\"Loan Intent\")\nplt.ylabel(\"Age\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Homeownership\")\n\n\n\n\n\n\n\n\nThis boxplot helps visualize the distribution of borrowers’ ages across different loan intents, while also distinguishing between different homeownership statuses. An immediate observation that I have is that the interquartile range for every loan type and homeownership status falls around 20-30 years of age, meaning that is the main age group seeking all types of loans. Another notable observation is the outliers per category. There are a lot more y-axis outliers (i.e. older people) that rent their homes, meaning it is common for older renters to be seeking a loan in almost all loan categories as opposed to older homeowners or mortgagers. Further, the largest age outlier category falls in the blue medical section of the x-axis, meaning that older-age renters are often seeking medical loans.\n\nplt.figure(figsize=(12, 6))\nsns.scatterplot(x='person_income', y='loan_int_rate', hue='loan_status', data=df_train, alpha=0.7)\nplt.xscale(\"log\")  # Log scale to handle wide income range\nplt.title(\"Interest Rate vs. Income by Loan Status\")\nplt.xlabel(\"Annual Income (Log Scale)\")\nplt.ylabel(\"Interest Rate (%)\")\nplt.legend(title=\"Loan Defaulted\")\nplt.show()\n\n\n\n\n\n\n\n\nThis scatter plot helps us visualize the relationship between a person’s income and the interest rate they are offered, and whether or not that loan was defaulted. As we can see by the scattering, there is no clear relationship between income and interest rate; there are both high income people with low and high interest rates, and there are low income people with both high and low interest rates. However, the abundance of orange to the top and left of the plot tells us that those with lower annual incomes tend to default on their loans much more often as opposed to higher-income individuals.\n\nsummary_table = df_train.groupby('person_emp_length').agg({\n    'loan_amnt': 'mean',\n    'loan_int_rate': 'mean',\n    'loan_percent_income': 'mean'\n}).reset_index()\n\n# Rename columns for clarity\nsummary_table.columns = ['Employment Length (Years)', 'Avg Loan Amount', 'Avg Interest Rate (%)', 'Avg Loan as % of Income']\n\n# Display the summary table\ndisplay(summary_table)\n\n\n\n\n\n\n\n\nEmployment Length (Years)\nAvg Loan Amount\nAvg Interest Rate (%)\nAvg Loan as % of Income\n\n\n\n\n0\n0.0\n8572.957492\n11.212005\n0.174275\n\n\n1\n1.0\n9196.537307\n11.356443\n0.178396\n\n\n2\n2.0\n9142.199217\n11.302131\n0.174799\n\n\n3\n3.0\n9448.739572\n11.095799\n0.170120\n\n\n4\n4.0\n9392.330416\n11.215608\n0.169326\n\n\n5\n5.0\n9596.843434\n10.857377\n0.171835\n\n\n6\n6.0\n9735.197368\n10.775047\n0.172580\n\n\n7\n7.0\n10095.776060\n10.930345\n0.163076\n\n\n8\n8.0\n10439.578714\n10.577886\n0.162084\n\n\n9\n9.0\n10799.319419\n10.904209\n0.160971\n\n\n10\n10.0\n10989.527629\n10.994492\n0.163850\n\n\n11\n11.0\n10832.043478\n10.936378\n0.146348\n\n\n12\n12.0\n10988.591703\n10.562252\n0.155087\n\n\n13\n13.0\n10824.500000\n10.692848\n0.161286\n\n\n14\n14.0\n11635.018727\n10.776500\n0.155393\n\n\n15\n15.0\n10270.750000\n10.474341\n0.156400\n\n\n16\n16.0\n11522.674419\n9.895086\n0.157364\n\n\n17\n17.0\n9693.181818\n10.468352\n0.140101\n\n\n18\n18.0\n10328.481013\n10.228333\n0.136329\n\n\n19\n19.0\n13135.795455\n11.061190\n0.153864\n\n\n20\n20.0\n12926.973684\n11.654412\n0.175526\n\n\n21\n21.0\n10059.166667\n12.068889\n0.144333\n\n\n22\n22.0\n10470.000000\n12.206429\n0.144000\n\n\n23\n23.0\n9700.000000\n11.513333\n0.188333\n\n\n24\n24.0\n10045.000000\n9.986250\n0.140000\n\n\n25\n25.0\n7625.000000\n10.353333\n0.111667\n\n\n26\n26.0\n12000.000000\n8.540000\n0.130000\n\n\n27\n27.0\n13200.000000\n9.938000\n0.186000\n\n\n28\n28.0\n17666.666667\n15.330000\n0.220000\n\n\n29\n29.0\n25000.000000\n13.430000\n0.340000\n\n\n30\n30.0\n24000.000000\n10.380000\n0.155000\n\n\n31\n31.0\n13500.000000\n10.450000\n0.075000\n\n\n32\n34.0\n7500.000000\n13.550000\n0.150000\n\n\n33\n38.0\n20000.000000\n9.880000\n0.190000\n\n\n34\n41.0\n3000.000000\n7.510000\n0.060000\n\n\n35\n123.0\n27500.000000\n11.280000\n0.345000\n\n\n\n\n\n\n\nFrom this summary table, we can see that typically those who have been employed longer get access to larger lines of credit. While the correlation is not super strong, it can be said that those who have been employed a short time (0-6) years are much more likely to get a loan that is worth less than $10,000.\n\n\nBuilding a Model to Find a Weight Vector\nBelow, I have trained a logistic regression model to predict whether a borrower will default on a loan based on all of the given features in the data set aside from loan grade and loan status (the target variable). I first separate the features into numeric (e.g., income, loan amount) and categorical (e.g., homeownership, loan intent). The numeric features are standardized using StandardScaler, while categorical features are one-hot encoded using OneHotEncoder, all managed through a ColumnTransformer. A Pipeline is then created to preprocess the data and train a logistic regression model. Finally, the weight vector (model coefficients) is retrieved, and cross-validation is performed to assess the model’s accuracy across five folds. The accuracy scores and mean accuracy are printed to evaluate the model’s predictive performance.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import cross_val_score\n\n\n\n# List of features (excluding forbidden ones)\nfeatures = [\n    \"person_age\", \"person_income\", \"person_home_ownership\", \"person_emp_length\",\n    \"loan_intent\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\",\n    \"cb_person_default_on_file\", \"cb_person_cred_hist_length\"\n]\n\n# Target variable\ntarget = \"loan_status\"\n\n# Example placeholder for DataFrame\n\n# Splitting numeric and categorical features\nnumeric_features = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\ncategorical_features = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\n\n# Preprocessing\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ]\n)\ndf = df_train.dropna()\n# Define logistic regression model\nmodel = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n])\n\n# Prepare data\nX = df[features]\ny = df[target]\n\n# Ensure target variable is binary (1 = Default, 0 = Non-Default)\ny = y.map({'Default': 1, 'Non-Default': 0}) if y.dtype == 'object' else y\n\n# Fit the model\nmodel.fit(X, y)\n\n# Retrieve the weight vector\nweights = model.named_steps['classifier'].coef_\nprint(\"Weight vector (w):\", weights)\n\n# Cross-validation to evaluate model accuracy\ncv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\nprint(\"Cross-validation accuracy scores:\", cv_scores)\nprint(\"Mean accuracy:\", np.mean(cv_scores))\n\nWeight vector (w): [[-0.0383707   0.04510115 -0.02445074 -0.58076891  1.03715014  1.3244998\n  -0.01195797  0.12828473  0.37763254 -1.36247275  0.86221669  0.42979327\n  -0.37518745  0.50616243  0.24713985 -0.19722704 -0.60501984 -0.0351749\n   0.04083611]]\nCross-validation accuracy scores: [0.85268442 0.85421213 0.84566688 0.84654006 0.84981445]\nMean accuracy: 0.8497835888866307\n\n\nFrom this we can see that our weight vector for all of the features is w = [-0.0383707 0.04510115 -0.02445074 -0.58076891 1.03715014 1.3244998 -0.01195797 0.12828473 0.37763254 -1.36247275 0.86221669 0.42979327 -0.37518745 0.50616243 0.24713985 -0.19722704 -0.60501984 -0.0351749 0.04083611]\nAnd our mean accuracy across our cross-validations is 0.849, meaning we predicted around 85% of loan approval decisions to be correct (did not default).\n\n\nFinding a Threshold\nNow we must find a threshold. To do this, the code below first generates predicted probabilities of loan default from the logistic regression model and defines a range of threshold values between 0 and 1. The profit if a loan is repaid and the loss if a loan defaults are computed using the following assumptions:\n\nIf ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loan ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍repaid ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍full, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍profit ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bank ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍equal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loan_amnt(1 + 0.25loan_int_rate)**10 - loan_amnt. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍This ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍formula ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍assumes ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍profit ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍earned ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bank ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍on ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍10-year ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loan ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍equal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍25% ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍interest ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍rate ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍each ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍year, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍with ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍other ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍75% ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍interest ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍going ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍things ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍like ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍salaries ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍people ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍who ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍manage ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bank. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍It ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍extremely ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍simplistic ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍does ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍not ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍account ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍inflation, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍amortization ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍over ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍time, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍opportunity ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍costs, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍etc.\nIf ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍borrower ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍defaults ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍on ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loan, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍“profit” ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bank ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍equal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loan_amnt(1 + 0.25loan_int_rate)**3 - 1.7*loan_amnt. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍This ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍formula ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍corresponds ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍same ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍profit-earning ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍mechanism ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍as ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍above, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍but ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍assumes ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍borrower ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍defaults ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍three ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍years ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍into ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loan ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bank ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loses ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍70% ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍principal.\n\nFor each threshold, the model classifies loan applicants, determining whether they are approved or denied. The total expected profit is then computed by summing profits from repaid loans and losses from defaulted loans. The optimal threshold is selected as the one that yields the highest total profit.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate predicted probabilities from the logistic regression model\ny_prob = model.predict_proba(X)[:, 1]\n\n# Define a range of threshold values to test\nthresholds = np.linspace(0, 1, 100)\n\n# Extract loan amounts and interest rates as numpy arrays\nloan_amounts = X[\"loan_amnt\"].to_numpy()\ninterest_rates = X[\"loan_int_rate\"].to_numpy()\n\n# Ensure interest rates are correctly scaled (divided by 100 if necessary)\nif interest_rates.max() &gt; 1:  # If the max value is greater than 1, assume percentages need scaling\n    interest_rates /= 100\n\n# Compute profit if loan is repaid and loss if defaulted (vectorized)\nprofit_if_repaid = loan_amounts * ((1 + 0.25 * interest_rates) ** 10) - loan_amounts\nloss_if_defaulted = loan_amounts * ((1 + 0.25 * interest_rates) ** 3) - (1.7 * loan_amounts)\n\n# Ensure y is a NumPy array\ny_np = y.to_numpy()\n\n# Vectorized computation of profit for each threshold\nprofits = np.zeros_like(thresholds)\n\nfor i, t in enumerate(thresholds):\n    predictions = (y_prob &gt;= t).astype(int)  # Convert probabilities to binary predictions\n    approved_loans = predictions == 0  # Loans that are approved\n\n    # Compute total profit\n    total_profit = np.sum(profit_if_repaid[(y_np == 0) & approved_loans]) + np.sum(loss_if_defaulted[(y_np == 1) & approved_loans])\n    profits[i] = total_profit\n\n# Find the threshold that maximizes profit\noptimal_threshold = thresholds[np.argmax(profits)]\nmax_profit = profits.max()\nexpected_profit_per_borrower = max_profit / len(y_prob)\n\n# Plot the profit curve\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, profits, label=\"Total Profit\", color='blue')\nplt.axvline(optimal_threshold, color='red', linestyle=\"--\", label=f\"Optimal Threshold: {optimal_threshold:.4f}\")\nplt.xlabel(\"Decision Threshold\")\nplt.ylabel(\"Total Profit\")\nplt.title(\"Profit Optimization for Loan Approval\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Output the results\nprint(f\"Optimal Threshold: {optimal_threshold:.4f}\")\nprint(f\"Maximum Profit: {max_profit:.2f}\")\nprint(f\"Expected Profit per Borrower: {expected_profit_per_borrower:.2f}\")\n\n\n\n\n\n\n\n\nOptimal Threshold: 0.4040\nMaximum Profit: 32086998.37\nExpected Profit per Borrower: 1400.75\n\n\nAbove, I have plotted a profit curve to visualize how profit changes with different decision thresholds, with the optimal threshold marked in red. As we can see, the threshold that optimizes profits is t = 0.4040, generating $1400.75 in profit per borrower.\n\n\nModel Evaluation: Bank’s Perspective\nNow that I have finalized our weight vector w and threshold t, I am going to evaluate the performance of the logistic regression model on the test dataset. The code below loads and cleans the test data by dropping missing values, extracts relevant features, and the target variable (loan_status) is mapped to binary values.\nUnder the financial formula assumptions provided above, the optimal threshold of t = 0.4040 is applied to classify loans as approved (0) or denied (1). The total profit is computed by summing profits from repaid loans and losses from defaulted loans within the approved subset. Finally, the expected profit per borrower is derived by dividing total profit by the number of test borrowers.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test_dirty = pd.read_csv(url)\ndf_test = df_test_dirty.dropna()\n\nimport pandas as pd\nimport numpy as np\n\n# Extract relevant features from the test set (excluding forbidden ones)\nX_test = df_test[[\"person_age\", \"person_income\", \"person_home_ownership\", \"person_emp_length\",\n                  \"loan_intent\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\",\n                  \"cb_person_default_on_file\", \"cb_person_cred_hist_length\"]]\n\n# Extract true labels\ny_test = df_test[\"loan_status\"]\n\n# Ensure target variable is binary (1 = Default, 0 = Non-Default)\ny_test = y_test.map({'Default': 1, 'Non-Default': 0}) if y_test.dtype == 'object' else y_test\n\n# Get predicted probabilities for the positive class (default) using trained model\ny_prob_test = model.predict_proba(X_test)[:, 1]\n\n# Extract loan amounts and interest rates as numpy arrays\nloan_amounts_test = X_test[\"loan_amnt\"].to_numpy()\ninterest_rates_test = X_test[\"loan_int_rate\"].to_numpy()\n\n# Ensure interest rates are correctly scaled (divided by 100 if necessary)\nif interest_rates_test.max() &gt; 1:  # If the max value is greater than 1, assume percentages need scaling\n    interest_rates_test /= 100\n\n# Compute profit if loan is repaid and loss if defaulted (vectorized)\nprofit_if_repaid_test = loan_amounts_test * ((1 + 0.25 * interest_rates_test) ** 10) - loan_amounts_test\nloss_if_defaulted_test = loan_amounts_test * ((1 + 0.25 * interest_rates_test) ** 3) - (1.7 * loan_amounts_test)\n\n# Ensure y_test is a NumPy array\ny_test_np = y_test.to_numpy()\n\n# Apply the chosen threshold to make loan approval decisions\npredictions_test = (y_prob_test &gt;= optimal_threshold).astype(int)\napproved_loans_test = predictions_test == 0  # Loans that are approved\n\n# Compute total profit on the test set\ntotal_profit_test = np.sum(profit_if_repaid_test[(y_test_np == 0) & approved_loans_test]) + \\\n                    np.sum(loss_if_defaulted_test[(y_test_np == 1) & approved_loans_test])\n\n# Compute expected profit per borrower\nexpected_profit_per_borrower_test = total_profit_test / len(y_test_np)\n\nprint(f\"Total Profit on Test Set: {total_profit_test:.2f}\")\nprint(f\"Expected Profit per Borrower on Test Set: {expected_profit_per_borrower_test:.2f}\")\n\nTotal Profit on Test Set: 7684515.98\nExpected Profit per Borrower on Test Set: 1340.87\n\n\nAs we can see, the expected profit per borrower on the test set is $1340.87, which is very similar to the value of $1400.75 that was calculated on the training set. From a Bank’s perspective, our model is reasonably profitable and accurate.\n\n\nModel Evaluation: Borrower’s Perspective\nNow we will evaluate our model from the borrower’s perspective. This code analyzes the loan approval rates under the model’s decision system across different demographic and financial groups. The analysis is divided into three key perspectives:\nAge Groups: Borrowers are categorized into age brackets, and approval rates are computed for each group.\nLoan Type (Medical vs. Other): Loans are grouped into Medical and Other categories, and their respective approval and default rates are calculated.\nIncome Levels: Borrowers are divided into five quintile income brackets, and approval rates for each group are determined.\n\nimport pandas as pd\nimport numpy as np\n\n# Create a copy to avoid modifying the original DataFrame\ndf_test = df_test.copy()\n\n# Assign approval results safely\ndf_test.loc[:, \"approved\"] = approved_loans_test\n\n# Analyze approval rates across age groups\nage_groups = pd.cut(df_test[\"person_age\"], bins=[18, 25, 35, 50, 65, 100], labels=[\"18-24\", \"25-34\", \"35-49\", \"50-64\", \"65+\"])\napproval_rate_by_age = df_test.groupby(age_groups)[\"approved\"].mean()\n\n# Create a new column to categorize loans as 'Medical' or 'Other'\ndf_test.loc[:, \"loan_category\"] = df_test[\"loan_intent\"].apply(lambda x: \"Medical\" if x == \"MEDICAL\" else \"Other\")\n\n# Compute approval rates\napproval_rate_medical = df_test[df_test[\"loan_category\"] == \"Medical\"][\"approved\"].mean()\napproval_rate_other = df_test[df_test[\"loan_category\"] == \"Other\"][\"approved\"].mean()\n\n# Compute default rates\ndefault_rate_medical = df_test[df_test[\"loan_category\"] == \"Medical\"][\"loan_status\"].mean()\ndefault_rate_other = df_test[df_test[\"loan_category\"] == \"Other\"][\"loan_status\"].mean()\n\n# Analyze impact of income on loan approvals\nincome_groups = pd.qcut(df_test[\"person_income\"], q=5, labels=[\"Low\", \"Lower-Middle\", \"Middle\", \"Upper-Middle\", \"High\"])\napproval_rate_by_income = df_test.groupby(income_groups)[\"approved\"].mean()\n\n# Display results\nprint(\"Loan Approval Rates by Age Group:\")\nprint(approval_rate_by_age)\n\nprint(\"\\nLoan Approval and Default Rates by Loan Type:\")\nprint(f\"Medical - Approval Rate: {approval_rate_medical:.2f}, Default Rate: {default_rate_medical:.2f}\")\nprint(f\"Other - Approval Rate: {approval_rate_other:.2f}, Default Rate: {default_rate_other:.2f}\")\n\nprint(\"\\nLoan Approval Rates by Income Group:\")\nprint(approval_rate_by_income)\n\nLoan Approval Rates by Age Group:\nperson_age\n18-24    0.779661\n25-34    0.815186\n35-49    0.848432\n50-64    0.731707\n65+      1.000000\nName: approved, dtype: float64\n\nLoan Approval and Default Rates by Loan Type:\n\nLoan Approval and Default Rates for Medical vs. Other Loans:\nMedical - Approval Rate: 0.74, Default Rate: 0.28\nOther - Approval Rate: 0.82, Default Rate: 0.21\n\nLoan Approval Rates by Income Group:\nperson_income\nLow             0.575152\nLower-Middle    0.723222\nMiddle          0.835861\nUpper-Middle    0.901482\nHigh            0.973799\nName: approved, dtype: float64\n\n\nFrom the first table we can see that my model predicts that the group with the lowest loan approval rates is 50-64. Aside from that group, the younger age groups tend to have lower approval ratings. This means, according to my model, it is most difficult for people aged 50-64 to get loans. Whereas people above the age of 65 are guaranteed to be approved.\nFrom the second table we can see that my model predicts medical loans have a lower approval rating and a higher default rate when compared to other loans. So, according to my model, it is more difficult for people to get loans for medical expenses than it is for business, education, homes, etc., but it is also more common for people to default on medical loans.\nFrom the third table, we can see that my model predicts the loan approval rate to be worse for lower income individuals, and as someone moves into a higher income group, they have a higher chance of loan approval. Thus, it is much easer for a wealthier person to get access to credit under my system.\n\n\nReflection\nThoughout this project, I learned a lot about automated decision systems. While the model was designed to maximize lender profitability and reduce financial risk, these systems have far-reaching consequences beyond financial efficiency. Decisions made by models like mine directly impact people’s access to credit—a critical resource for economic mobility, education, healthcare, and entrepreneurship. By relying on historical data and statistical relationships, machine learning models can often reinforce existing inequalities in financial systems. For example, my study revealed that younger applicants and lower-income individuals had lower approval rates, reflecting biases where those already struggling financially find it even harder to secure credit. It is not feasible for banks to give out loans to people who are likely to default on them, and machine learning models can very accurately predict whether someone is high or low risk. However, these models are rarely 100% accurate (mine was 85%). For someone in a high-risk group that will be able to pay back a loan but never gets accepted, this decision-making system seems unfair and negatively impacts people from these high-risk groups.\nAnother controversy arises when discussing medical loans. Applicants seeking loans for medical expenses faced higher rejection rates due to their higher historical default rates. Considering ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍people ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍seeking ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loans ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍medical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍expense ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍have ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍high ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍rates ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍default, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍it ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍fair ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍it ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍more ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍difficult ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍them ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍obtain ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍access ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍credit? In the context of credit, I would define a fair system as one that uses objective and relevant financial factors without discrimination on vulnerable groups. However, sick people are often vulnerable as they often cannot work and have no control over their financial situation. This highlights the flaws in purely data-driven credit decisions. While lenders must manage risk, rejecting medical borrowers based solely on historical default rates ignores the uncontrollable nature of medical crises. True fairness should account for context, distinguishing between defaults caused by financial irresponsibility and those driven by hardship. A more ethical system would incorporate alternative lending structures—such as flexible repayment plans or government-backed medical loans—to ensure vulnerable borrowers aren’t automatically excluded from financial support. The challenge is not just predicting risk, but balancing objective financial metrics with contextual and individual nuance in credit decisions."
  },
  {
    "objectID": "posts/penguins/penguins.html",
    "href": "posts/penguins/penguins.html",
    "title": "Palmer Penguins Classification",
    "section": "",
    "text": "Abstract\nThis goal of this project was to develop a machine learning model to classify penguin species based on quantitative and qualititave characteristics using the Palmer Penguins dataset. After accessing the data and splitting it up into training and testing sets, I created visualizations on the training data to better understand which species exhibited particular traits. Then, I used a feature selection system to determine which features (two quantitative and one qualitiative) would be the best predictor of a penguin’s species. From there, I used these features to train a logistic regression model. The model’s performance is assessed using training and test accuracy and decision region visualizations. The goal of this assignment was to take a penguin with unknown species, analyze three of its given features, and with 100% accuracy, predict its species.\n\n\nData Preparation\nFirst I must access and read the data from the source.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nNow, I must prepare the dataset for machine learning by encoding the target variable (Species) into numerical values, dropping unnecessary columns, removing missing values and invalid entries, converting categorical columns into numerical representations via one-hot encoding, and splitting up training and testing sets.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\n# Initialize LabelEncoder for the target variable\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n# Function to preprocess data\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]  # Remove rows with invalid 'Sex' values\n    df = df.dropna()  # Remove any remaining missing values\n    y = le.transform(df[\"Species\"])  # Encode the target variable\n    df = df.drop([\"Species\"], axis=1)  # Remove the target column from features\n    df = pd.get_dummies(df)  # One-hot encode categorical features\n    return df, y\n\n# Prepare the dataset\nX, y = prepare_data(train)\n\n# Split the dataset into 80% training and 20% testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Convert training data back into a DataFrame for visualization purposes\ntrain_data = X_train.copy()\ntrain_data[\"Species\"] = le.inverse_transform(y_train)  # Convert encoded labels back to species names\n\n# Reconstruct the 'Island' column from one-hot encoded values\nisland_columns = ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\ntrain_data[\"Island\"] = train_data[island_columns].idxmax(axis=1).str.replace(\"Island_\", \"\")\n\n# Check the sizes of the splits\nprint(f\"Training Set Size: {X_train.shape[0]}\")\nprint(f\"Test Set Size: {X_test.shape[0]}\")\n\nTraining Set Size: 204\nTest Set Size: 52\n\n\n\n\nVisualizations\nNow, to visualize the dataset, I have created a scatter plot that plots flipper length vs. body mass of the three species and a bar chart showing the distribution of Penguins found on the three different islands. Finally, I created a summary table that shows each species’ average Culmen Length and Culmen Depth.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=train_data, x='Flipper Length (mm)', y='Body Mass (g)', hue='Species', style='Species')\n\n# Labels and title\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Body Mass (g)')\nplt.title('Flipper Length vs. Body Mass by Species (Training Data Only)')\nplt.legend(title='Species')\nplt.show()\n\n\n\n\n\n\n\n\nAbove, I have created a scatter plot that plots the flipper length and body mass of each penguin observed in the training set. I created a key with different symbols to plot each species so I could visualize the physical differences between each species. This visualization gave me valuable insight, informing me that Gentoo penguins are much larger in terms of body mass and flipper length than both Adelie and Chinstrap penguins. Adelie and Chinstrap penguins, as represented by the blue circles and green squares, share very similar sizes in these metrics. From this, we can conclude that body mass and flipper length would be excellent features with which to train our model in order to distinguish between Gentoo or not-gentoo, but these features would not help our model distinguish between Adelie and Chinstrap.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create a count plot for species distribution by island (training data only)\nplt.figure(figsize=(8, 6))\nsns.countplot(data=train_data, x='Island', hue='Species')\n\n# Labels and title\nplt.xlabel('Island')\nplt.ylabel('Count')\nplt.title('Penguin Species Distribution by Island (Training Data Only)')\nplt.legend(title='Species')\nplt.show()\n\n\n\n\n\n\n\n\nFor this visualization, I created a bar chart to see the island where each observed penguin was found. Here, we can see that Gentoo penguins are only found on Biscoe Island and Chinstrap penguins are only found on Dream Island. Adelie penguins, on the other hand, are found on all three islands. This bar chart tells me that Island location could help train my model to distinguish between Adelie and Chinstrap because Chinstrap penguins seem to only be found on Dream Island. So, if we see a penguin with quantitaive measurables that could fall under Adelie or Chinstrap, but the penguin was found on Torgersen Island, the model would correctly predict that the penguin is Adelie. This is especially helpful considering the quantitative features observed in my first visualization had a lot of overlap between Adelie and Chinstrap.\n\nimport pandas as pd\n\n# Compute the average Culmen Length & Depth in the training set only\nsummary_table = train_data.groupby(\"Species\")[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].mean()\n\n# Display the summary table\nprint(summary_table)\n\n\n                                           Culmen Length (mm)  \\\nSpecies                                                         \nAdelie Penguin (Pygoscelis adeliae)                 39.048837   \nChinstrap penguin (Pygoscelis antarctica)           48.697778   \nGentoo penguin (Pygoscelis papua)                   47.031507   \n\n                                           Culmen Depth (mm)  \nSpecies                                                       \nAdelie Penguin (Pygoscelis adeliae)                18.412791  \nChinstrap penguin (Pygoscelis antarctica)          18.406667  \nGentoo penguin (Pygoscelis papua)                  14.957534  \n\n\nHere, I created a table to see the average culmen length and culmen depth of each penguin species from the data set. From this, we can see that culmen length would be a great feature to distinguish Adelie penguins from the other two species because they have considerably shorter culmens. However, culmen length would not be very helpful in distinguishing Gentoo from Chinstrap because they have very similar measurements. As for culmen depth, we see that Adelie and Chinstrap have very similar average measurements while Gentoo penguins have considerably smaller culmen depths. This means culmen depth would be a good feature for my model to distinguish Gentoo penguins from the other two species.\n\n\nData Visualization Conclusions\nAfter creating three data visualizations, I have intuition for some of the features that would help train a successful model. From my scatter plot, I saw that body mass and flipper length would both be great features to help my model distinguish between Gentoo and Adelie or Chinstrap, but would not be very helpful in distinguishing Adelie from Chinstrap. Then, from the summary table, we could see that Adelie penguins, on average, have around a 20% shorter culmen length than Chinstrap penguins, so that would be a great feature to help my model distinguish between those two species. Finally, since Chinstrap penguins were only found on Dream Island and Gentoo Penguins were only found on Biscoe Island as represented by my bar chart, I figured this would be a nice qualitative feature with which to train my model.\n\n\nFeature Selection\nNow we must choose which three features we want to use to predict the penguin species. I gained some valuable insight from my visualizations but now want to use a systematic approach to select the three best features. To do this, I used a Random Forest Classifier by measuring how much each feature contributes to making accurate predictions. When splitting a node in a decision tree, features that provide better separation between classes are preferred. The model tracks how often a feature is used in important splits and how much it improves classification accuracy. The importance of each feature is calculated as the total reduction in prediction error (impurity) it provides across all trees. Features with higher scores contribute more to the model’s decision-making.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train a random forest to assess feature importance\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Get feature importance scores\nfeature_importance = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': model.feature_importances_\n})\n\n# Sort by importance\nfeature_importance = feature_importance.sort_values(by='Importance', ascending=False)\nprint(feature_importance)\n\n                     Feature  Importance\n0         Culmen Length (mm)    0.234636\n2        Flipper Length (mm)    0.179364\n5          Delta 13 C (o/oo)    0.161621\n1          Culmen Depth (mm)    0.148071\n4          Delta 15 N (o/oo)    0.075136\n7               Island_Dream    0.063007\n6              Island_Biscoe    0.059049\n3              Body Mass (g)    0.058736\n8           Island_Torgersen    0.012969\n10      Clutch Completion_No    0.002622\n12                Sex_FEMALE    0.002250\n13                  Sex_MALE    0.001822\n11     Clutch Completion_Yes    0.000717\n9   Stage_Adult, 1 Egg Stage    0.000000\n\n\nFrom this, we can conclude that the best qualitative category is Island and the best quantitative categories are Culmen Length and Flipper Length. So, these are the features we will train our model on.\n\n\nLogistic Regression Model Testing\nThe model below uses logistic regression to classify data based on flipper length, culmen length, and island. First, it standardizes the numerical features using StandardScaler() to ensure consistent scaling. Then, it trains a logistic regression model with increased iterations (1000) to ensure convergence. Finally, it evaluates performance using training and test accuracy, helping assess how well the model generalizes.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# Define selected features\ncols = ['Flipper Length (mm)', 'Culmen Length (mm)','Island_Biscoe', 'Island_Dream', 'Island_Torgersen',]\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[cols]), columns=cols)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test[cols]), columns=cols)\n\n\n# Initialize and train logistic regression with increased iterations\nLR = LogisticRegression(max_iter=1000)  # Increased max_iter to ensure convergence\nLR.fit(X_train_scaled, y_train)\n\n# Evaluate model\nprint(\"Training Accuracy:\", LR.score(X_train_scaled, y_train))\nprint(\"Test Accuracy:\", LR.score(X_test_scaled, y_test))\n\n\n\nTraining Accuracy: 0.9754901960784313\nTest Accuracy: 1.0\n\n\n\n\nPlotting Decision Regions\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, scaler):\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(7, 3))\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n\n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({X.columns[0]: XX, X.columns[1]: YY})\n\n        # Initialize categorical features as zeros\n        for j in qual_features:\n            XY[j] = 0\n\n        # Set the specific category feature to 1\n        XY[qual_features[i]] = 1\n\n        # Standardize XY to match model input\n        XY_scaled = pd.DataFrame(scaler.transform(XY), columns=X.columns)\n\n        # Predict decision boundary\n        p = model.predict(XY_scaled)\n        p = p.reshape(xx.shape)\n\n        # Use contour plot to visualize the predictions\n        decision_cmap = ListedColormap([\"blue\", \"green\", \"orange\"])  # Match species colors\n        axarr[i].contourf(xx, yy, p, cmap=decision_cmap, alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n\n        # Plot the actual training data points\n        species_cmap = ListedColormap([\"blue\", \"green\", \"orange\"])  # blue = Adelie, Green = Chinstrap, orange = Gentoo\n        axarr[i].scatter(x0[ix], x1[ix], c=y[ix], cmap=species_cmap, vmin=0, vmax=2)\n\n        axarr[i].set(\n            xlabel=X.columns[0],\n            ylabel=X.columns[1],\n            title=qual_features[i]\n        )\n\n        patches = []\n        for color, spec in zip([\"blue\", \"green\", \"orange\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color=color, label=spec))\n\n        axarr[i].legend(handles=patches, title=\"Species\", loc=\"best\")\n\n    plt.tight_layout()\n\n# Call the function with scaler applied\nplot_regions(LR, X_train[cols], y_train, scaler)\nplot_regions(LR, X_test[cols], y_test, scaler)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbove, I have plotted my model’s decision regions on both the training and test sets, showing the thresholds of prediction for each species split up by each island.\nFor both the training and test set of penguins on Torgersen Island, my model predicted with 100% success as there were no other penguin found on this island besides Adelies.\nDream Island contained both Adelie and Chinstrap penguins, which both have very similar flipper length, so the decision regions for that island are based almost entirely by culmen length, as can be seen by the near-horizontal divide between the green and blue regions. My model did have some error on the training set for Dream Island as there were some outliers of the decision regions – Chinstraps with below average culmen lengths and Adelies with above average culmen lengths. The test set did not have as many outliers and therefore still predicted with 100% accuracy on Dream Island.\nAs for Biscoe Island, there were both Gentoo and Adelie penguins. As observed in my data visualizations, Gentoo penguins are considerably larger than Adelie in both flipper length and culmen length so the regions are divided diagonally. There were no overlaps in the decision regions on Biscoe in either the test or training set.\nIt is interesting to me that even though no Gentoo or Chinstrap penguins were found on Torgersen in the entire data set, there are still orange and green regions. Similarly, no Gentoos are found on Dream or Torgerson, but my model still has an orange decision region for both of those islands. This makes sense to me because, for instance, if the model found a penguin with a 230 mm flipper length and 60 mm culmen depth on Torgersen, it would be a reasonable prediction for that penguin to be a Gentoo, even if there are no other Gentoos on the island.\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test_scaled)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[22,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 19]])\n\n\nAbove is my models confusion matrix for the test set. The rows represent the actual species of a penguin while the columns represent the predicted species. As we can see, my model predicted all 22 Adelies to be Adelie, all 11 Chinstraps to be Chinstraps, and all 19 Gentoos to be gentoos. Therefore, my model correctly predicted the species of all 52 penguins in the test set with no errors.\n\n\nDiscussion\nThroughout this project, I gained valuable insights into the entire machine learning process. First, it was essential to prepare the data into training and test sections, encode qualitative regions into quantitave values, drop unessecary columns, and simplify column names. Then, the data visualizations helped me uncover trends in the dataset. This allowed me to get a foundational understanding of the variance between the species’ characteristics. A key takeaway was the importance of feature selection. I initially just analyzed my data visualizations and chose features I thought would be strong predictors. Even though this was somewhat accurate, when I automated my feature selection with a RandomForestClassifier and chose the most statistically important feautures, I was able to bring my model’s prediction success up to 100% for the test set. I also gained experience with model selection and scaling. After trying out a few options, I chose the Logistic Regression model because it was consistently high performing. During this process, I also realized that I had to standardize the numerical features to ensure consistent scaling. Perhaps the most interesting discovery of the entire process was plotting the decision regions to see where my model numerically distinguished between the species’ and to see the entire regions where a particular species would fall."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Evan Flaks’ Machine Learning Blog",
    "section": "",
    "text": "Design and Impact of Automated Decision Systems\n\n\n\n\n\nBuilding and analyzing an automated system for a bank extending credit\n\n\n\n\n\nMar 5, 2025\n\n\nEvan Flaks\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nMachine Learning model to predict penguin species\n\n\n\n\n\nMar 5, 2025\n\n\nEvan Flaks\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Classification\n\n\n\n\n\nMachine Learning model to predict penguin species\n\n\n\n\n\nFeb 18, 2025\n\n\nEvan Flaks\n\n\n\n\n\n\nNo matching items"
  }
]