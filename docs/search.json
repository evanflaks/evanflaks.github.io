[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Evan Flaks and this is my Machine Learning blog."
  },
  {
    "objectID": "posts/kernel/index.html",
    "href": "posts/kernel/index.html",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt\n\n%load_ext autoreload\n%autoreload 2\nfrom kernel import KernelLogisticRegression\n\n\nAbstract\n\n\nImplementation\n\n\nExperiments\nLet’s generate ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍simple ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍set ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍with ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍roughly ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍linear ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍separating ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍boundary using the code below.\n\nimport torch \nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    X = X - X.mean(dim = 0, keepdim = True)\n    return X, y\n\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(n_points = 100, noise = 0.4)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nThe rbf_kernel function below computes the Radial Basis Functionkernel — a popular way to measure similarity between data points. It takes two sets of vectors, \\(X_1\\) and \\(X_2\\), and returns a matrix where each entry represents how similar a point in \\(X_1\\) is to a point in \\(X_2\\). It does this by computing the squared Euclidean distances between the points (via torch.cdist), scaling those distances by gamma, and applying the exponential function. The result is a kernel matrix where closer points have values near 1 and more distant points have values near 0.\n\ndef rbf_kernel(X_1, X_2, gamma):\n    return torch.exp(-gamma*torch.cdist(X_1, X_2)**2)\n\nBelow, I have ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍code that specifies ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍500,000 ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍epochs ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍gradient ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍descent ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍with ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍learning ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍rate ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍ \\(10^{-4}\\). ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍The ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍parameter gamma ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍keyword ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍passed ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍rbf_kernel ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍which ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍controls ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bandwidth as it is a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍hyperparameter ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍can ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍be ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍tuned ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍cross-validation.\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 0.1, gamma = 1)\nKR.fit(X, y, epochs = 500000, lr = 0.0001)\n\nEpoch 0: Loss = 0.6931\nEpoch 100: Loss = 0.6923\nEpoch 200: Loss = 0.6914\nEpoch 300: Loss = 0.6906\nEpoch 400: Loss = 0.6898\nEpoch 500: Loss = 0.6890\nEpoch 600: Loss = 0.6882\nEpoch 700: Loss = 0.6874\nEpoch 800: Loss = 0.6867\nEpoch 900: Loss = 0.6860\nEpoch 1000: Loss = 0.6853\nEpoch 1100: Loss = 0.6846\nEpoch 1200: Loss = 0.6839\nEpoch 1300: Loss = 0.6833\nEpoch 1400: Loss = 0.6827\nEpoch 1500: Loss = 0.6820\nEpoch 1600: Loss = 0.6814\nEpoch 1700: Loss = 0.6808\nEpoch 1800: Loss = 0.6803\nEpoch 1900: Loss = 0.6797\nEpoch 2000: Loss = 0.6791\nEpoch 2100: Loss = 0.6786\nEpoch 2200: Loss = 0.6781\nEpoch 2300: Loss = 0.6776\nEpoch 2400: Loss = 0.6770\nEpoch 2500: Loss = 0.6766\nEpoch 2600: Loss = 0.6761\nEpoch 2700: Loss = 0.6756\nEpoch 2800: Loss = 0.6751\nEpoch 2900: Loss = 0.6747\nEpoch 3000: Loss = 0.6742\nEpoch 3100: Loss = 0.6738\nEpoch 3200: Loss = 0.6734\nEpoch 3300: Loss = 0.6730\nEpoch 3400: Loss = 0.6726\nEpoch 3500: Loss = 0.6722\nEpoch 3600: Loss = 0.6718\nEpoch 3700: Loss = 0.6714\nEpoch 3800: Loss = 0.6710\nEpoch 3900: Loss = 0.6707\nEpoch 4000: Loss = 0.6703\nEpoch 4100: Loss = 0.6700\nEpoch 4200: Loss = 0.6696\nEpoch 4300: Loss = 0.6693\nEpoch 4400: Loss = 0.6689\nEpoch 4500: Loss = 0.6686\nEpoch 4600: Loss = 0.6683\nEpoch 4700: Loss = 0.6680\nEpoch 4800: Loss = 0.6677\nEpoch 4900: Loss = 0.6674\nEpoch 5000: Loss = 0.6671\nEpoch 5100: Loss = 0.6668\nEpoch 5200: Loss = 0.6665\nEpoch 5300: Loss = 0.6663\nEpoch 5400: Loss = 0.6660\nEpoch 5500: Loss = 0.6657\nEpoch 5600: Loss = 0.6655\nEpoch 5700: Loss = 0.6652\nEpoch 5800: Loss = 0.6650\nEpoch 5900: Loss = 0.6647\nEpoch 6000: Loss = 0.6645\nEpoch 6100: Loss = 0.6643\nEpoch 6200: Loss = 0.6640\nEpoch 6300: Loss = 0.6638\nEpoch 6400: Loss = 0.6636\nEpoch 6500: Loss = 0.6634\nEpoch 6600: Loss = 0.6632\nEpoch 6700: Loss = 0.6629\nEpoch 6800: Loss = 0.6627\nEpoch 6900: Loss = 0.6625\nEpoch 7000: Loss = 0.6623\nEpoch 7100: Loss = 0.6621\nEpoch 7200: Loss = 0.6619\nEpoch 7300: Loss = 0.6618\nEpoch 7400: Loss = 0.6616\nEpoch 7500: Loss = 0.6614\nEpoch 7600: Loss = 0.6612\nEpoch 7700: Loss = 0.6610\nEpoch 7800: Loss = 0.6609\nEpoch 7900: Loss = 0.6607\nEpoch 8000: Loss = 0.6605\nEpoch 8100: Loss = 0.6604\nEpoch 8200: Loss = 0.6602\nEpoch 8300: Loss = 0.6600\nEpoch 8400: Loss = 0.6599\nEpoch 8500: Loss = 0.6597\nEpoch 8600: Loss = 0.6596\nEpoch 8700: Loss = 0.6594\nEpoch 8800: Loss = 0.6593\nEpoch 8900: Loss = 0.6591\nEpoch 9000: Loss = 0.6590\nEpoch 9100: Loss = 0.6588\nEpoch 9200: Loss = 0.6587\nEpoch 9300: Loss = 0.6586\nEpoch 9400: Loss = 0.6584\nEpoch 9500: Loss = 0.6583\nEpoch 9600: Loss = 0.6582\nEpoch 9700: Loss = 0.6581\nEpoch 9800: Loss = 0.6579\nEpoch 9900: Loss = 0.6578\nEpoch 10000: Loss = 0.6577\nEpoch 10100: Loss = 0.6576\nEpoch 10200: Loss = 0.6575\nEpoch 10300: Loss = 0.6574\nEpoch 10400: Loss = 0.6572\nEpoch 10500: Loss = 0.6571\nEpoch 10600: Loss = 0.6570\nEpoch 10700: Loss = 0.6569\nEpoch 10800: Loss = 0.6568\nEpoch 10900: Loss = 0.6567\nEpoch 11000: Loss = 0.6566\nEpoch 11100: Loss = 0.6565\nEpoch 11200: Loss = 0.6564\nEpoch 11300: Loss = 0.6563\nEpoch 11400: Loss = 0.6562\nEpoch 11500: Loss = 0.6561\nEpoch 11600: Loss = 0.6560\nEpoch 11700: Loss = 0.6559\nEpoch 11800: Loss = 0.6559\nEpoch 11900: Loss = 0.6558\nEpoch 12000: Loss = 0.6557\nEpoch 12100: Loss = 0.6556\nEpoch 12200: Loss = 0.6555\nEpoch 12300: Loss = 0.6554\nEpoch 12400: Loss = 0.6554\nEpoch 12500: Loss = 0.6553\nEpoch 12600: Loss = 0.6552\nEpoch 12700: Loss = 0.6551\nEpoch 12800: Loss = 0.6550\nEpoch 12900: Loss = 0.6550\nEpoch 13000: Loss = 0.6549\nEpoch 13100: Loss = 0.6548\nEpoch 13200: Loss = 0.6547\nEpoch 13300: Loss = 0.6547\nEpoch 13400: Loss = 0.6546\nEpoch 13500: Loss = 0.6545\nEpoch 13600: Loss = 0.6545\nEpoch 13700: Loss = 0.6544\nEpoch 13800: Loss = 0.6543\nEpoch 13900: Loss = 0.6543\nEpoch 14000: Loss = 0.6542\nEpoch 14100: Loss = 0.6541\nEpoch 14200: Loss = 0.6541\nEpoch 14300: Loss = 0.6540\nEpoch 14400: Loss = 0.6540\nEpoch 14500: Loss = 0.6539\nEpoch 14600: Loss = 0.6538\nEpoch 14700: Loss = 0.6538\nEpoch 14800: Loss = 0.6537\nEpoch 14900: Loss = 0.6537\nEpoch 15000: Loss = 0.6536\nEpoch 15100: Loss = 0.6536\nEpoch 15200: Loss = 0.6535\nEpoch 15300: Loss = 0.6535\nEpoch 15400: Loss = 0.6534\nEpoch 15500: Loss = 0.6533\nEpoch 15600: Loss = 0.6533\nEpoch 15700: Loss = 0.6532\nEpoch 15800: Loss = 0.6532\nEpoch 15900: Loss = 0.6531\nEpoch 16000: Loss = 0.6531\nEpoch 16100: Loss = 0.6530\nEpoch 16200: Loss = 0.6530\nEpoch 16300: Loss = 0.6530\nEpoch 16400: Loss = 0.6529\nEpoch 16500: Loss = 0.6529\nEpoch 16600: Loss = 0.6528\nEpoch 16700: Loss = 0.6528\nEpoch 16800: Loss = 0.6527\nEpoch 16900: Loss = 0.6527\nEpoch 17000: Loss = 0.6526\nEpoch 17100: Loss = 0.6526\nEpoch 17200: Loss = 0.6526\nEpoch 17300: Loss = 0.6525\nEpoch 17400: Loss = 0.6525\nEpoch 17500: Loss = 0.6524\nEpoch 17600: Loss = 0.6524\nEpoch 17700: Loss = 0.6524\nEpoch 17800: Loss = 0.6523\nEpoch 17900: Loss = 0.6523\nEpoch 18000: Loss = 0.6522\nEpoch 18100: Loss = 0.6522\nEpoch 18200: Loss = 0.6522\nEpoch 18300: Loss = 0.6521\nEpoch 18400: Loss = 0.6521\nEpoch 18500: Loss = 0.6521\nEpoch 18600: Loss = 0.6520\nEpoch 18700: Loss = 0.6520\nEpoch 18800: Loss = 0.6520\nEpoch 18900: Loss = 0.6519\nEpoch 19000: Loss = 0.6519\nEpoch 19100: Loss = 0.6519\nEpoch 19200: Loss = 0.6518\nEpoch 19300: Loss = 0.6518\nEpoch 19400: Loss = 0.6518\nEpoch 19500: Loss = 0.6517\nEpoch 19600: Loss = 0.6517\nEpoch 19700: Loss = 0.6517\nEpoch 19800: Loss = 0.6516\nEpoch 19900: Loss = 0.6516\nEpoch 20000: Loss = 0.6516\nEpoch 20100: Loss = 0.6515\nEpoch 20200: Loss = 0.6515\nEpoch 20300: Loss = 0.6515\nEpoch 20400: Loss = 0.6515\nEpoch 20500: Loss = 0.6514\nEpoch 20600: Loss = 0.6514\nEpoch 20700: Loss = 0.6514\nEpoch 20800: Loss = 0.6513\nEpoch 20900: Loss = 0.6513\nEpoch 21000: Loss = 0.6513\nEpoch 21100: Loss = 0.6513\nEpoch 21200: Loss = 0.6512\nEpoch 21300: Loss = 0.6512\nEpoch 21400: Loss = 0.6512\nEpoch 21500: Loss = 0.6511\nEpoch 21600: Loss = 0.6511\nEpoch 21700: Loss = 0.6511\nEpoch 21800: Loss = 0.6511\nEpoch 21900: Loss = 0.6510\nEpoch 22000: Loss = 0.6510\nEpoch 22100: Loss = 0.6510\nEpoch 22200: Loss = 0.6510\nEpoch 22300: Loss = 0.6509\nEpoch 22400: Loss = 0.6509\nEpoch 22500: Loss = 0.6509\nEpoch 22600: Loss = 0.6509\nEpoch 22700: Loss = 0.6508\nEpoch 22800: Loss = 0.6508\nEpoch 22900: Loss = 0.6508\nEpoch 23000: Loss = 0.6508\nEpoch 23100: Loss = 0.6507\nEpoch 23200: Loss = 0.6507\nEpoch 23300: Loss = 0.6507\nEpoch 23400: Loss = 0.6507\nEpoch 23500: Loss = 0.6507\nEpoch 23600: Loss = 0.6506\nEpoch 23700: Loss = 0.6506\nEpoch 23800: Loss = 0.6506\nEpoch 23900: Loss = 0.6506\nEpoch 24000: Loss = 0.6505\nEpoch 24100: Loss = 0.6505\nEpoch 24200: Loss = 0.6505\nEpoch 24300: Loss = 0.6505\nEpoch 24400: Loss = 0.6505\nEpoch 24500: Loss = 0.6504\nEpoch 24600: Loss = 0.6504\nEpoch 24700: Loss = 0.6504\nEpoch 24800: Loss = 0.6504\nEpoch 24900: Loss = 0.6504\nEpoch 25000: Loss = 0.6504\nEpoch 25100: Loss = 0.6503\nEpoch 25200: Loss = 0.6503\nEpoch 25300: Loss = 0.6503\nEpoch 25400: Loss = 0.6503\nEpoch 25500: Loss = 0.6503\nEpoch 25600: Loss = 0.6502\nEpoch 25700: Loss = 0.6502\nEpoch 25800: Loss = 0.6502\nEpoch 25900: Loss = 0.6502\nEpoch 26000: Loss = 0.6502\nEpoch 26100: Loss = 0.6501\nEpoch 26200: Loss = 0.6501\nEpoch 26300: Loss = 0.6501\nEpoch 26400: Loss = 0.6501\nEpoch 26500: Loss = 0.6501\nEpoch 26600: Loss = 0.6501\nEpoch 26700: Loss = 0.6500\nEpoch 26800: Loss = 0.6500\nEpoch 26900: Loss = 0.6500\nEpoch 27000: Loss = 0.6500\nEpoch 27100: Loss = 0.6500\nEpoch 27200: Loss = 0.6500\nEpoch 27300: Loss = 0.6499\nEpoch 27400: Loss = 0.6499\nEpoch 27500: Loss = 0.6499\nEpoch 27600: Loss = 0.6499\nEpoch 27700: Loss = 0.6499\nEpoch 27800: Loss = 0.6499\nEpoch 27900: Loss = 0.6498\nEpoch 28000: Loss = 0.6498\nEpoch 28100: Loss = 0.6498\nEpoch 28200: Loss = 0.6498\nEpoch 28300: Loss = 0.6498\nEpoch 28400: Loss = 0.6498\nEpoch 28500: Loss = 0.6497\nEpoch 28600: Loss = 0.6497\nEpoch 28700: Loss = 0.6497\nEpoch 28800: Loss = 0.6497\nEpoch 28900: Loss = 0.6497\nEpoch 29000: Loss = 0.6497\nEpoch 29100: Loss = 0.6496\nEpoch 29200: Loss = 0.6496\nEpoch 29300: Loss = 0.6496\nEpoch 29400: Loss = 0.6496\nEpoch 29500: Loss = 0.6496\nEpoch 29600: Loss = 0.6496\nEpoch 29700: Loss = 0.6496\nEpoch 29800: Loss = 0.6495\nEpoch 29900: Loss = 0.6495\nEpoch 30000: Loss = 0.6495\nEpoch 30100: Loss = 0.6495\nEpoch 30200: Loss = 0.6495\nEpoch 30300: Loss = 0.6495\nEpoch 30400: Loss = 0.6494\nEpoch 30500: Loss = 0.6494\nEpoch 30600: Loss = 0.6494\nEpoch 30700: Loss = 0.6494\nEpoch 30800: Loss = 0.6494\nEpoch 30900: Loss = 0.6494\nEpoch 31000: Loss = 0.6493\nEpoch 31100: Loss = 0.6493\nEpoch 31200: Loss = 0.6493\nEpoch 31300: Loss = 0.6493\nEpoch 31400: Loss = 0.6493\nEpoch 31500: Loss = 0.6493\nEpoch 31600: Loss = 0.6493\nEpoch 31700: Loss = 0.6492\nEpoch 31800: Loss = 0.6492\nEpoch 31900: Loss = 0.6492\nEpoch 32000: Loss = 0.6492\nEpoch 32100: Loss = 0.6492\nEpoch 32200: Loss = 0.6492\nEpoch 32300: Loss = 0.6492\nEpoch 32400: Loss = 0.6491\nEpoch 32500: Loss = 0.6491\nEpoch 32600: Loss = 0.6491\nEpoch 32700: Loss = 0.6491\nEpoch 32800: Loss = 0.6491\nEpoch 32900: Loss = 0.6491\nEpoch 33000: Loss = 0.6491\nEpoch 33100: Loss = 0.6491\nEpoch 33200: Loss = 0.6490\nEpoch 33300: Loss = 0.6490\nEpoch 33400: Loss = 0.6490\nEpoch 33500: Loss = 0.6490\nEpoch 33600: Loss = 0.6490\nEpoch 33700: Loss = 0.6490\nEpoch 33800: Loss = 0.6490\nEpoch 33900: Loss = 0.6489\nEpoch 34000: Loss = 0.6489\nEpoch 34100: Loss = 0.6489\nEpoch 34200: Loss = 0.6489\nEpoch 34300: Loss = 0.6489\nEpoch 34400: Loss = 0.6489\nEpoch 34500: Loss = 0.6489\nEpoch 34600: Loss = 0.6489\nEpoch 34700: Loss = 0.6488\nEpoch 34800: Loss = 0.6488\nEpoch 34900: Loss = 0.6488\nEpoch 35000: Loss = 0.6488\nEpoch 35100: Loss = 0.6488\nEpoch 35200: Loss = 0.6488\nEpoch 35300: Loss = 0.6488\nEpoch 35400: Loss = 0.6488\nEpoch 35500: Loss = 0.6487\nEpoch 35600: Loss = 0.6487\nEpoch 35700: Loss = 0.6487\nEpoch 35800: Loss = 0.6487\nEpoch 35900: Loss = 0.6487\nEpoch 36000: Loss = 0.6487\nEpoch 36100: Loss = 0.6487\nEpoch 36200: Loss = 0.6487\nEpoch 36300: Loss = 0.6486\nEpoch 36400: Loss = 0.6486\nEpoch 36500: Loss = 0.6486\nEpoch 36600: Loss = 0.6486\nEpoch 36700: Loss = 0.6486\nEpoch 36800: Loss = 0.6486\nEpoch 36900: Loss = 0.6486\nEpoch 37000: Loss = 0.6486\nEpoch 37100: Loss = 0.6486\nEpoch 37200: Loss = 0.6485\nEpoch 37300: Loss = 0.6485\nEpoch 37400: Loss = 0.6485\nEpoch 37500: Loss = 0.6485\nEpoch 37600: Loss = 0.6485\nEpoch 37700: Loss = 0.6485\nEpoch 37800: Loss = 0.6485\nEpoch 37900: Loss = 0.6485\nEpoch 38000: Loss = 0.6484\nEpoch 38100: Loss = 0.6484\nEpoch 38200: Loss = 0.6484\nEpoch 38300: Loss = 0.6484\nEpoch 38400: Loss = 0.6484\nEpoch 38500: Loss = 0.6484\nEpoch 38600: Loss = 0.6484\nEpoch 38700: Loss = 0.6484\nEpoch 38800: Loss = 0.6484\nEpoch 38900: Loss = 0.6484\nEpoch 39000: Loss = 0.6483\nEpoch 39100: Loss = 0.6483\nEpoch 39200: Loss = 0.6483\nEpoch 39300: Loss = 0.6483\nEpoch 39400: Loss = 0.6483\nEpoch 39500: Loss = 0.6483\nEpoch 39600: Loss = 0.6483\nEpoch 39700: Loss = 0.6483\nEpoch 39800: Loss = 0.6483\nEpoch 39900: Loss = 0.6482\nEpoch 40000: Loss = 0.6482\nEpoch 40100: Loss = 0.6482\nEpoch 40200: Loss = 0.6482\nEpoch 40300: Loss = 0.6482\nEpoch 40400: Loss = 0.6482\nEpoch 40500: Loss = 0.6482\nEpoch 40600: Loss = 0.6482\nEpoch 40700: Loss = 0.6482\nEpoch 40800: Loss = 0.6481\nEpoch 40900: Loss = 0.6481\nEpoch 41000: Loss = 0.6481\nEpoch 41100: Loss = 0.6481\nEpoch 41200: Loss = 0.6481\nEpoch 41300: Loss = 0.6481\nEpoch 41400: Loss = 0.6481\nEpoch 41500: Loss = 0.6481\nEpoch 41600: Loss = 0.6481\nEpoch 41700: Loss = 0.6481\nEpoch 41800: Loss = 0.6480\nEpoch 41900: Loss = 0.6480\nEpoch 42000: Loss = 0.6480\nEpoch 42100: Loss = 0.6480\nEpoch 42200: Loss = 0.6480\nEpoch 42300: Loss = 0.6480\nEpoch 42400: Loss = 0.6480\nEpoch 42500: Loss = 0.6480\nEpoch 42600: Loss = 0.6480\nEpoch 42700: Loss = 0.6480\nEpoch 42800: Loss = 0.6480\nEpoch 42900: Loss = 0.6479\nEpoch 43000: Loss = 0.6479\nEpoch 43100: Loss = 0.6479\nEpoch 43200: Loss = 0.6479\nEpoch 43300: Loss = 0.6479\nEpoch 43400: Loss = 0.6479\nEpoch 43500: Loss = 0.6479\nEpoch 43600: Loss = 0.6479\nEpoch 43700: Loss = 0.6479\nEpoch 43800: Loss = 0.6479\nEpoch 43900: Loss = 0.6479\nEpoch 44000: Loss = 0.6479\nEpoch 44100: Loss = 0.6478\nEpoch 44200: Loss = 0.6478\nEpoch 44300: Loss = 0.6478\nEpoch 44400: Loss = 0.6478\nEpoch 44500: Loss = 0.6478\nEpoch 44600: Loss = 0.6478\nEpoch 44700: Loss = 0.6478\nEpoch 44800: Loss = 0.6478\nEpoch 44900: Loss = 0.6478\nEpoch 45000: Loss = 0.6478\nEpoch 45100: Loss = 0.6478\nEpoch 45200: Loss = 0.6477\nEpoch 45300: Loss = 0.6477\nEpoch 45400: Loss = 0.6477\nEpoch 45500: Loss = 0.6477\nEpoch 45600: Loss = 0.6477\nEpoch 45700: Loss = 0.6477\nEpoch 45800: Loss = 0.6477\nEpoch 45900: Loss = 0.6477\nEpoch 46000: Loss = 0.6477\nEpoch 46100: Loss = 0.6477\nEpoch 46200: Loss = 0.6477\nEpoch 46300: Loss = 0.6477\nEpoch 46400: Loss = 0.6476\nEpoch 46500: Loss = 0.6476\nEpoch 46600: Loss = 0.6476\nEpoch 46700: Loss = 0.6476\nEpoch 46800: Loss = 0.6476\nEpoch 46900: Loss = 0.6476\nEpoch 47000: Loss = 0.6476\nEpoch 47100: Loss = 0.6476\nEpoch 47200: Loss = 0.6476\nEpoch 47300: Loss = 0.6476\nEpoch 47400: Loss = 0.6476\nEpoch 47500: Loss = 0.6476\nEpoch 47600: Loss = 0.6475\nEpoch 47700: Loss = 0.6475\nEpoch 47800: Loss = 0.6475\nEpoch 47900: Loss = 0.6475\nEpoch 48000: Loss = 0.6475\nEpoch 48100: Loss = 0.6475\nEpoch 48200: Loss = 0.6475\nEpoch 48300: Loss = 0.6475\nEpoch 48400: Loss = 0.6475\nEpoch 48500: Loss = 0.6475\nEpoch 48600: Loss = 0.6475\nEpoch 48700: Loss = 0.6475\nEpoch 48800: Loss = 0.6474\nEpoch 48900: Loss = 0.6474\nEpoch 49000: Loss = 0.6474\nEpoch 49100: Loss = 0.6474\nEpoch 49200: Loss = 0.6474\nEpoch 49300: Loss = 0.6474\nEpoch 49400: Loss = 0.6474\nEpoch 49500: Loss = 0.6474\nEpoch 49600: Loss = 0.6474\nEpoch 49700: Loss = 0.6474\nEpoch 49800: Loss = 0.6474\nEpoch 49900: Loss = 0.6474\nEpoch 50000: Loss = 0.6473\nEpoch 50100: Loss = 0.6473\nEpoch 50200: Loss = 0.6473\nEpoch 50300: Loss = 0.6473\nEpoch 50400: Loss = 0.6473\nEpoch 50500: Loss = 0.6473\nEpoch 50600: Loss = 0.6473\nEpoch 50700: Loss = 0.6473\nEpoch 50800: Loss = 0.6473\nEpoch 50900: Loss = 0.6473\nEpoch 51000: Loss = 0.6473\nEpoch 51100: Loss = 0.6473\nEpoch 51200: Loss = 0.6473\nEpoch 51300: Loss = 0.6472\nEpoch 51400: Loss = 0.6472\nEpoch 51500: Loss = 0.6472\nEpoch 51600: Loss = 0.6472\nEpoch 51700: Loss = 0.6472\nEpoch 51800: Loss = 0.6472\nEpoch 51900: Loss = 0.6472\nEpoch 52000: Loss = 0.6472\nEpoch 52100: Loss = 0.6472\nEpoch 52200: Loss = 0.6472\nEpoch 52300: Loss = 0.6472\nEpoch 52400: Loss = 0.6472\nEpoch 52500: Loss = 0.6472\nEpoch 52600: Loss = 0.6471\nEpoch 52700: Loss = 0.6471\nEpoch 52800: Loss = 0.6471\nEpoch 52900: Loss = 0.6471\nEpoch 53000: Loss = 0.6471\nEpoch 53100: Loss = 0.6471\nEpoch 53200: Loss = 0.6471\nEpoch 53300: Loss = 0.6471\nEpoch 53400: Loss = 0.6471\nEpoch 53500: Loss = 0.6471\nEpoch 53600: Loss = 0.6471\nEpoch 53700: Loss = 0.6471\nEpoch 53800: Loss = 0.6471\nEpoch 53900: Loss = 0.6471\nEpoch 54000: Loss = 0.6471\nEpoch 54100: Loss = 0.6470\nEpoch 54200: Loss = 0.6470\nEpoch 54300: Loss = 0.6470\nEpoch 54400: Loss = 0.6470\nEpoch 54500: Loss = 0.6470\nEpoch 54600: Loss = 0.6470\nEpoch 54700: Loss = 0.6470\nEpoch 54800: Loss = 0.6470\nEpoch 54900: Loss = 0.6470\nEpoch 55000: Loss = 0.6470\nEpoch 55100: Loss = 0.6470\nEpoch 55200: Loss = 0.6470\nEpoch 55300: Loss = 0.6470\nEpoch 55400: Loss = 0.6470\nEpoch 55500: Loss = 0.6470\nEpoch 55600: Loss = 0.6469\nEpoch 55700: Loss = 0.6469\nEpoch 55800: Loss = 0.6469\nEpoch 55900: Loss = 0.6469\nEpoch 56000: Loss = 0.6469\nEpoch 56100: Loss = 0.6469\nEpoch 56200: Loss = 0.6469\nEpoch 56300: Loss = 0.6469\nEpoch 56400: Loss = 0.6469\nEpoch 56500: Loss = 0.6469\nEpoch 56600: Loss = 0.6469\nEpoch 56700: Loss = 0.6469\nEpoch 56800: Loss = 0.6469\nEpoch 56900: Loss = 0.6469\nEpoch 57000: Loss = 0.6469\nEpoch 57100: Loss = 0.6469\nEpoch 57200: Loss = 0.6468\nEpoch 57300: Loss = 0.6468\nEpoch 57400: Loss = 0.6468\nEpoch 57500: Loss = 0.6468\nEpoch 57600: Loss = 0.6468\nEpoch 57700: Loss = 0.6468\nEpoch 57800: Loss = 0.6468\nEpoch 57900: Loss = 0.6468\nEpoch 58000: Loss = 0.6468\nEpoch 58100: Loss = 0.6468\nEpoch 58200: Loss = 0.6468\nEpoch 58300: Loss = 0.6468\nEpoch 58400: Loss = 0.6468\nEpoch 58500: Loss = 0.6468\nEpoch 58600: Loss = 0.6468\nEpoch 58700: Loss = 0.6467\nEpoch 58800: Loss = 0.6467\nEpoch 58900: Loss = 0.6467\nEpoch 59000: Loss = 0.6467\nEpoch 59100: Loss = 0.6467\nEpoch 59200: Loss = 0.6467\nEpoch 59300: Loss = 0.6467\nEpoch 59400: Loss = 0.6467\nEpoch 59500: Loss = 0.6467\nEpoch 59600: Loss = 0.6467\nEpoch 59700: Loss = 0.6467\nEpoch 59800: Loss = 0.6467\nEpoch 59900: Loss = 0.6467\nEpoch 60000: Loss = 0.6467\nEpoch 60100: Loss = 0.6467\nEpoch 60200: Loss = 0.6467\nEpoch 60300: Loss = 0.6466\nEpoch 60400: Loss = 0.6466\nEpoch 60500: Loss = 0.6466\nEpoch 60600: Loss = 0.6466\nEpoch 60700: Loss = 0.6466\nEpoch 60800: Loss = 0.6466\nEpoch 60900: Loss = 0.6466\nEpoch 61000: Loss = 0.6466\nEpoch 61100: Loss = 0.6466\nEpoch 61200: Loss = 0.6466\nEpoch 61300: Loss = 0.6466\nEpoch 61400: Loss = 0.6466\nEpoch 61500: Loss = 0.6466\nEpoch 61600: Loss = 0.6466\nEpoch 61700: Loss = 0.6466\nEpoch 61800: Loss = 0.6465\nEpoch 61900: Loss = 0.6465\nEpoch 62000: Loss = 0.6465\nEpoch 62100: Loss = 0.6465\nEpoch 62200: Loss = 0.6465\nEpoch 62300: Loss = 0.6465\nEpoch 62400: Loss = 0.6465\nEpoch 62500: Loss = 0.6465\nEpoch 62600: Loss = 0.6465\nEpoch 62700: Loss = 0.6465\nEpoch 62800: Loss = 0.6465\nEpoch 62900: Loss = 0.6465\nEpoch 63000: Loss = 0.6465\nEpoch 63100: Loss = 0.6465\nEpoch 63200: Loss = 0.6465\nEpoch 63300: Loss = 0.6465\nEpoch 63400: Loss = 0.6464\nEpoch 63500: Loss = 0.6464\nEpoch 63600: Loss = 0.6464\nEpoch 63700: Loss = 0.6464\nEpoch 63800: Loss = 0.6464\nEpoch 63900: Loss = 0.6464\nEpoch 64000: Loss = 0.6464\nEpoch 64100: Loss = 0.6464\nEpoch 64200: Loss = 0.6464\nEpoch 64300: Loss = 0.6464\nEpoch 64400: Loss = 0.6464\nEpoch 64500: Loss = 0.6464\nEpoch 64600: Loss = 0.6464\nEpoch 64700: Loss = 0.6464\nEpoch 64800: Loss = 0.6464\nEpoch 64900: Loss = 0.6464\nEpoch 65000: Loss = 0.6464\nEpoch 65100: Loss = 0.6464\nEpoch 65200: Loss = 0.6463\nEpoch 65300: Loss = 0.6463\nEpoch 65400: Loss = 0.6463\nEpoch 65500: Loss = 0.6463\nEpoch 65600: Loss = 0.6463\nEpoch 65700: Loss = 0.6463\nEpoch 65800: Loss = 0.6463\nEpoch 65900: Loss = 0.6463\nEpoch 66000: Loss = 0.6463\nEpoch 66100: Loss = 0.6463\nEpoch 66200: Loss = 0.6463\nEpoch 66300: Loss = 0.6463\nEpoch 66400: Loss = 0.6463\nEpoch 66500: Loss = 0.6463\nEpoch 66600: Loss = 0.6463\nEpoch 66700: Loss = 0.6463\nEpoch 66800: Loss = 0.6463\nEpoch 66900: Loss = 0.6462\nEpoch 67000: Loss = 0.6462\nEpoch 67100: Loss = 0.6462\nEpoch 67200: Loss = 0.6462\nEpoch 67300: Loss = 0.6462\nEpoch 67400: Loss = 0.6462\nEpoch 67500: Loss = 0.6462\nEpoch 67600: Loss = 0.6462\nEpoch 67700: Loss = 0.6462\nEpoch 67800: Loss = 0.6462\nEpoch 67900: Loss = 0.6462\nEpoch 68000: Loss = 0.6462\nEpoch 68100: Loss = 0.6462\nEpoch 68200: Loss = 0.6462\nEpoch 68300: Loss = 0.6462\nEpoch 68400: Loss = 0.6462\nEpoch 68500: Loss = 0.6462\nEpoch 68600: Loss = 0.6462\nEpoch 68700: Loss = 0.6462\nEpoch 68800: Loss = 0.6462\nEpoch 68900: Loss = 0.6461\nEpoch 69000: Loss = 0.6461\nEpoch 69100: Loss = 0.6461\nEpoch 69200: Loss = 0.6461\nEpoch 69300: Loss = 0.6461\nEpoch 69400: Loss = 0.6461\nEpoch 69500: Loss = 0.6461\nEpoch 69600: Loss = 0.6461\nEpoch 69700: Loss = 0.6461\nEpoch 69800: Loss = 0.6461\nEpoch 69900: Loss = 0.6461\nEpoch 70000: Loss = 0.6461\nEpoch 70100: Loss = 0.6461\nEpoch 70200: Loss = 0.6461\nEpoch 70300: Loss = 0.6461\nEpoch 70400: Loss = 0.6461\nEpoch 70500: Loss = 0.6461\nEpoch 70600: Loss = 0.6461\nEpoch 70700: Loss = 0.6461\nEpoch 70800: Loss = 0.6461\nEpoch 70900: Loss = 0.6461\nEpoch 71000: Loss = 0.6460\nEpoch 71100: Loss = 0.6460\nEpoch 71200: Loss = 0.6460\nEpoch 71300: Loss = 0.6460\nEpoch 71400: Loss = 0.6460\nEpoch 71500: Loss = 0.6460\nEpoch 71600: Loss = 0.6460\nEpoch 71700: Loss = 0.6460\nEpoch 71800: Loss = 0.6460\nEpoch 71900: Loss = 0.6460\nEpoch 72000: Loss = 0.6460\nEpoch 72100: Loss = 0.6460\nEpoch 72200: Loss = 0.6460\nEpoch 72300: Loss = 0.6460\nEpoch 72400: Loss = 0.6460\nEpoch 72500: Loss = 0.6460\nEpoch 72600: Loss = 0.6460\nEpoch 72700: Loss = 0.6460\nEpoch 72800: Loss = 0.6460\nEpoch 72900: Loss = 0.6460\nEpoch 73000: Loss = 0.6460\nEpoch 73100: Loss = 0.6460\nEpoch 73200: Loss = 0.6460\nEpoch 73300: Loss = 0.6459\nEpoch 73400: Loss = 0.6459\nEpoch 73500: Loss = 0.6459\nEpoch 73600: Loss = 0.6459\nEpoch 73700: Loss = 0.6459\nEpoch 73800: Loss = 0.6459\nEpoch 73900: Loss = 0.6459\nEpoch 74000: Loss = 0.6459\nEpoch 74100: Loss = 0.6459\nEpoch 74200: Loss = 0.6459\nEpoch 74300: Loss = 0.6459\nEpoch 74400: Loss = 0.6459\nEpoch 74500: Loss = 0.6459\nEpoch 74600: Loss = 0.6459\nEpoch 74700: Loss = 0.6459\nEpoch 74800: Loss = 0.6459\nEpoch 74900: Loss = 0.6459\nEpoch 75000: Loss = 0.6459\nEpoch 75100: Loss = 0.6459\nEpoch 75200: Loss = 0.6459\nEpoch 75300: Loss = 0.6459\nEpoch 75400: Loss = 0.6459\nEpoch 75500: Loss = 0.6459\nEpoch 75600: Loss = 0.6458\nEpoch 75700: Loss = 0.6458\nEpoch 75800: Loss = 0.6458\nEpoch 75900: Loss = 0.6458\nEpoch 76000: Loss = 0.6458\nEpoch 76100: Loss = 0.6458\nEpoch 76200: Loss = 0.6458\nEpoch 76300: Loss = 0.6458\nEpoch 76400: Loss = 0.6458\nEpoch 76500: Loss = 0.6458\nEpoch 76600: Loss = 0.6458\nEpoch 76700: Loss = 0.6458\nEpoch 76800: Loss = 0.6458\nEpoch 76900: Loss = 0.6458\nEpoch 77000: Loss = 0.6458\nEpoch 77100: Loss = 0.6458\nEpoch 77200: Loss = 0.6458\nEpoch 77300: Loss = 0.6458\nEpoch 77400: Loss = 0.6458\nEpoch 77500: Loss = 0.6458\nEpoch 77600: Loss = 0.6458\nEpoch 77700: Loss = 0.6458\nEpoch 77800: Loss = 0.6458\nEpoch 77900: Loss = 0.6458\nEpoch 78000: Loss = 0.6458\nEpoch 78100: Loss = 0.6457\nEpoch 78200: Loss = 0.6457\nEpoch 78300: Loss = 0.6457\nEpoch 78400: Loss = 0.6457\nEpoch 78500: Loss = 0.6457\nEpoch 78600: Loss = 0.6457\nEpoch 78700: Loss = 0.6457\nEpoch 78800: Loss = 0.6457\nEpoch 78900: Loss = 0.6457\nEpoch 79000: Loss = 0.6457\nEpoch 79100: Loss = 0.6457\nEpoch 79200: Loss = 0.6457\nEpoch 79300: Loss = 0.6457\nEpoch 79400: Loss = 0.6457\nEpoch 79500: Loss = 0.6457\nEpoch 79600: Loss = 0.6457\nEpoch 79700: Loss = 0.6457\nEpoch 79800: Loss = 0.6457\nEpoch 79900: Loss = 0.6457\nEpoch 80000: Loss = 0.6457\nEpoch 80100: Loss = 0.6457\nEpoch 80200: Loss = 0.6457\nEpoch 80300: Loss = 0.6457\nEpoch 80400: Loss = 0.6457\nEpoch 80500: Loss = 0.6456\nEpoch 80600: Loss = 0.6456\nEpoch 80700: Loss = 0.6456\nEpoch 80800: Loss = 0.6456\nEpoch 80900: Loss = 0.6456\nEpoch 81000: Loss = 0.6456\nEpoch 81100: Loss = 0.6456\nEpoch 81200: Loss = 0.6456\nEpoch 81300: Loss = 0.6456\nEpoch 81400: Loss = 0.6456\nEpoch 81500: Loss = 0.6456\nEpoch 81600: Loss = 0.6456\nEpoch 81700: Loss = 0.6456\nEpoch 81800: Loss = 0.6456\nEpoch 81900: Loss = 0.6456\nEpoch 82000: Loss = 0.6456\nEpoch 82100: Loss = 0.6456\nEpoch 82200: Loss = 0.6456\nEpoch 82300: Loss = 0.6456\nEpoch 82400: Loss = 0.6456\nEpoch 82500: Loss = 0.6456\nEpoch 82600: Loss = 0.6456\nEpoch 82700: Loss = 0.6456\nEpoch 82800: Loss = 0.6456\nEpoch 82900: Loss = 0.6456\nEpoch 83000: Loss = 0.6456\nEpoch 83100: Loss = 0.6456\nEpoch 83200: Loss = 0.6455\nEpoch 83300: Loss = 0.6455\nEpoch 83400: Loss = 0.6455\nEpoch 83500: Loss = 0.6455\nEpoch 83600: Loss = 0.6455\nEpoch 83700: Loss = 0.6455\nEpoch 83800: Loss = 0.6455\nEpoch 83900: Loss = 0.6455\nEpoch 84000: Loss = 0.6455\nEpoch 84100: Loss = 0.6455\nEpoch 84200: Loss = 0.6455\nEpoch 84300: Loss = 0.6455\nEpoch 84400: Loss = 0.6455\nEpoch 84500: Loss = 0.6455\nEpoch 84600: Loss = 0.6455\nEpoch 84700: Loss = 0.6455\nEpoch 84800: Loss = 0.6455\nEpoch 84900: Loss = 0.6455\nEpoch 85000: Loss = 0.6455\nEpoch 85100: Loss = 0.6455\nEpoch 85200: Loss = 0.6455\nEpoch 85300: Loss = 0.6455\nEpoch 85400: Loss = 0.6455\nEpoch 85500: Loss = 0.6455\nEpoch 85600: Loss = 0.6455\nEpoch 85700: Loss = 0.6455\nEpoch 85800: Loss = 0.6455\nEpoch 85900: Loss = 0.6455\nEpoch 86000: Loss = 0.6455\nEpoch 86100: Loss = 0.6455\nEpoch 86200: Loss = 0.6454\nEpoch 86300: Loss = 0.6454\nEpoch 86400: Loss = 0.6454\nEpoch 86500: Loss = 0.6454\nEpoch 86600: Loss = 0.6454\nEpoch 86700: Loss = 0.6454\nEpoch 86800: Loss = 0.6454\nEpoch 86900: Loss = 0.6454\nEpoch 87000: Loss = 0.6454\nEpoch 87100: Loss = 0.6454\nEpoch 87200: Loss = 0.6454\nEpoch 87300: Loss = 0.6454\nEpoch 87400: Loss = 0.6454\nEpoch 87500: Loss = 0.6454\nEpoch 87600: Loss = 0.6454\nEpoch 87700: Loss = 0.6454\nEpoch 87800: Loss = 0.6454\nEpoch 87900: Loss = 0.6454\nEpoch 88000: Loss = 0.6454\nEpoch 88100: Loss = 0.6454\nEpoch 88200: Loss = 0.6454\nEpoch 88300: Loss = 0.6454\nEpoch 88400: Loss = 0.6454\nEpoch 88500: Loss = 0.6454\nEpoch 88600: Loss = 0.6454\nEpoch 88700: Loss = 0.6454\nEpoch 88800: Loss = 0.6454\nEpoch 88900: Loss = 0.6454\nEpoch 89000: Loss = 0.6454\nEpoch 89100: Loss = 0.6454\nEpoch 89200: Loss = 0.6454\nEpoch 89300: Loss = 0.6453\nEpoch 89400: Loss = 0.6453\nEpoch 89500: Loss = 0.6453\nEpoch 89600: Loss = 0.6453\nEpoch 89700: Loss = 0.6453\nEpoch 89800: Loss = 0.6453\nEpoch 89900: Loss = 0.6453\nEpoch 90000: Loss = 0.6453\nEpoch 90100: Loss = 0.6453\nEpoch 90200: Loss = 0.6453\nEpoch 90300: Loss = 0.6453\nEpoch 90400: Loss = 0.6453\nEpoch 90500: Loss = 0.6453\nEpoch 90600: Loss = 0.6453\nEpoch 90700: Loss = 0.6453\nEpoch 90800: Loss = 0.6453\nEpoch 90900: Loss = 0.6453\nEpoch 91000: Loss = 0.6453\nEpoch 91100: Loss = 0.6453\nEpoch 91200: Loss = 0.6453\nEpoch 91300: Loss = 0.6453\nEpoch 91400: Loss = 0.6453\nEpoch 91500: Loss = 0.6453\nEpoch 91600: Loss = 0.6453\nEpoch 91700: Loss = 0.6453\nEpoch 91800: Loss = 0.6453\nEpoch 91900: Loss = 0.6453\nEpoch 92000: Loss = 0.6453\nEpoch 92100: Loss = 0.6453\nEpoch 92200: Loss = 0.6453\nEpoch 92300: Loss = 0.6452\nEpoch 92400: Loss = 0.6453\nEpoch 92500: Loss = 0.6452\nEpoch 92600: Loss = 0.6452\nEpoch 92700: Loss = 0.6452\nEpoch 92800: Loss = 0.6452\nEpoch 92900: Loss = 0.6452\nEpoch 93000: Loss = 0.6452\nEpoch 93100: Loss = 0.6452\nEpoch 93200: Loss = 0.6452\nEpoch 93300: Loss = 0.6452\nEpoch 93400: Loss = 0.6452\nEpoch 93500: Loss = 0.6452\nEpoch 93600: Loss = 0.6452\nEpoch 93700: Loss = 0.6452\nEpoch 93800: Loss = 0.6452\nEpoch 93900: Loss = 0.6452\nEpoch 94000: Loss = 0.6452\nEpoch 94100: Loss = 0.6452\nEpoch 94200: Loss = 0.6452\nEpoch 94300: Loss = 0.6452\nEpoch 94400: Loss = 0.6452\nEpoch 94500: Loss = 0.6452\nEpoch 94600: Loss = 0.6452\nEpoch 94700: Loss = 0.6452\nEpoch 94800: Loss = 0.6452\nEpoch 94900: Loss = 0.6452\nEpoch 95000: Loss = 0.6452\nEpoch 95100: Loss = 0.6452\nEpoch 95200: Loss = 0.6452\nEpoch 95300: Loss = 0.6452\nEpoch 95400: Loss = 0.6452\nEpoch 95500: Loss = 0.6452\nEpoch 95600: Loss = 0.6451\nEpoch 95700: Loss = 0.6451\nEpoch 95800: Loss = 0.6451\nEpoch 95900: Loss = 0.6451\nEpoch 96000: Loss = 0.6451\nEpoch 96100: Loss = 0.6451\nEpoch 96200: Loss = 0.6451\nEpoch 96300: Loss = 0.6451\nEpoch 96400: Loss = 0.6451\nEpoch 96500: Loss = 0.6451\nEpoch 96600: Loss = 0.6451\nEpoch 96700: Loss = 0.6451\nEpoch 96800: Loss = 0.6451\nEpoch 96900: Loss = 0.6451\nEpoch 97000: Loss = 0.6451\nEpoch 97100: Loss = 0.6451\nEpoch 97200: Loss = 0.6451\nEpoch 97300: Loss = 0.6451\nEpoch 97400: Loss = 0.6451\nEpoch 97500: Loss = 0.6451\nEpoch 97600: Loss = 0.6451\nEpoch 97700: Loss = 0.6451\nEpoch 97800: Loss = 0.6451\nEpoch 97900: Loss = 0.6451\nEpoch 98000: Loss = 0.6451\nEpoch 98100: Loss = 0.6451\nEpoch 98200: Loss = 0.6451\nEpoch 98300: Loss = 0.6451\nEpoch 98400: Loss = 0.6451\nEpoch 98500: Loss = 0.6451\nEpoch 98600: Loss = 0.6451\nEpoch 98700: Loss = 0.6451\nEpoch 98800: Loss = 0.6450\nEpoch 98900: Loss = 0.6450\nEpoch 99000: Loss = 0.6450\nEpoch 99100: Loss = 0.6450\nEpoch 99200: Loss = 0.6450\nEpoch 99300: Loss = 0.6450\nEpoch 99400: Loss = 0.6450\nEpoch 99500: Loss = 0.6450\nEpoch 99600: Loss = 0.6450\nEpoch 99700: Loss = 0.6450\nEpoch 99800: Loss = 0.6450\nEpoch 99900: Loss = 0.6450\nEpoch 100000: Loss = 0.6450\nEpoch 100100: Loss = 0.6450\nEpoch 100200: Loss = 0.6450\nEpoch 100300: Loss = 0.6450\nEpoch 100400: Loss = 0.6450\nEpoch 100500: Loss = 0.6450\nEpoch 100600: Loss = 0.6450\nEpoch 100700: Loss = 0.6450\nEpoch 100800: Loss = 0.6450\nEpoch 100900: Loss = 0.6450\nEpoch 101000: Loss = 0.6450\nEpoch 101100: Loss = 0.6450\nEpoch 101200: Loss = 0.6450\nEpoch 101300: Loss = 0.6450\nEpoch 101400: Loss = 0.6450\nEpoch 101500: Loss = 0.6450\nEpoch 101600: Loss = 0.6450\nEpoch 101700: Loss = 0.6450\nEpoch 101800: Loss = 0.6450\nEpoch 101900: Loss = 0.6450\nEpoch 102000: Loss = 0.6450\nEpoch 102100: Loss = 0.6450\nEpoch 102200: Loss = 0.6450\nEpoch 102300: Loss = 0.6450\nEpoch 102400: Loss = 0.6449\nEpoch 102500: Loss = 0.6449\nEpoch 102600: Loss = 0.6449\nEpoch 102700: Loss = 0.6449\nEpoch 102800: Loss = 0.6449\nEpoch 102900: Loss = 0.6449\nEpoch 103000: Loss = 0.6449\nEpoch 103100: Loss = 0.6449\nEpoch 103200: Loss = 0.6449\nEpoch 103300: Loss = 0.6449\nEpoch 103400: Loss = 0.6449\nEpoch 103500: Loss = 0.6449\nEpoch 103600: Loss = 0.6449\nEpoch 103700: Loss = 0.6449\nEpoch 103800: Loss = 0.6449\nEpoch 103900: Loss = 0.6449\nEpoch 104000: Loss = 0.6449\nEpoch 104100: Loss = 0.6449\nEpoch 104200: Loss = 0.6449\nEpoch 104300: Loss = 0.6449\nEpoch 104400: Loss = 0.6449\nEpoch 104500: Loss = 0.6449\nEpoch 104600: Loss = 0.6449\nEpoch 104700: Loss = 0.6449\nEpoch 104800: Loss = 0.6449\nEpoch 104900: Loss = 0.6449\nEpoch 105000: Loss = 0.6449\nEpoch 105100: Loss = 0.6449\nEpoch 105200: Loss = 0.6449\nEpoch 105300: Loss = 0.6449\nEpoch 105400: Loss = 0.6449\nEpoch 105500: Loss = 0.6449\nEpoch 105600: Loss = 0.6449\nEpoch 105700: Loss = 0.6449\nEpoch 105800: Loss = 0.6449\nEpoch 105900: Loss = 0.6449\nEpoch 106000: Loss = 0.6448\nEpoch 106100: Loss = 0.6448\nEpoch 106200: Loss = 0.6448\nEpoch 106300: Loss = 0.6448\nEpoch 106400: Loss = 0.6448\nEpoch 106500: Loss = 0.6448\nEpoch 106600: Loss = 0.6448\nEpoch 106700: Loss = 0.6448\nEpoch 106800: Loss = 0.6448\nEpoch 106900: Loss = 0.6448\nEpoch 107000: Loss = 0.6448\nEpoch 107100: Loss = 0.6448\nEpoch 107200: Loss = 0.6448\nEpoch 107300: Loss = 0.6448\nEpoch 107400: Loss = 0.6448\nEpoch 107500: Loss = 0.6448\nEpoch 107600: Loss = 0.6448\nEpoch 107700: Loss = 0.6448\nEpoch 107800: Loss = 0.6448\nEpoch 107900: Loss = 0.6448\nEpoch 108000: Loss = 0.6448\nEpoch 108100: Loss = 0.6448\nEpoch 108200: Loss = 0.6448\nEpoch 108300: Loss = 0.6448\nEpoch 108400: Loss = 0.6448\nEpoch 108500: Loss = 0.6448\nEpoch 108600: Loss = 0.6448\nEpoch 108700: Loss = 0.6448\nEpoch 108800: Loss = 0.6448\nEpoch 108900: Loss = 0.6448\nEpoch 109000: Loss = 0.6448\nEpoch 109100: Loss = 0.6448\nEpoch 109200: Loss = 0.6448\nEpoch 109300: Loss = 0.6448\nEpoch 109400: Loss = 0.6448\nEpoch 109500: Loss = 0.6448\nEpoch 109600: Loss = 0.6448\nEpoch 109700: Loss = 0.6447\nEpoch 109800: Loss = 0.6447\nEpoch 109900: Loss = 0.6447\nEpoch 110000: Loss = 0.6447\nEpoch 110100: Loss = 0.6447\nEpoch 110200: Loss = 0.6447\nEpoch 110300: Loss = 0.6447\nEpoch 110400: Loss = 0.6447\nEpoch 110500: Loss = 0.6447\nEpoch 110600: Loss = 0.6447\nEpoch 110700: Loss = 0.6447\nEpoch 110800: Loss = 0.6447\nEpoch 110900: Loss = 0.6447\nEpoch 111000: Loss = 0.6447\nEpoch 111100: Loss = 0.6447\nEpoch 111200: Loss = 0.6447\nEpoch 111300: Loss = 0.6447\nEpoch 111400: Loss = 0.6447\nEpoch 111500: Loss = 0.6447\nEpoch 111600: Loss = 0.6447\nEpoch 111700: Loss = 0.6447\nEpoch 111800: Loss = 0.6447\nEpoch 111900: Loss = 0.6447\nEpoch 112000: Loss = 0.6447\nEpoch 112100: Loss = 0.6447\nEpoch 112200: Loss = 0.6447\nEpoch 112300: Loss = 0.6447\nEpoch 112400: Loss = 0.6447\nEpoch 112500: Loss = 0.6447\nEpoch 112600: Loss = 0.6447\nEpoch 112700: Loss = 0.6447\nEpoch 112800: Loss = 0.6447\nEpoch 112900: Loss = 0.6447\nEpoch 113000: Loss = 0.6447\nEpoch 113100: Loss = 0.6447\nEpoch 113200: Loss = 0.6447\nEpoch 113300: Loss = 0.6447\nEpoch 113400: Loss = 0.6447\nEpoch 113500: Loss = 0.6446\nEpoch 113600: Loss = 0.6446\nEpoch 113700: Loss = 0.6446\nEpoch 113800: Loss = 0.6446\nEpoch 113900: Loss = 0.6446\nEpoch 114000: Loss = 0.6446\nEpoch 114100: Loss = 0.6446\nEpoch 114200: Loss = 0.6446\nEpoch 114300: Loss = 0.6446\nEpoch 114400: Loss = 0.6446\nEpoch 114500: Loss = 0.6446\nEpoch 114600: Loss = 0.6446\nEpoch 114700: Loss = 0.6446\nEpoch 114800: Loss = 0.6446\nEpoch 114900: Loss = 0.6446\nEpoch 115000: Loss = 0.6446\nEpoch 115100: Loss = 0.6446\nEpoch 115200: Loss = 0.6446\nEpoch 115300: Loss = 0.6446\nEpoch 115400: Loss = 0.6446\nEpoch 115500: Loss = 0.6446\nEpoch 115600: Loss = 0.6446\nEpoch 115700: Loss = 0.6446\nEpoch 115800: Loss = 0.6446\nEpoch 115900: Loss = 0.6446\nEpoch 116000: Loss = 0.6446\nEpoch 116100: Loss = 0.6446\nEpoch 116200: Loss = 0.6446\nEpoch 116300: Loss = 0.6446\nEpoch 116400: Loss = 0.6446\nEpoch 116500: Loss = 0.6446\nEpoch 116600: Loss = 0.6446\nEpoch 116700: Loss = 0.6446\nEpoch 116800: Loss = 0.6446\nEpoch 116900: Loss = 0.6446\nEpoch 117000: Loss = 0.6446\nEpoch 117100: Loss = 0.6446\nEpoch 117200: Loss = 0.6446\nEpoch 117300: Loss = 0.6446\nEpoch 117400: Loss = 0.6446\nEpoch 117500: Loss = 0.6446\nEpoch 117600: Loss = 0.6446\nEpoch 117700: Loss = 0.6445\nEpoch 117800: Loss = 0.6445\nEpoch 117900: Loss = 0.6445\nEpoch 118000: Loss = 0.6445\nEpoch 118100: Loss = 0.6445\nEpoch 118200: Loss = 0.6445\nEpoch 118300: Loss = 0.6445\nEpoch 118400: Loss = 0.6445\nEpoch 118500: Loss = 0.6445\nEpoch 118600: Loss = 0.6445\nEpoch 118700: Loss = 0.6445\nEpoch 118800: Loss = 0.6445\nEpoch 118900: Loss = 0.6445\nEpoch 119000: Loss = 0.6445\nEpoch 119100: Loss = 0.6445\nEpoch 119200: Loss = 0.6445\nEpoch 119300: Loss = 0.6445\nEpoch 119400: Loss = 0.6445\nEpoch 119500: Loss = 0.6445\nEpoch 119600: Loss = 0.6445\nEpoch 119700: Loss = 0.6445\nEpoch 119800: Loss = 0.6445\nEpoch 119900: Loss = 0.6445\nEpoch 120000: Loss = 0.6445\nEpoch 120100: Loss = 0.6445\nEpoch 120200: Loss = 0.6445\nEpoch 120300: Loss = 0.6445\nEpoch 120400: Loss = 0.6445\nEpoch 120500: Loss = 0.6445\nEpoch 120600: Loss = 0.6445\nEpoch 120700: Loss = 0.6445\nEpoch 120800: Loss = 0.6445\nEpoch 120900: Loss = 0.6445\nEpoch 121000: Loss = 0.6445\nEpoch 121100: Loss = 0.6445\nEpoch 121200: Loss = 0.6445\nEpoch 121300: Loss = 0.6445\nEpoch 121400: Loss = 0.6445\nEpoch 121500: Loss = 0.6445\nEpoch 121600: Loss = 0.6445\nEpoch 121700: Loss = 0.6445\nEpoch 121800: Loss = 0.6444\nEpoch 121900: Loss = 0.6444\nEpoch 122000: Loss = 0.6444\nEpoch 122100: Loss = 0.6444\nEpoch 122200: Loss = 0.6444\nEpoch 122300: Loss = 0.6444\nEpoch 122400: Loss = 0.6444\nEpoch 122500: Loss = 0.6444\nEpoch 122600: Loss = 0.6444\nEpoch 122700: Loss = 0.6444\nEpoch 122800: Loss = 0.6444\nEpoch 122900: Loss = 0.6444\nEpoch 123000: Loss = 0.6444\nEpoch 123100: Loss = 0.6444\nEpoch 123200: Loss = 0.6444\nEpoch 123300: Loss = 0.6444\nEpoch 123400: Loss = 0.6444\nEpoch 123500: Loss = 0.6444\nEpoch 123600: Loss = 0.6444\nEpoch 123700: Loss = 0.6444\nEpoch 123800: Loss = 0.6444\nEpoch 123900: Loss = 0.6444\nEpoch 124000: Loss = 0.6444\nEpoch 124100: Loss = 0.6444\nEpoch 124200: Loss = 0.6444\nEpoch 124300: Loss = 0.6444\nEpoch 124400: Loss = 0.6444\nEpoch 124500: Loss = 0.6444\nEpoch 124600: Loss = 0.6444\nEpoch 124700: Loss = 0.6444\nEpoch 124800: Loss = 0.6444\nEpoch 124900: Loss = 0.6444\nEpoch 125000: Loss = 0.6444\nEpoch 125100: Loss = 0.6444\nEpoch 125200: Loss = 0.6444\nEpoch 125300: Loss = 0.6444\nEpoch 125400: Loss = 0.6444\nEpoch 125500: Loss = 0.6444\nEpoch 125600: Loss = 0.6444\nEpoch 125700: Loss = 0.6444\nEpoch 125800: Loss = 0.6444\nEpoch 125900: Loss = 0.6443\nEpoch 126000: Loss = 0.6443\nEpoch 126100: Loss = 0.6443\nEpoch 126200: Loss = 0.6443\nEpoch 126300: Loss = 0.6443\nEpoch 126400: Loss = 0.6443\nEpoch 126500: Loss = 0.6443\nEpoch 126600: Loss = 0.6443\nEpoch 126700: Loss = 0.6443\nEpoch 126800: Loss = 0.6443\nEpoch 126900: Loss = 0.6443\nEpoch 127000: Loss = 0.6443\nEpoch 127100: Loss = 0.6443\nEpoch 127200: Loss = 0.6443\nEpoch 127300: Loss = 0.6443\nEpoch 127400: Loss = 0.6443\nEpoch 127500: Loss = 0.6443\nEpoch 127600: Loss = 0.6443\nEpoch 127700: Loss = 0.6443\nEpoch 127800: Loss = 0.6443\nEpoch 127900: Loss = 0.6443\nEpoch 128000: Loss = 0.6443\nEpoch 128100: Loss = 0.6443\nEpoch 128200: Loss = 0.6443\nEpoch 128300: Loss = 0.6443\nEpoch 128400: Loss = 0.6443\nEpoch 128500: Loss = 0.6443\nEpoch 128600: Loss = 0.6443\nEpoch 128700: Loss = 0.6443\nEpoch 128800: Loss = 0.6443\nEpoch 128900: Loss = 0.6443\nEpoch 129000: Loss = 0.6443\nEpoch 129100: Loss = 0.6443\nEpoch 129200: Loss = 0.6443\nEpoch 129300: Loss = 0.6443\nEpoch 129400: Loss = 0.6443\nEpoch 129500: Loss = 0.6443\nEpoch 129600: Loss = 0.6443\nEpoch 129700: Loss = 0.6443\nEpoch 129800: Loss = 0.6443\nEpoch 129900: Loss = 0.6443\nEpoch 130000: Loss = 0.6443\nEpoch 130100: Loss = 0.6443\nEpoch 130200: Loss = 0.6442\nEpoch 130300: Loss = 0.6442\nEpoch 130400: Loss = 0.6442\nEpoch 130500: Loss = 0.6442\nEpoch 130600: Loss = 0.6442\nEpoch 130700: Loss = 0.6442\nEpoch 130800: Loss = 0.6442\nEpoch 130900: Loss = 0.6442\nEpoch 131000: Loss = 0.6442\nEpoch 131100: Loss = 0.6442\nEpoch 131200: Loss = 0.6442\nEpoch 131300: Loss = 0.6442\nEpoch 131400: Loss = 0.6442\nEpoch 131500: Loss = 0.6442\nEpoch 131600: Loss = 0.6442\nEpoch 131700: Loss = 0.6442\nEpoch 131800: Loss = 0.6442\nEpoch 131900: Loss = 0.6442\nEpoch 132000: Loss = 0.6442\nEpoch 132100: Loss = 0.6442\nEpoch 132200: Loss = 0.6442\nEpoch 132300: Loss = 0.6442\nEpoch 132400: Loss = 0.6442\nEpoch 132500: Loss = 0.6442\nEpoch 132600: Loss = 0.6442\nEpoch 132700: Loss = 0.6442\nEpoch 132800: Loss = 0.6442\nEpoch 132900: Loss = 0.6442\nEpoch 133000: Loss = 0.6442\nEpoch 133100: Loss = 0.6442\nEpoch 133200: Loss = 0.6442\nEpoch 133300: Loss = 0.6442\nEpoch 133400: Loss = 0.6442\nEpoch 133500: Loss = 0.6442\nEpoch 133600: Loss = 0.6442\nEpoch 133700: Loss = 0.6442\nEpoch 133800: Loss = 0.6442\nEpoch 133900: Loss = 0.6442\nEpoch 134000: Loss = 0.6442\nEpoch 134100: Loss = 0.6442\nEpoch 134200: Loss = 0.6442\nEpoch 134300: Loss = 0.6441\nEpoch 134400: Loss = 0.6441\nEpoch 134500: Loss = 0.6441\nEpoch 134600: Loss = 0.6441\nEpoch 134700: Loss = 0.6441\nEpoch 134800: Loss = 0.6441\nEpoch 134900: Loss = 0.6441\nEpoch 135000: Loss = 0.6441\nEpoch 135100: Loss = 0.6441\nEpoch 135200: Loss = 0.6441\nEpoch 135300: Loss = 0.6441\nEpoch 135400: Loss = 0.6441\nEpoch 135500: Loss = 0.6441\nEpoch 135600: Loss = 0.6441\nEpoch 135700: Loss = 0.6441\nEpoch 135800: Loss = 0.6441\nEpoch 135900: Loss = 0.6441\nEpoch 136000: Loss = 0.6441\nEpoch 136100: Loss = 0.6441\nEpoch 136200: Loss = 0.6441\nEpoch 136300: Loss = 0.6441\nEpoch 136400: Loss = 0.6441\nEpoch 136500: Loss = 0.6441\nEpoch 136600: Loss = 0.6441\nEpoch 136700: Loss = 0.6441\nEpoch 136800: Loss = 0.6441\nEpoch 136900: Loss = 0.6441\nEpoch 137000: Loss = 0.6441\nEpoch 137100: Loss = 0.6441\nEpoch 137200: Loss = 0.6441\nEpoch 137300: Loss = 0.6441\nEpoch 137400: Loss = 0.6441\nEpoch 137500: Loss = 0.6441\nEpoch 137600: Loss = 0.6441\nEpoch 137700: Loss = 0.6441\nEpoch 137800: Loss = 0.6441\nEpoch 137900: Loss = 0.6441\nEpoch 138000: Loss = 0.6441\nEpoch 138100: Loss = 0.6441\nEpoch 138200: Loss = 0.6441\nEpoch 138300: Loss = 0.6441\nEpoch 138400: Loss = 0.6441\nEpoch 138500: Loss = 0.6440\nEpoch 138600: Loss = 0.6440\nEpoch 138700: Loss = 0.6440\nEpoch 138800: Loss = 0.6440\nEpoch 138900: Loss = 0.6440\nEpoch 139000: Loss = 0.6440\nEpoch 139100: Loss = 0.6440\nEpoch 139200: Loss = 0.6440\nEpoch 139300: Loss = 0.6440\nEpoch 139400: Loss = 0.6440\nEpoch 139500: Loss = 0.6440\nEpoch 139600: Loss = 0.6440\nEpoch 139700: Loss = 0.6440\nEpoch 139800: Loss = 0.6440\nEpoch 139900: Loss = 0.6440\nEpoch 140000: Loss = 0.6440\nEpoch 140100: Loss = 0.6440\nEpoch 140200: Loss = 0.6440\nEpoch 140300: Loss = 0.6440\nEpoch 140400: Loss = 0.6440\nEpoch 140500: Loss = 0.6440\nEpoch 140600: Loss = 0.6440\nEpoch 140700: Loss = 0.6440\nEpoch 140800: Loss = 0.6440\nEpoch 140900: Loss = 0.6440\nEpoch 141000: Loss = 0.6440\nEpoch 141100: Loss = 0.6440\nEpoch 141200: Loss = 0.6440\nEpoch 141300: Loss = 0.6440\nEpoch 141400: Loss = 0.6440\nEpoch 141500: Loss = 0.6440\nEpoch 141600: Loss = 0.6440\nEpoch 141700: Loss = 0.6440\nEpoch 141800: Loss = 0.6440\nEpoch 141900: Loss = 0.6440\nEpoch 142000: Loss = 0.6440\nEpoch 142100: Loss = 0.6440\nEpoch 142200: Loss = 0.6440\nEpoch 142300: Loss = 0.6440\nEpoch 142400: Loss = 0.6440\nEpoch 142500: Loss = 0.6440\nEpoch 142600: Loss = 0.6440\nEpoch 142700: Loss = 0.6440\nEpoch 142800: Loss = 0.6440\nEpoch 142900: Loss = 0.6440\nEpoch 143000: Loss = 0.6440\nEpoch 143100: Loss = 0.6440\nEpoch 143200: Loss = 0.6440\nEpoch 143300: Loss = 0.6440\nEpoch 143400: Loss = 0.6440\nEpoch 143500: Loss = 0.6440\nEpoch 143600: Loss = 0.6440\nEpoch 143700: Loss = 0.6440\nEpoch 143800: Loss = 0.6440\nEpoch 143900: Loss = 0.6440\nEpoch 144000: Loss = 0.6439\nEpoch 144100: Loss = 0.6439\nEpoch 144200: Loss = 0.6439\nEpoch 144300: Loss = 0.6439\nEpoch 144400: Loss = 0.6439\nEpoch 144500: Loss = 0.6439\nEpoch 144600: Loss = 0.6439\nEpoch 144700: Loss = 0.6439\nEpoch 144800: Loss = 0.6439\nEpoch 144900: Loss = 0.6439\nEpoch 145000: Loss = 0.6439\nEpoch 145100: Loss = 0.6439\nEpoch 145200: Loss = 0.6439\nEpoch 145300: Loss = 0.6439\nEpoch 145400: Loss = 0.6439\nEpoch 145500: Loss = 0.6439\nEpoch 145600: Loss = 0.6439\nEpoch 145700: Loss = 0.6439\nEpoch 145800: Loss = 0.6439\nEpoch 145900: Loss = 0.6439\nEpoch 146000: Loss = 0.6439\nEpoch 146100: Loss = 0.6439\nEpoch 146200: Loss = 0.6439\nEpoch 146300: Loss = 0.6439\nEpoch 146400: Loss = 0.6439\nEpoch 146500: Loss = 0.6439\nEpoch 146600: Loss = 0.6439\nEpoch 146700: Loss = 0.6439\nEpoch 146800: Loss = 0.6439\nEpoch 146900: Loss = 0.6439\nEpoch 147000: Loss = 0.6439\nEpoch 147100: Loss = 0.6439\nEpoch 147200: Loss = 0.6439\nEpoch 147300: Loss = 0.6439\nEpoch 147400: Loss = 0.6439\nEpoch 147500: Loss = 0.6439\nEpoch 147600: Loss = 0.6439\nEpoch 147700: Loss = 0.6439\nEpoch 147800: Loss = 0.6439\nEpoch 147900: Loss = 0.6439\nEpoch 148000: Loss = 0.6439\nEpoch 148100: Loss = 0.6439\nEpoch 148200: Loss = 0.6439\nEpoch 148300: Loss = 0.6439\nEpoch 148400: Loss = 0.6439\nEpoch 148500: Loss = 0.6439\nEpoch 148600: Loss = 0.6439\nEpoch 148700: Loss = 0.6439\nEpoch 148800: Loss = 0.6439\nEpoch 148900: Loss = 0.6439\nEpoch 149000: Loss = 0.6439\nEpoch 149100: Loss = 0.6439\nEpoch 149200: Loss = 0.6439\nEpoch 149300: Loss = 0.6439\nEpoch 149400: Loss = 0.6439\nEpoch 149500: Loss = 0.6439\nEpoch 149600: Loss = 0.6439\nEpoch 149700: Loss = 0.6439\nEpoch 149800: Loss = 0.6439\nEpoch 149900: Loss = 0.6439\nEpoch 150000: Loss = 0.6439\nEpoch 150100: Loss = 0.6439\nEpoch 150200: Loss = 0.6439\nEpoch 150300: Loss = 0.6439\nEpoch 150400: Loss = 0.6439\nEpoch 150500: Loss = 0.6439\nEpoch 150600: Loss = 0.6439\nEpoch 150700: Loss = 0.6439\nEpoch 150800: Loss = 0.6439\nEpoch 150900: Loss = 0.6439\nEpoch 151000: Loss = 0.6439\nEpoch 151100: Loss = 0.6439\nEpoch 151200: Loss = 0.6439\nEpoch 151300: Loss = 0.6439\nEpoch 151400: Loss = 0.6439\nEpoch 151500: Loss = 0.6439\nEpoch 151600: Loss = 0.6439\nEpoch 151700: Loss = 0.6439\nEpoch 151800: Loss = 0.6439\nEpoch 151900: Loss = 0.6439\nEpoch 152000: Loss = 0.6438\nEpoch 152100: Loss = 0.6439\nEpoch 152200: Loss = 0.6438\nEpoch 152300: Loss = 0.6438\nEpoch 152400: Loss = 0.6438\nEpoch 152500: Loss = 0.6438\nEpoch 152600: Loss = 0.6438\nEpoch 152700: Loss = 0.6438\nEpoch 152800: Loss = 0.6438\nEpoch 152900: Loss = 0.6438\nEpoch 153000: Loss = 0.6438\nEpoch 153100: Loss = 0.6438\nEpoch 153200: Loss = 0.6438\nEpoch 153300: Loss = 0.6438\nEpoch 153400: Loss = 0.6438\nEpoch 153500: Loss = 0.6438\nEpoch 153600: Loss = 0.6438\nEpoch 153700: Loss = 0.6438\nEpoch 153800: Loss = 0.6438\nEpoch 153900: Loss = 0.6438\nEpoch 154000: Loss = 0.6438\nEpoch 154100: Loss = 0.6438\nEpoch 154200: Loss = 0.6438\nEpoch 154300: Loss = 0.6438\nEpoch 154400: Loss = 0.6438\nEpoch 154500: Loss = 0.6438\nEpoch 154600: Loss = 0.6438\nEpoch 154700: Loss = 0.6438\nEpoch 154800: Loss = 0.6438\nEpoch 154900: Loss = 0.6438\nEpoch 155000: Loss = 0.6438\nEpoch 155100: Loss = 0.6438\nEpoch 155200: Loss = 0.6438\nEpoch 155300: Loss = 0.6438\nEpoch 155400: Loss = 0.6438\nEpoch 155500: Loss = 0.6438\nEpoch 155600: Loss = 0.6438\nEpoch 155700: Loss = 0.6438\nEpoch 155800: Loss = 0.6438\nEpoch 155900: Loss = 0.6438\nEpoch 156000: Loss = 0.6438\nEpoch 156100: Loss = 0.6438\nEpoch 156200: Loss = 0.6438\nEpoch 156300: Loss = 0.6438\nEpoch 156400: Loss = 0.6438\nEpoch 156500: Loss = 0.6438\nEpoch 156600: Loss = 0.6438\nEpoch 156700: Loss = 0.6438\nEpoch 156800: Loss = 0.6438\nEpoch 156900: Loss = 0.6438\nEpoch 157000: Loss = 0.6438\nEpoch 157100: Loss = 0.6438\nEpoch 157200: Loss = 0.6438\nEpoch 157300: Loss = 0.6438\nEpoch 157400: Loss = 0.6438\nEpoch 157500: Loss = 0.6438\nEpoch 157600: Loss = 0.6438\nEpoch 157700: Loss = 0.6438\nEpoch 157800: Loss = 0.6438\nEpoch 157900: Loss = 0.6438\nEpoch 158000: Loss = 0.6438\nEpoch 158100: Loss = 0.6438\nEpoch 158200: Loss = 0.6438\nEpoch 158300: Loss = 0.6438\nEpoch 158400: Loss = 0.6438\nEpoch 158500: Loss = 0.6438\nEpoch 158600: Loss = 0.6438\nEpoch 158700: Loss = 0.6438\nEpoch 158800: Loss = 0.6438\nEpoch 158900: Loss = 0.6438\nEpoch 159000: Loss = 0.6438\nEpoch 159100: Loss = 0.6438\nEpoch 159200: Loss = 0.6438\nEpoch 159300: Loss = 0.6438\nEpoch 159400: Loss = 0.6438\nEpoch 159500: Loss = 0.6438\nEpoch 159600: Loss = 0.6438\nEpoch 159700: Loss = 0.6438\nEpoch 159800: Loss = 0.6438\nEpoch 159900: Loss = 0.6438\nEpoch 160000: Loss = 0.6438\nEpoch 160100: Loss = 0.6438\nEpoch 160200: Loss = 0.6438\nEpoch 160300: Loss = 0.6438\nEpoch 160400: Loss = 0.6438\nEpoch 160500: Loss = 0.6438\nEpoch 160600: Loss = 0.6438\nEpoch 160700: Loss = 0.6437\nEpoch 160800: Loss = 0.6438\nEpoch 160900: Loss = 0.6438\nEpoch 161000: Loss = 0.6438\nEpoch 161100: Loss = 0.6437\nEpoch 161200: Loss = 0.6437\nEpoch 161300: Loss = 0.6437\nEpoch 161400: Loss = 0.6437\nEpoch 161500: Loss = 0.6437\nEpoch 161600: Loss = 0.6437\nEpoch 161700: Loss = 0.6437\nEpoch 161800: Loss = 0.6437\nEpoch 161900: Loss = 0.6437\nEpoch 162000: Loss = 0.6437\nEpoch 162100: Loss = 0.6437\nEpoch 162200: Loss = 0.6437\nEpoch 162300: Loss = 0.6437\nEpoch 162400: Loss = 0.6437\nEpoch 162500: Loss = 0.6437\nEpoch 162600: Loss = 0.6437\nEpoch 162700: Loss = 0.6437\nEpoch 162800: Loss = 0.6437\nEpoch 162900: Loss = 0.6437\nEpoch 163000: Loss = 0.6437\nEpoch 163100: Loss = 0.6437\nEpoch 163200: Loss = 0.6437\nEpoch 163300: Loss = 0.6437\nEpoch 163400: Loss = 0.6437\nEpoch 163500: Loss = 0.6437\nEpoch 163600: Loss = 0.6437\nEpoch 163700: Loss = 0.6437\nEpoch 163800: Loss = 0.6437\nEpoch 163900: Loss = 0.6437\nEpoch 164000: Loss = 0.6437\nEpoch 164100: Loss = 0.6437\nEpoch 164200: Loss = 0.6437\nEpoch 164300: Loss = 0.6437\nEpoch 164400: Loss = 0.6437\nEpoch 164500: Loss = 0.6437\nEpoch 164600: Loss = 0.6437\nEpoch 164700: Loss = 0.6437\nEpoch 164800: Loss = 0.6437\nEpoch 164900: Loss = 0.6437\nEpoch 165000: Loss = 0.6437\nEpoch 165100: Loss = 0.6437\nEpoch 165200: Loss = 0.6437\nEpoch 165300: Loss = 0.6437\nEpoch 165400: Loss = 0.6437\nEpoch 165500: Loss = 0.6437\nEpoch 165600: Loss = 0.6437\nEpoch 165700: Loss = 0.6437\nEpoch 165800: Loss = 0.6437\nEpoch 165900: Loss = 0.6437\nEpoch 166000: Loss = 0.6437\nEpoch 166100: Loss = 0.6437\nEpoch 166200: Loss = 0.6437\nEpoch 166300: Loss = 0.6437\nEpoch 166400: Loss = 0.6437\nEpoch 166500: Loss = 0.6437\nEpoch 166600: Loss = 0.6437\nEpoch 166700: Loss = 0.6437\nEpoch 166800: Loss = 0.6437\nEpoch 166900: Loss = 0.6437\nEpoch 167000: Loss = 0.6437\nEpoch 167100: Loss = 0.6437\nEpoch 167200: Loss = 0.6437\nEpoch 167300: Loss = 0.6437\nEpoch 167400: Loss = 0.6437\nEpoch 167500: Loss = 0.6437\nEpoch 167600: Loss = 0.6437\nEpoch 167700: Loss = 0.6437\nEpoch 167800: Loss = 0.6437\nEpoch 167900: Loss = 0.6437\nEpoch 168000: Loss = 0.6437\nEpoch 168100: Loss = 0.6437\nEpoch 168200: Loss = 0.6437\nEpoch 168300: Loss = 0.6437\nEpoch 168400: Loss = 0.6437\nEpoch 168500: Loss = 0.6437\nEpoch 168600: Loss = 0.6437\nEpoch 168700: Loss = 0.6437\nEpoch 168800: Loss = 0.6437\nEpoch 168900: Loss = 0.6437\nEpoch 169000: Loss = 0.6437\nEpoch 169100: Loss = 0.6437\nEpoch 169200: Loss = 0.6437\nEpoch 169300: Loss = 0.6437\nEpoch 169400: Loss = 0.6437\nEpoch 169500: Loss = 0.6437\nEpoch 169600: Loss = 0.6437\nEpoch 169700: Loss = 0.6437\nEpoch 169800: Loss = 0.6437\nEpoch 169900: Loss = 0.6437\nEpoch 170000: Loss = 0.6437\nEpoch 170100: Loss = 0.6437\nEpoch 170200: Loss = 0.6437\nEpoch 170300: Loss = 0.6437\nEpoch 170400: Loss = 0.6437\nEpoch 170500: Loss = 0.6437\nEpoch 170600: Loss = 0.6437\nEpoch 170700: Loss = 0.6436\nEpoch 170800: Loss = 0.6436\nEpoch 170900: Loss = 0.6436\nEpoch 171000: Loss = 0.6436\nEpoch 171100: Loss = 0.6436\nEpoch 171200: Loss = 0.6436\nEpoch 171300: Loss = 0.6436\nEpoch 171400: Loss = 0.6436\nEpoch 171500: Loss = 0.6436\nEpoch 171600: Loss = 0.6436\nEpoch 171700: Loss = 0.6436\nEpoch 171800: Loss = 0.6436\nEpoch 171900: Loss = 0.6436\nEpoch 172000: Loss = 0.6436\nEpoch 172100: Loss = 0.6436\nEpoch 172200: Loss = 0.6436\nEpoch 172300: Loss = 0.6436\nEpoch 172400: Loss = 0.6436\nEpoch 172500: Loss = 0.6436\nEpoch 172600: Loss = 0.6436\nEpoch 172700: Loss = 0.6436\nEpoch 172800: Loss = 0.6436\nEpoch 172900: Loss = 0.6436\nEpoch 173000: Loss = 0.6436\nEpoch 173100: Loss = 0.6436\nEpoch 173200: Loss = 0.6436\nEpoch 173300: Loss = 0.6436\nEpoch 173400: Loss = 0.6436\nEpoch 173500: Loss = 0.6436\nEpoch 173600: Loss = 0.6436\nEpoch 173700: Loss = 0.6436\nEpoch 173800: Loss = 0.6436\nEpoch 173900: Loss = 0.6436\nEpoch 174000: Loss = 0.6436\nEpoch 174100: Loss = 0.6436\nEpoch 174200: Loss = 0.6436\nEpoch 174300: Loss = 0.6436\nEpoch 174400: Loss = 0.6436\nEpoch 174500: Loss = 0.6436\nEpoch 174600: Loss = 0.6436\nEpoch 174700: Loss = 0.6436\nEpoch 174800: Loss = 0.6436\nEpoch 174900: Loss = 0.6436\nEpoch 175000: Loss = 0.6436\nEpoch 175100: Loss = 0.6436\nEpoch 175200: Loss = 0.6436\nEpoch 175300: Loss = 0.6436\nEpoch 175400: Loss = 0.6436\nEpoch 175500: Loss = 0.6436\nEpoch 175600: Loss = 0.6436\nEpoch 175700: Loss = 0.6436\nEpoch 175800: Loss = 0.6436\nEpoch 175900: Loss = 0.6436\nEpoch 176000: Loss = 0.6436\nEpoch 176100: Loss = 0.6436\nEpoch 176200: Loss = 0.6436\nEpoch 176300: Loss = 0.6436\nEpoch 176400: Loss = 0.6436\nEpoch 176500: Loss = 0.6436\nEpoch 176600: Loss = 0.6436\nEpoch 176700: Loss = 0.6436\nEpoch 176800: Loss = 0.6436\nEpoch 176900: Loss = 0.6436\nEpoch 177000: Loss = 0.6436\nEpoch 177100: Loss = 0.6436\nEpoch 177200: Loss = 0.6436\nEpoch 177300: Loss = 0.6436\nEpoch 177400: Loss = 0.6436\nEpoch 177500: Loss = 0.6436\nEpoch 177600: Loss = 0.6436\nEpoch 177700: Loss = 0.6436\nEpoch 177800: Loss = 0.6436\nEpoch 177900: Loss = 0.6436\nEpoch 178000: Loss = 0.6436\nEpoch 178100: Loss = 0.6436\nEpoch 178200: Loss = 0.6436\nEpoch 178300: Loss = 0.6436\nEpoch 178400: Loss = 0.6436\nEpoch 178500: Loss = 0.6436\nEpoch 178600: Loss = 0.6436\nEpoch 178700: Loss = 0.6436\nEpoch 178800: Loss = 0.6436\nEpoch 178900: Loss = 0.6436\nEpoch 179000: Loss = 0.6436\nEpoch 179100: Loss = 0.6436\nEpoch 179200: Loss = 0.6436\nEpoch 179300: Loss = 0.6436\nEpoch 179400: Loss = 0.6436\nEpoch 179500: Loss = 0.6436\nEpoch 179600: Loss = 0.6436\nEpoch 179700: Loss = 0.6436\nEpoch 179800: Loss = 0.6436\nEpoch 179900: Loss = 0.6436\nEpoch 180000: Loss = 0.6436\nEpoch 180100: Loss = 0.6436\nEpoch 180200: Loss = 0.6436\nEpoch 180300: Loss = 0.6435\nEpoch 180400: Loss = 0.6435\nEpoch 180500: Loss = 0.6435\nEpoch 180600: Loss = 0.6435\nEpoch 180700: Loss = 0.6435\nEpoch 180800: Loss = 0.6435\nEpoch 180900: Loss = 0.6435\nEpoch 181000: Loss = 0.6435\nEpoch 181100: Loss = 0.6435\nEpoch 181200: Loss = 0.6435\nEpoch 181300: Loss = 0.6435\nEpoch 181400: Loss = 0.6435\nEpoch 181500: Loss = 0.6435\nEpoch 181600: Loss = 0.6435\nEpoch 181700: Loss = 0.6435\nEpoch 181800: Loss = 0.6435\nEpoch 181900: Loss = 0.6435\nEpoch 182000: Loss = 0.6435\nEpoch 182100: Loss = 0.6435\nEpoch 182200: Loss = 0.6435\nEpoch 182300: Loss = 0.6435\nEpoch 182400: Loss = 0.6435\nEpoch 182500: Loss = 0.6435\nEpoch 182600: Loss = 0.6435\nEpoch 182700: Loss = 0.6435\nEpoch 182800: Loss = 0.6435\nEpoch 182900: Loss = 0.6435\nEpoch 183000: Loss = 0.6435\nEpoch 183100: Loss = 0.6435\nEpoch 183200: Loss = 0.6435\nEpoch 183300: Loss = 0.6435\nEpoch 183400: Loss = 0.6435\nEpoch 183500: Loss = 0.6435\nEpoch 183600: Loss = 0.6435\nEpoch 183700: Loss = 0.6435\nEpoch 183800: Loss = 0.6435\nEpoch 183900: Loss = 0.6435\nEpoch 184000: Loss = 0.6435\nEpoch 184100: Loss = 0.6435\nEpoch 184200: Loss = 0.6435\nEpoch 184300: Loss = 0.6435\nEpoch 184400: Loss = 0.6435\nEpoch 184500: Loss = 0.6435\nEpoch 184600: Loss = 0.6435\nEpoch 184700: Loss = 0.6435\nEpoch 184800: Loss = 0.6435\nEpoch 184900: Loss = 0.6435\nEpoch 185000: Loss = 0.6435\nEpoch 185100: Loss = 0.6435\nEpoch 185200: Loss = 0.6435\nEpoch 185300: Loss = 0.6435\nEpoch 185400: Loss = 0.6435\nEpoch 185500: Loss = 0.6435\nEpoch 185600: Loss = 0.6435\nEpoch 185700: Loss = 0.6435\nEpoch 185800: Loss = 0.6435\nEpoch 185900: Loss = 0.6435\nEpoch 186000: Loss = 0.6435\nEpoch 186100: Loss = 0.6435\nEpoch 186200: Loss = 0.6435\nEpoch 186300: Loss = 0.6435\nEpoch 186400: Loss = 0.6435\nEpoch 186500: Loss = 0.6435\nEpoch 186600: Loss = 0.6435\nEpoch 186700: Loss = 0.6435\nEpoch 186800: Loss = 0.6435\nEpoch 186900: Loss = 0.6435\nEpoch 187000: Loss = 0.6435\nEpoch 187100: Loss = 0.6435\nEpoch 187200: Loss = 0.6435\nEpoch 187300: Loss = 0.6435\nEpoch 187400: Loss = 0.6435\nEpoch 187500: Loss = 0.6435\nEpoch 187600: Loss = 0.6435\nEpoch 187700: Loss = 0.6435\nEpoch 187800: Loss = 0.6435\nEpoch 187900: Loss = 0.6435\nEpoch 188000: Loss = 0.6435\nEpoch 188100: Loss = 0.6435\nEpoch 188200: Loss = 0.6435\nEpoch 188300: Loss = 0.6435\nEpoch 188400: Loss = 0.6435\nEpoch 188500: Loss = 0.6435\nEpoch 188600: Loss = 0.6435\nEpoch 188700: Loss = 0.6435\nEpoch 188800: Loss = 0.6435\nEpoch 188900: Loss = 0.6435\nEpoch 189000: Loss = 0.6435\nEpoch 189100: Loss = 0.6435\nEpoch 189200: Loss = 0.6435\nEpoch 189300: Loss = 0.6435\nEpoch 189400: Loss = 0.6435\nEpoch 189500: Loss = 0.6435\nEpoch 189600: Loss = 0.6435\nEpoch 189700: Loss = 0.6435\nEpoch 189800: Loss = 0.6434\nEpoch 189900: Loss = 0.6434\nEpoch 190000: Loss = 0.6434\nEpoch 190100: Loss = 0.6434\nEpoch 190200: Loss = 0.6434\nEpoch 190300: Loss = 0.6434\nEpoch 190400: Loss = 0.6434\nEpoch 190500: Loss = 0.6434\nEpoch 190600: Loss = 0.6434\nEpoch 190700: Loss = 0.6434\nEpoch 190800: Loss = 0.6434\nEpoch 190900: Loss = 0.6434\nEpoch 191000: Loss = 0.6434\nEpoch 191100: Loss = 0.6434\nEpoch 191200: Loss = 0.6434\nEpoch 191300: Loss = 0.6434\nEpoch 191400: Loss = 0.6434\nEpoch 191500: Loss = 0.6434\nEpoch 191600: Loss = 0.6434\nEpoch 191700: Loss = 0.6434\nEpoch 191800: Loss = 0.6434\nEpoch 191900: Loss = 0.6434\nEpoch 192000: Loss = 0.6434\nEpoch 192100: Loss = 0.6434\nEpoch 192200: Loss = 0.6434\nEpoch 192300: Loss = 0.6434\nEpoch 192400: Loss = 0.6434\nEpoch 192500: Loss = 0.6434\nEpoch 192600: Loss = 0.6434\nEpoch 192700: Loss = 0.6434\nEpoch 192800: Loss = 0.6434\nEpoch 192900: Loss = 0.6434\nEpoch 193000: Loss = 0.6434\nEpoch 193100: Loss = 0.6434\nEpoch 193200: Loss = 0.6434\nEpoch 193300: Loss = 0.6434\nEpoch 193400: Loss = 0.6434\nEpoch 193500: Loss = 0.6434\nEpoch 193600: Loss = 0.6434\nEpoch 193700: Loss = 0.6434\nEpoch 193800: Loss = 0.6434\nEpoch 193900: Loss = 0.6434\nEpoch 194000: Loss = 0.6434\nEpoch 194100: Loss = 0.6434\nEpoch 194200: Loss = 0.6434\nEpoch 194300: Loss = 0.6434\nEpoch 194400: Loss = 0.6434\nEpoch 194500: Loss = 0.6434\nEpoch 194600: Loss = 0.6434\nEpoch 194700: Loss = 0.6434\nEpoch 194800: Loss = 0.6434\nEpoch 194900: Loss = 0.6434\nEpoch 195000: Loss = 0.6434\nEpoch 195100: Loss = 0.6434\nEpoch 195200: Loss = 0.6434\nEpoch 195300: Loss = 0.6434\nEpoch 195400: Loss = 0.6434\nEpoch 195500: Loss = 0.6434\nEpoch 195600: Loss = 0.6434\nEpoch 195700: Loss = 0.6434\nEpoch 195800: Loss = 0.6434\nEpoch 195900: Loss = 0.6434\nEpoch 196000: Loss = 0.6434\nEpoch 196100: Loss = 0.6434\nEpoch 196200: Loss = 0.6434\nEpoch 196300: Loss = 0.6434\nEpoch 196400: Loss = 0.6434\nEpoch 196500: Loss = 0.6434\nEpoch 196600: Loss = 0.6434\nEpoch 196700: Loss = 0.6434\nEpoch 196800: Loss = 0.6434\nEpoch 196900: Loss = 0.6434\nEpoch 197000: Loss = 0.6434\nEpoch 197100: Loss = 0.6434\nEpoch 197200: Loss = 0.6434\nEpoch 197300: Loss = 0.6434\nEpoch 197400: Loss = 0.6434\nEpoch 197500: Loss = 0.6434\nEpoch 197600: Loss = 0.6434\nEpoch 197700: Loss = 0.6434\nEpoch 197800: Loss = 0.6434\nEpoch 197900: Loss = 0.6434\nEpoch 198000: Loss = 0.6434\nEpoch 198100: Loss = 0.6434\nEpoch 198200: Loss = 0.6434\nEpoch 198300: Loss = 0.6434\nEpoch 198400: Loss = 0.6434\nEpoch 198500: Loss = 0.6434\nEpoch 198600: Loss = 0.6434\nEpoch 198700: Loss = 0.6434\nEpoch 198800: Loss = 0.6434\nEpoch 198900: Loss = 0.6434\nEpoch 199000: Loss = 0.6434\nEpoch 199100: Loss = 0.6434\nEpoch 199200: Loss = 0.6434\nEpoch 199300: Loss = 0.6434\nEpoch 199400: Loss = 0.6434\nEpoch 199500: Loss = 0.6434\nEpoch 199600: Loss = 0.6433\nEpoch 199700: Loss = 0.6433\nEpoch 199800: Loss = 0.6433\nEpoch 199900: Loss = 0.6433\nEpoch 200000: Loss = 0.6433\nEpoch 200100: Loss = 0.6433\nEpoch 200200: Loss = 0.6433\nEpoch 200300: Loss = 0.6433\nEpoch 200400: Loss = 0.6433\nEpoch 200500: Loss = 0.6433\nEpoch 200600: Loss = 0.6433\nEpoch 200700: Loss = 0.6433\nEpoch 200800: Loss = 0.6433\nEpoch 200900: Loss = 0.6433\nEpoch 201000: Loss = 0.6433\nEpoch 201100: Loss = 0.6433\nEpoch 201200: Loss = 0.6433\nEpoch 201300: Loss = 0.6433\nEpoch 201400: Loss = 0.6433\nEpoch 201500: Loss = 0.6433\nEpoch 201600: Loss = 0.6433\nEpoch 201700: Loss = 0.6433\nEpoch 201800: Loss = 0.6433\nEpoch 201900: Loss = 0.6433\nEpoch 202000: Loss = 0.6433\nEpoch 202100: Loss = 0.6433\nEpoch 202200: Loss = 0.6433\nEpoch 202300: Loss = 0.6433\nEpoch 202400: Loss = 0.6433\nEpoch 202500: Loss = 0.6433\nEpoch 202600: Loss = 0.6433\nEpoch 202700: Loss = 0.6433\nEpoch 202800: Loss = 0.6433\nEpoch 202900: Loss = 0.6433\nEpoch 203000: Loss = 0.6433\nEpoch 203100: Loss = 0.6433\nEpoch 203200: Loss = 0.6433\nEpoch 203300: Loss = 0.6433\nEpoch 203400: Loss = 0.6433\nEpoch 203500: Loss = 0.6433\nEpoch 203600: Loss = 0.6433\nEpoch 203700: Loss = 0.6433\nEpoch 203800: Loss = 0.6433\nEpoch 203900: Loss = 0.6433\nEpoch 204000: Loss = 0.6433\nEpoch 204100: Loss = 0.6433\nEpoch 204200: Loss = 0.6433\nEpoch 204300: Loss = 0.6433\nEpoch 204400: Loss = 0.6433\nEpoch 204500: Loss = 0.6433\nEpoch 204600: Loss = 0.6433\nEpoch 204700: Loss = 0.6433\nEpoch 204800: Loss = 0.6433\nEpoch 204900: Loss = 0.6433\nEpoch 205000: Loss = 0.6433\nEpoch 205100: Loss = 0.6433\nEpoch 205200: Loss = 0.6433\nEpoch 205300: Loss = 0.6433\nEpoch 205400: Loss = 0.6433\nEpoch 205500: Loss = 0.6433\nEpoch 205600: Loss = 0.6433\nEpoch 205700: Loss = 0.6433\nEpoch 205800: Loss = 0.6433\nEpoch 205900: Loss = 0.6433\nEpoch 206000: Loss = 0.6433\nEpoch 206100: Loss = 0.6433\nEpoch 206200: Loss = 0.6433\nEpoch 206300: Loss = 0.6433\nEpoch 206400: Loss = 0.6433\nEpoch 206500: Loss = 0.6433\nEpoch 206600: Loss = 0.6433\nEpoch 206700: Loss = 0.6433\nEpoch 206800: Loss = 0.6433\nEpoch 206900: Loss = 0.6433\nEpoch 207000: Loss = 0.6433\nEpoch 207100: Loss = 0.6433\nEpoch 207200: Loss = 0.6433\nEpoch 207300: Loss = 0.6433\nEpoch 207400: Loss = 0.6433\nEpoch 207500: Loss = 0.6433\nEpoch 207600: Loss = 0.6433\nEpoch 207700: Loss = 0.6433\nEpoch 207800: Loss = 0.6433\nEpoch 207900: Loss = 0.6433\nEpoch 208000: Loss = 0.6433\nEpoch 208100: Loss = 0.6433\nEpoch 208200: Loss = 0.6433\nEpoch 208300: Loss = 0.6433\nEpoch 208400: Loss = 0.6433\nEpoch 208500: Loss = 0.6433\nEpoch 208600: Loss = 0.6433\nEpoch 208700: Loss = 0.6433\nEpoch 208800: Loss = 0.6433\nEpoch 208900: Loss = 0.6433\nEpoch 209000: Loss = 0.6433\nEpoch 209100: Loss = 0.6433\nEpoch 209200: Loss = 0.6433\nEpoch 209300: Loss = 0.6433\nEpoch 209400: Loss = 0.6433\nEpoch 209500: Loss = 0.6433\nEpoch 209600: Loss = 0.6433\nEpoch 209700: Loss = 0.6433\nEpoch 209800: Loss = 0.6433\nEpoch 209900: Loss = 0.6433\nEpoch 210000: Loss = 0.6433\nEpoch 210100: Loss = 0.6433\nEpoch 210200: Loss = 0.6433\nEpoch 210300: Loss = 0.6433\nEpoch 210400: Loss = 0.6432\nEpoch 210500: Loss = 0.6432\nEpoch 210600: Loss = 0.6432\nEpoch 210700: Loss = 0.6432\nEpoch 210800: Loss = 0.6432\nEpoch 210900: Loss = 0.6432\nEpoch 211000: Loss = 0.6432\nEpoch 211100: Loss = 0.6432\nEpoch 211200: Loss = 0.6432\nEpoch 211300: Loss = 0.6432\nEpoch 211400: Loss = 0.6432\nEpoch 211500: Loss = 0.6432\nEpoch 211600: Loss = 0.6432\nEpoch 211700: Loss = 0.6432\nEpoch 211800: Loss = 0.6432\nEpoch 211900: Loss = 0.6432\nEpoch 212000: Loss = 0.6432\nEpoch 212100: Loss = 0.6432\nEpoch 212200: Loss = 0.6432\nEpoch 212300: Loss = 0.6432\nEpoch 212400: Loss = 0.6432\nEpoch 212500: Loss = 0.6432\nEpoch 212600: Loss = 0.6432\nEpoch 212700: Loss = 0.6432\nEpoch 212800: Loss = 0.6432\nEpoch 212900: Loss = 0.6432\nEpoch 213000: Loss = 0.6432\nEpoch 213100: Loss = 0.6432\nEpoch 213200: Loss = 0.6432\nEpoch 213300: Loss = 0.6432\nEpoch 213400: Loss = 0.6432\nEpoch 213500: Loss = 0.6432\nEpoch 213600: Loss = 0.6432\nEpoch 213700: Loss = 0.6432\nEpoch 213800: Loss = 0.6432\nEpoch 213900: Loss = 0.6432\nEpoch 214000: Loss = 0.6432\nEpoch 214100: Loss = 0.6432\nEpoch 214200: Loss = 0.6432\nEpoch 214300: Loss = 0.6432\nEpoch 214400: Loss = 0.6432\nEpoch 214500: Loss = 0.6432\nEpoch 214600: Loss = 0.6432\nEpoch 214700: Loss = 0.6432\nEpoch 214800: Loss = 0.6432\nEpoch 214900: Loss = 0.6432\nEpoch 215000: Loss = 0.6432\nEpoch 215100: Loss = 0.6432\nEpoch 215200: Loss = 0.6432\nEpoch 215300: Loss = 0.6432\nEpoch 215400: Loss = 0.6432\nEpoch 215500: Loss = 0.6432\nEpoch 215600: Loss = 0.6432\nEpoch 215700: Loss = 0.6432\nEpoch 215800: Loss = 0.6432\nEpoch 215900: Loss = 0.6432\nEpoch 216000: Loss = 0.6432\nEpoch 216100: Loss = 0.6432\nEpoch 216200: Loss = 0.6432\nEpoch 216300: Loss = 0.6432\nEpoch 216400: Loss = 0.6432\nEpoch 216500: Loss = 0.6432\nEpoch 216600: Loss = 0.6432\nEpoch 216700: Loss = 0.6432\nEpoch 216800: Loss = 0.6432\nEpoch 216900: Loss = 0.6432\nEpoch 217000: Loss = 0.6432\nEpoch 217100: Loss = 0.6432\nEpoch 217200: Loss = 0.6432\nEpoch 217300: Loss = 0.6432\nEpoch 217400: Loss = 0.6432\nEpoch 217500: Loss = 0.6432\nEpoch 217600: Loss = 0.6432\nEpoch 217700: Loss = 0.6432\nEpoch 217800: Loss = 0.6432\nEpoch 217900: Loss = 0.6432\nEpoch 218000: Loss = 0.6432\nEpoch 218100: Loss = 0.6432\nEpoch 218200: Loss = 0.6432\nEpoch 218300: Loss = 0.6432\nEpoch 218400: Loss = 0.6432\nEpoch 218500: Loss = 0.6432\nEpoch 218600: Loss = 0.6432\nEpoch 218700: Loss = 0.6432\nEpoch 218800: Loss = 0.6432\nEpoch 218900: Loss = 0.6432\nEpoch 219000: Loss = 0.6432\nEpoch 219100: Loss = 0.6432\nEpoch 219200: Loss = 0.6432\nEpoch 219300: Loss = 0.6432\nEpoch 219400: Loss = 0.6432\nEpoch 219500: Loss = 0.6432\nEpoch 219600: Loss = 0.6432\nEpoch 219700: Loss = 0.6432\nEpoch 219800: Loss = 0.6432\nEpoch 219900: Loss = 0.6432\nEpoch 220000: Loss = 0.6432\nEpoch 220100: Loss = 0.6432\nEpoch 220200: Loss = 0.6432\nEpoch 220300: Loss = 0.6432\nEpoch 220400: Loss = 0.6432\nEpoch 220500: Loss = 0.6432\nEpoch 220600: Loss = 0.6432\nEpoch 220700: Loss = 0.6432\nEpoch 220800: Loss = 0.6432\nEpoch 220900: Loss = 0.6432\nEpoch 221000: Loss = 0.6432\nEpoch 221100: Loss = 0.6432\nEpoch 221200: Loss = 0.6432\nEpoch 221300: Loss = 0.6432\nEpoch 221400: Loss = 0.6432\nEpoch 221500: Loss = 0.6432\nEpoch 221600: Loss = 0.6432\nEpoch 221700: Loss = 0.6432\nEpoch 221800: Loss = 0.6431\nEpoch 221900: Loss = 0.6431\nEpoch 222000: Loss = 0.6431\nEpoch 222100: Loss = 0.6431\nEpoch 222200: Loss = 0.6431\nEpoch 222300: Loss = 0.6431\nEpoch 222400: Loss = 0.6431\nEpoch 222500: Loss = 0.6431\nEpoch 222600: Loss = 0.6431\nEpoch 222700: Loss = 0.6431\nEpoch 222800: Loss = 0.6431\nEpoch 222900: Loss = 0.6431\nEpoch 223000: Loss = 0.6431\nEpoch 223100: Loss = 0.6431\nEpoch 223200: Loss = 0.6431\nEpoch 223300: Loss = 0.6431\nEpoch 223400: Loss = 0.6431\nEpoch 223500: Loss = 0.6431\nEpoch 223600: Loss = 0.6431\nEpoch 223700: Loss = 0.6431\nEpoch 223800: Loss = 0.6431\nEpoch 223900: Loss = 0.6431\nEpoch 224000: Loss = 0.6431\nEpoch 224100: Loss = 0.6431\nEpoch 224200: Loss = 0.6431\nEpoch 224300: Loss = 0.6431\nEpoch 224400: Loss = 0.6431\nEpoch 224500: Loss = 0.6431\nEpoch 224600: Loss = 0.6431\nEpoch 224700: Loss = 0.6431\nEpoch 224800: Loss = 0.6431\nEpoch 224900: Loss = 0.6431\nEpoch 225000: Loss = 0.6431\nEpoch 225100: Loss = 0.6431\nEpoch 225200: Loss = 0.6431\nEpoch 225300: Loss = 0.6431\nEpoch 225400: Loss = 0.6431\nEpoch 225500: Loss = 0.6431\nEpoch 225600: Loss = 0.6431\nEpoch 225700: Loss = 0.6431\nEpoch 225800: Loss = 0.6431\nEpoch 225900: Loss = 0.6431\nEpoch 226000: Loss = 0.6431\nEpoch 226100: Loss = 0.6431\nEpoch 226200: Loss = 0.6431\nEpoch 226300: Loss = 0.6431\nEpoch 226400: Loss = 0.6431\nEpoch 226500: Loss = 0.6431\nEpoch 226600: Loss = 0.6431\nEpoch 226700: Loss = 0.6431\nEpoch 226800: Loss = 0.6431\nEpoch 226900: Loss = 0.6431\nEpoch 227000: Loss = 0.6431\nEpoch 227100: Loss = 0.6431\nEpoch 227200: Loss = 0.6431\nEpoch 227300: Loss = 0.6431\nEpoch 227400: Loss = 0.6431\nEpoch 227500: Loss = 0.6431\nEpoch 227600: Loss = 0.6431\nEpoch 227700: Loss = 0.6431\nEpoch 227800: Loss = 0.6431\nEpoch 227900: Loss = 0.6431\nEpoch 228000: Loss = 0.6431\nEpoch 228100: Loss = 0.6431\nEpoch 228200: Loss = 0.6431\nEpoch 228300: Loss = 0.6431\nEpoch 228400: Loss = 0.6431\nEpoch 228500: Loss = 0.6431\nEpoch 228600: Loss = 0.6431\nEpoch 228700: Loss = 0.6431\nEpoch 228800: Loss = 0.6431\nEpoch 228900: Loss = 0.6431\nEpoch 229000: Loss = 0.6431\nEpoch 229100: Loss = 0.6431\nEpoch 229200: Loss = 0.6431\nEpoch 229300: Loss = 0.6431\nEpoch 229400: Loss = 0.6431\nEpoch 229500: Loss = 0.6431\nEpoch 229600: Loss = 0.6431\nEpoch 229700: Loss = 0.6431\nEpoch 229800: Loss = 0.6431\nEpoch 229900: Loss = 0.6431\nEpoch 230000: Loss = 0.6431\nEpoch 230100: Loss = 0.6431\nEpoch 230200: Loss = 0.6431\nEpoch 230300: Loss = 0.6431\nEpoch 230400: Loss = 0.6431\nEpoch 230500: Loss = 0.6431\nEpoch 230600: Loss = 0.6431\nEpoch 230700: Loss = 0.6431\nEpoch 230800: Loss = 0.6431\nEpoch 230900: Loss = 0.6431\nEpoch 231000: Loss = 0.6431\nEpoch 231100: Loss = 0.6431\nEpoch 231200: Loss = 0.6431\nEpoch 231300: Loss = 0.6431\nEpoch 231400: Loss = 0.6431\nEpoch 231500: Loss = 0.6431\nEpoch 231600: Loss = 0.6431\nEpoch 231700: Loss = 0.6431\nEpoch 231800: Loss = 0.6431\nEpoch 231900: Loss = 0.6431\nEpoch 232000: Loss = 0.6431\nEpoch 232100: Loss = 0.6431\nEpoch 232200: Loss = 0.6431\nEpoch 232300: Loss = 0.6431\nEpoch 232400: Loss = 0.6431\nEpoch 232500: Loss = 0.6431\nEpoch 232600: Loss = 0.6431\nEpoch 232700: Loss = 0.6431\nEpoch 232800: Loss = 0.6431\nEpoch 232900: Loss = 0.6431\nEpoch 233000: Loss = 0.6431\nEpoch 233100: Loss = 0.6430\nEpoch 233200: Loss = 0.6430\nEpoch 233300: Loss = 0.6430\nEpoch 233400: Loss = 0.6430\nEpoch 233500: Loss = 0.6430\nEpoch 233600: Loss = 0.6430\nEpoch 233700: Loss = 0.6430\nEpoch 233800: Loss = 0.6430\nEpoch 233900: Loss = 0.6430\nEpoch 234000: Loss = 0.6430\nEpoch 234100: Loss = 0.6430\nEpoch 234200: Loss = 0.6430\nEpoch 234300: Loss = 0.6430\nEpoch 234400: Loss = 0.6430\nEpoch 234500: Loss = 0.6430\nEpoch 234600: Loss = 0.6430\nEpoch 234700: Loss = 0.6430\nEpoch 234800: Loss = 0.6430\nEpoch 234900: Loss = 0.6430\nEpoch 235000: Loss = 0.6430\nEpoch 235100: Loss = 0.6430\nEpoch 235200: Loss = 0.6430\nEpoch 235300: Loss = 0.6430\nEpoch 235400: Loss = 0.6430\nEpoch 235500: Loss = 0.6430\nEpoch 235600: Loss = 0.6430\nEpoch 235700: Loss = 0.6430\nEpoch 235800: Loss = 0.6430\nEpoch 235900: Loss = 0.6430\nEpoch 236000: Loss = 0.6430\nEpoch 236100: Loss = 0.6430\nEpoch 236200: Loss = 0.6430\nEpoch 236300: Loss = 0.6430\nEpoch 236400: Loss = 0.6430\nEpoch 236500: Loss = 0.6430\nEpoch 236600: Loss = 0.6430\nEpoch 236700: Loss = 0.6430\nEpoch 236800: Loss = 0.6430\nEpoch 236900: Loss = 0.6430\nEpoch 237000: Loss = 0.6430\nEpoch 237100: Loss = 0.6430\nEpoch 237200: Loss = 0.6430\nEpoch 237300: Loss = 0.6430\nEpoch 237400: Loss = 0.6430\nEpoch 237500: Loss = 0.6430\nEpoch 237600: Loss = 0.6430\nEpoch 237700: Loss = 0.6430\nEpoch 237800: Loss = 0.6430\nEpoch 237900: Loss = 0.6430\nEpoch 238000: Loss = 0.6430\nEpoch 238100: Loss = 0.6430\nEpoch 238200: Loss = 0.6430\nEpoch 238300: Loss = 0.6430\nEpoch 238400: Loss = 0.6430\nEpoch 238500: Loss = 0.6430\nEpoch 238600: Loss = 0.6430\nEpoch 238700: Loss = 0.6430\nEpoch 238800: Loss = 0.6430\nEpoch 238900: Loss = 0.6430\nEpoch 239000: Loss = 0.6430\nEpoch 239100: Loss = 0.6430\nEpoch 239200: Loss = 0.6430\nEpoch 239300: Loss = 0.6430\nEpoch 239400: Loss = 0.6430\nEpoch 239500: Loss = 0.6430\nEpoch 239600: Loss = 0.6430\nEpoch 239700: Loss = 0.6430\nEpoch 239800: Loss = 0.6430\nEpoch 239900: Loss = 0.6430\nEpoch 240000: Loss = 0.6430\nEpoch 240100: Loss = 0.6430\nEpoch 240200: Loss = 0.6430\nEpoch 240300: Loss = 0.6430\nEpoch 240400: Loss = 0.6430\nEpoch 240500: Loss = 0.6430\nEpoch 240600: Loss = 0.6430\nEpoch 240700: Loss = 0.6430\nEpoch 240800: Loss = 0.6430\nEpoch 240900: Loss = 0.6430\nEpoch 241000: Loss = 0.6430\nEpoch 241100: Loss = 0.6430\nEpoch 241200: Loss = 0.6430\nEpoch 241300: Loss = 0.6430\nEpoch 241400: Loss = 0.6430\nEpoch 241500: Loss = 0.6430\nEpoch 241600: Loss = 0.6430\nEpoch 241700: Loss = 0.6430\nEpoch 241800: Loss = 0.6430\nEpoch 241900: Loss = 0.6430\nEpoch 242000: Loss = 0.6430\nEpoch 242100: Loss = 0.6430\nEpoch 242200: Loss = 0.6430\nEpoch 242300: Loss = 0.6430\nEpoch 242400: Loss = 0.6430\nEpoch 242500: Loss = 0.6430\nEpoch 242600: Loss = 0.6430\nEpoch 242700: Loss = 0.6430\nEpoch 242800: Loss = 0.6430\nEpoch 242900: Loss = 0.6430\nEpoch 243000: Loss = 0.6430\nEpoch 243100: Loss = 0.6430\nEpoch 243200: Loss = 0.6430\nEpoch 243300: Loss = 0.6430\nEpoch 243400: Loss = 0.6430\nEpoch 243500: Loss = 0.6430\nEpoch 243600: Loss = 0.6430\nEpoch 243700: Loss = 0.6430\nEpoch 243800: Loss = 0.6430\nEpoch 243900: Loss = 0.6430\nEpoch 244000: Loss = 0.6430\nEpoch 244100: Loss = 0.6430\nEpoch 244200: Loss = 0.6430\nEpoch 244300: Loss = 0.6430\nEpoch 244400: Loss = 0.6430\nEpoch 244500: Loss = 0.6430\nEpoch 244600: Loss = 0.6430\nEpoch 244700: Loss = 0.6430\nEpoch 244800: Loss = 0.6430\nEpoch 244900: Loss = 0.6430\nEpoch 245000: Loss = 0.6430\nEpoch 245100: Loss = 0.6430\nEpoch 245200: Loss = 0.6430\nEpoch 245300: Loss = 0.6430\nEpoch 245400: Loss = 0.6430\nEpoch 245500: Loss = 0.6430\nEpoch 245600: Loss = 0.6430\nEpoch 245700: Loss = 0.6430\nEpoch 245800: Loss = 0.6430\nEpoch 245900: Loss = 0.6430\nEpoch 246000: Loss = 0.6430\nEpoch 246100: Loss = 0.6430\nEpoch 246200: Loss = 0.6430\nEpoch 246300: Loss = 0.6430\nEpoch 246400: Loss = 0.6430\nEpoch 246500: Loss = 0.6430\nEpoch 246600: Loss = 0.6429\nEpoch 246700: Loss = 0.6430\nEpoch 246800: Loss = 0.6430\nEpoch 246900: Loss = 0.6430\nEpoch 247000: Loss = 0.6429\nEpoch 247100: Loss = 0.6429\nEpoch 247200: Loss = 0.6429\nEpoch 247300: Loss = 0.6429\nEpoch 247400: Loss = 0.6429\nEpoch 247500: Loss = 0.6429\nEpoch 247600: Loss = 0.6429\nEpoch 247700: Loss = 0.6429\nEpoch 247800: Loss = 0.6429\nEpoch 247900: Loss = 0.6429\nEpoch 248000: Loss = 0.6429\nEpoch 248100: Loss = 0.6429\nEpoch 248200: Loss = 0.6429\nEpoch 248300: Loss = 0.6429\nEpoch 248400: Loss = 0.6429\nEpoch 248500: Loss = 0.6429\nEpoch 248600: Loss = 0.6429\nEpoch 248700: Loss = 0.6429\nEpoch 248800: Loss = 0.6429\nEpoch 248900: Loss = 0.6429\nEpoch 249000: Loss = 0.6429\nEpoch 249100: Loss = 0.6429\nEpoch 249200: Loss = 0.6429\nEpoch 249300: Loss = 0.6429\nEpoch 249400: Loss = 0.6429\nEpoch 249500: Loss = 0.6429\nEpoch 249600: Loss = 0.6429\nEpoch 249700: Loss = 0.6429\nEpoch 249800: Loss = 0.6429\nEpoch 249900: Loss = 0.6429\nEpoch 250000: Loss = 0.6429\nEpoch 250100: Loss = 0.6429\nEpoch 250200: Loss = 0.6429\nEpoch 250300: Loss = 0.6429\nEpoch 250400: Loss = 0.6429\nEpoch 250500: Loss = 0.6429\nEpoch 250600: Loss = 0.6429\nEpoch 250700: Loss = 0.6429\nEpoch 250800: Loss = 0.6429\nEpoch 250900: Loss = 0.6429\nEpoch 251000: Loss = 0.6429\nEpoch 251100: Loss = 0.6429\nEpoch 251200: Loss = 0.6429\nEpoch 251300: Loss = 0.6429\nEpoch 251400: Loss = 0.6429\nEpoch 251500: Loss = 0.6429\nEpoch 251600: Loss = 0.6429\nEpoch 251700: Loss = 0.6429\nEpoch 251800: Loss = 0.6429\nEpoch 251900: Loss = 0.6429\nEpoch 252000: Loss = 0.6429\nEpoch 252100: Loss = 0.6429\nEpoch 252200: Loss = 0.6429\nEpoch 252300: Loss = 0.6429\nEpoch 252400: Loss = 0.6429\nEpoch 252500: Loss = 0.6429\nEpoch 252600: Loss = 0.6429\nEpoch 252700: Loss = 0.6429\nEpoch 252800: Loss = 0.6429\nEpoch 252900: Loss = 0.6429\nEpoch 253000: Loss = 0.6429\nEpoch 253100: Loss = 0.6429\nEpoch 253200: Loss = 0.6429\nEpoch 253300: Loss = 0.6429\nEpoch 253400: Loss = 0.6429\nEpoch 253500: Loss = 0.6429\nEpoch 253600: Loss = 0.6429\nEpoch 253700: Loss = 0.6429\nEpoch 253800: Loss = 0.6429\nEpoch 253900: Loss = 0.6429\nEpoch 254000: Loss = 0.6429\nEpoch 254100: Loss = 0.6429\nEpoch 254200: Loss = 0.6429\nEpoch 254300: Loss = 0.6429\nEpoch 254400: Loss = 0.6429\nEpoch 254500: Loss = 0.6429\nEpoch 254600: Loss = 0.6429\nEpoch 254700: Loss = 0.6429\nEpoch 254800: Loss = 0.6429\nEpoch 254900: Loss = 0.6429\nEpoch 255000: Loss = 0.6429\nEpoch 255100: Loss = 0.6429\nEpoch 255200: Loss = 0.6429\nEpoch 255300: Loss = 0.6429\nEpoch 255400: Loss = 0.6429\nEpoch 255500: Loss = 0.6429\nEpoch 255600: Loss = 0.6429\nEpoch 255700: Loss = 0.6429\nEpoch 255800: Loss = 0.6429\nEpoch 255900: Loss = 0.6429\nEpoch 256000: Loss = 0.6429\nEpoch 256100: Loss = 0.6429\nEpoch 256200: Loss = 0.6429\nEpoch 256300: Loss = 0.6429\nEpoch 256400: Loss = 0.6429\nEpoch 256500: Loss = 0.6429\nEpoch 256600: Loss = 0.6429\nEpoch 256700: Loss = 0.6429\nEpoch 256800: Loss = 0.6429\nEpoch 256900: Loss = 0.6429\nEpoch 257000: Loss = 0.6429\nEpoch 257100: Loss = 0.6429\nEpoch 257200: Loss = 0.6429\nEpoch 257300: Loss = 0.6429\nEpoch 257400: Loss = 0.6429\nEpoch 257500: Loss = 0.6429\nEpoch 257600: Loss = 0.6429\nEpoch 257700: Loss = 0.6429\nEpoch 257800: Loss = 0.6429\nEpoch 257900: Loss = 0.6429\nEpoch 258000: Loss = 0.6429\nEpoch 258100: Loss = 0.6429\nEpoch 258200: Loss = 0.6429\nEpoch 258300: Loss = 0.6429\nEpoch 258400: Loss = 0.6429\nEpoch 258500: Loss = 0.6429\nEpoch 258600: Loss = 0.6429\nEpoch 258700: Loss = 0.6429\nEpoch 258800: Loss = 0.6429\nEpoch 258900: Loss = 0.6429\nEpoch 259000: Loss = 0.6429\nEpoch 259100: Loss = 0.6429\nEpoch 259200: Loss = 0.6429\nEpoch 259300: Loss = 0.6429\nEpoch 259400: Loss = 0.6429\nEpoch 259500: Loss = 0.6429\nEpoch 259600: Loss = 0.6429\nEpoch 259700: Loss = 0.6429\nEpoch 259800: Loss = 0.6429\nEpoch 259900: Loss = 0.6429\nEpoch 260000: Loss = 0.6429\nEpoch 260100: Loss = 0.6429\nEpoch 260200: Loss = 0.6429\nEpoch 260300: Loss = 0.6429\nEpoch 260400: Loss = 0.6429\nEpoch 260500: Loss = 0.6429\nEpoch 260600: Loss = 0.6429\nEpoch 260700: Loss = 0.6429\nEpoch 260800: Loss = 0.6428\nEpoch 260900: Loss = 0.6428\nEpoch 261000: Loss = 0.6429\nEpoch 261100: Loss = 0.6428\nEpoch 261200: Loss = 0.6429\nEpoch 261300: Loss = 0.6429\nEpoch 261400: Loss = 0.6428\nEpoch 261500: Loss = 0.6428\nEpoch 261600: Loss = 0.6428\nEpoch 261700: Loss = 0.6428\nEpoch 261800: Loss = 0.6428\nEpoch 261900: Loss = 0.6428\nEpoch 262000: Loss = 0.6428\nEpoch 262100: Loss = 0.6428\nEpoch 262200: Loss = 0.6428\nEpoch 262300: Loss = 0.6428\nEpoch 262400: Loss = 0.6428\nEpoch 262500: Loss = 0.6428\nEpoch 262600: Loss = 0.6428\nEpoch 262700: Loss = 0.6428\nEpoch 262800: Loss = 0.6428\nEpoch 262900: Loss = 0.6428\nEpoch 263000: Loss = 0.6428\nEpoch 263100: Loss = 0.6428\nEpoch 263200: Loss = 0.6428\nEpoch 263300: Loss = 0.6428\nEpoch 263400: Loss = 0.6428\nEpoch 263500: Loss = 0.6428\nEpoch 263600: Loss = 0.6428\nEpoch 263700: Loss = 0.6428\nEpoch 263800: Loss = 0.6428\nEpoch 263900: Loss = 0.6428\nEpoch 264000: Loss = 0.6428\nEpoch 264100: Loss = 0.6428\nEpoch 264200: Loss = 0.6428\nEpoch 264300: Loss = 0.6428\nEpoch 264400: Loss = 0.6428\nEpoch 264500: Loss = 0.6428\nEpoch 264600: Loss = 0.6428\nEpoch 264700: Loss = 0.6428\nEpoch 264800: Loss = 0.6428\nEpoch 264900: Loss = 0.6428\nEpoch 265000: Loss = 0.6428\nEpoch 265100: Loss = 0.6428\nEpoch 265200: Loss = 0.6428\nEpoch 265300: Loss = 0.6428\nEpoch 265400: Loss = 0.6428\nEpoch 265500: Loss = 0.6428\nEpoch 265600: Loss = 0.6428\nEpoch 265700: Loss = 0.6428\nEpoch 265800: Loss = 0.6428\nEpoch 265900: Loss = 0.6428\nEpoch 266000: Loss = 0.6428\nEpoch 266100: Loss = 0.6428\nEpoch 266200: Loss = 0.6428\nEpoch 266300: Loss = 0.6428\nEpoch 266400: Loss = 0.6428\nEpoch 266500: Loss = 0.6428\nEpoch 266600: Loss = 0.6428\nEpoch 266700: Loss = 0.6428\nEpoch 266800: Loss = 0.6428\nEpoch 266900: Loss = 0.6428\nEpoch 267000: Loss = 0.6428\nEpoch 267100: Loss = 0.6428\nEpoch 267200: Loss = 0.6428\nEpoch 267300: Loss = 0.6428\nEpoch 267400: Loss = 0.6428\nEpoch 267500: Loss = 0.6428\nEpoch 267600: Loss = 0.6428\nEpoch 267700: Loss = 0.6428\nEpoch 267800: Loss = 0.6428\nEpoch 267900: Loss = 0.6428\nEpoch 268000: Loss = 0.6428\nEpoch 268100: Loss = 0.6428\nEpoch 268200: Loss = 0.6428\nEpoch 268300: Loss = 0.6428\nEpoch 268400: Loss = 0.6428\nEpoch 268500: Loss = 0.6428\nEpoch 268600: Loss = 0.6428\nEpoch 268700: Loss = 0.6428\nEpoch 268800: Loss = 0.6428\nEpoch 268900: Loss = 0.6428\nEpoch 269000: Loss = 0.6428\nEpoch 269100: Loss = 0.6428\nEpoch 269200: Loss = 0.6428\nEpoch 269300: Loss = 0.6428\nEpoch 269400: Loss = 0.6428\nEpoch 269500: Loss = 0.6428\nEpoch 269600: Loss = 0.6428\nEpoch 269700: Loss = 0.6428\nEpoch 269800: Loss = 0.6428\nEpoch 269900: Loss = 0.6428\nEpoch 270000: Loss = 0.6428\nEpoch 270100: Loss = 0.6428\nEpoch 270200: Loss = 0.6428\nEpoch 270300: Loss = 0.6428\nEpoch 270400: Loss = 0.6428\nEpoch 270500: Loss = 0.6428\nEpoch 270600: Loss = 0.6428\nEpoch 270700: Loss = 0.6428\nEpoch 270800: Loss = 0.6428\nEpoch 270900: Loss = 0.6428\nEpoch 271000: Loss = 0.6428\nEpoch 271100: Loss = 0.6428\nEpoch 271200: Loss = 0.6428\nEpoch 271300: Loss = 0.6428\nEpoch 271400: Loss = 0.6428\nEpoch 271500: Loss = 0.6428\nEpoch 271600: Loss = 0.6428\nEpoch 271700: Loss = 0.6428\nEpoch 271800: Loss = 0.6428\nEpoch 271900: Loss = 0.6428\nEpoch 272000: Loss = 0.6428\nEpoch 272100: Loss = 0.6428\nEpoch 272200: Loss = 0.6428\nEpoch 272300: Loss = 0.6428\nEpoch 272400: Loss = 0.6428\nEpoch 272500: Loss = 0.6428\nEpoch 272600: Loss = 0.6428\nEpoch 272700: Loss = 0.6428\nEpoch 272800: Loss = 0.6428\nEpoch 272900: Loss = 0.6428\nEpoch 273000: Loss = 0.6428\nEpoch 273100: Loss = 0.6428\nEpoch 273200: Loss = 0.6428\nEpoch 273300: Loss = 0.6428\nEpoch 273400: Loss = 0.6428\nEpoch 273500: Loss = 0.6428\nEpoch 273600: Loss = 0.6428\nEpoch 273700: Loss = 0.6428\nEpoch 273800: Loss = 0.6428\nEpoch 273900: Loss = 0.6428\nEpoch 274000: Loss = 0.6428\nEpoch 274100: Loss = 0.6428\nEpoch 274200: Loss = 0.6428\nEpoch 274300: Loss = 0.6428\nEpoch 274400: Loss = 0.6428\nEpoch 274500: Loss = 0.6428\nEpoch 274600: Loss = 0.6428\nEpoch 274700: Loss = 0.6428\nEpoch 274800: Loss = 0.6428\nEpoch 274900: Loss = 0.6428\nEpoch 275000: Loss = 0.6428\nEpoch 275100: Loss = 0.6428\nEpoch 275200: Loss = 0.6428\nEpoch 275300: Loss = 0.6428\nEpoch 275400: Loss = 0.6428\nEpoch 275500: Loss = 0.6428\nEpoch 275600: Loss = 0.6428\nEpoch 275700: Loss = 0.6428\nEpoch 275800: Loss = 0.6428\nEpoch 275900: Loss = 0.6428\nEpoch 276000: Loss = 0.6428\nEpoch 276100: Loss = 0.6428\nEpoch 276200: Loss = 0.6428\nEpoch 276300: Loss = 0.6428\nEpoch 276400: Loss = 0.6428\nEpoch 276500: Loss = 0.6428\nEpoch 276600: Loss = 0.6428\nEpoch 276700: Loss = 0.6428\nEpoch 276800: Loss = 0.6428\nEpoch 276900: Loss = 0.6428\nEpoch 277000: Loss = 0.6428\nEpoch 277100: Loss = 0.6428\nEpoch 277200: Loss = 0.6428\nEpoch 277300: Loss = 0.6428\nEpoch 277400: Loss = 0.6428\nEpoch 277500: Loss = 0.6428\nEpoch 277600: Loss = 0.6428\nEpoch 277700: Loss = 0.6428\nEpoch 277800: Loss = 0.6428\nEpoch 277900: Loss = 0.6428\nEpoch 278000: Loss = 0.6428\nEpoch 278100: Loss = 0.6428\nEpoch 278200: Loss = 0.6428\nEpoch 278300: Loss = 0.6428\nEpoch 278400: Loss = 0.6428\nEpoch 278500: Loss = 0.6427\nEpoch 278600: Loss = 0.6427\nEpoch 278700: Loss = 0.6427\nEpoch 278800: Loss = 0.6427\nEpoch 278900: Loss = 0.6427\nEpoch 279000: Loss = 0.6427\nEpoch 279100: Loss = 0.6427\nEpoch 279200: Loss = 0.6427\nEpoch 279300: Loss = 0.6427\nEpoch 279400: Loss = 0.6427\nEpoch 279500: Loss = 0.6427\nEpoch 279600: Loss = 0.6427\nEpoch 279700: Loss = 0.6427\nEpoch 279800: Loss = 0.6427\nEpoch 279900: Loss = 0.6427\nEpoch 280000: Loss = 0.6427\nEpoch 280100: Loss = 0.6427\nEpoch 280200: Loss = 0.6427\nEpoch 280300: Loss = 0.6427\nEpoch 280400: Loss = 0.6427\nEpoch 280500: Loss = 0.6427\nEpoch 280600: Loss = 0.6427\nEpoch 280700: Loss = 0.6427\nEpoch 280800: Loss = 0.6427\nEpoch 280900: Loss = 0.6427\nEpoch 281000: Loss = 0.6427\nEpoch 281100: Loss = 0.6427\nEpoch 281200: Loss = 0.6427\nEpoch 281300: Loss = 0.6427\nEpoch 281400: Loss = 0.6427\nEpoch 281500: Loss = 0.6427\nEpoch 281600: Loss = 0.6427\nEpoch 281700: Loss = 0.6427\nEpoch 281800: Loss = 0.6427\nEpoch 281900: Loss = 0.6427\nEpoch 282000: Loss = 0.6427\nEpoch 282100: Loss = 0.6427\nEpoch 282200: Loss = 0.6427\nEpoch 282300: Loss = 0.6427\nEpoch 282400: Loss = 0.6427\nEpoch 282500: Loss = 0.6427\nEpoch 282600: Loss = 0.6427\nEpoch 282700: Loss = 0.6427\nEpoch 282800: Loss = 0.6427\nEpoch 282900: Loss = 0.6427\nEpoch 283000: Loss = 0.6427\nEpoch 283100: Loss = 0.6427\nEpoch 283200: Loss = 0.6427\nEpoch 283300: Loss = 0.6427\nEpoch 283400: Loss = 0.6427\nEpoch 283500: Loss = 0.6427\nEpoch 283600: Loss = 0.6427\nEpoch 283700: Loss = 0.6427\nEpoch 283800: Loss = 0.6427\nEpoch 283900: Loss = 0.6427\nEpoch 284000: Loss = 0.6427\nEpoch 284100: Loss = 0.6427\nEpoch 284200: Loss = 0.6427\nEpoch 284300: Loss = 0.6427\nEpoch 284400: Loss = 0.6427\nEpoch 284500: Loss = 0.6427\nEpoch 284600: Loss = 0.6427\nEpoch 284700: Loss = 0.6427\nEpoch 284800: Loss = 0.6427\nEpoch 284900: Loss = 0.6427\nEpoch 285000: Loss = 0.6427\nEpoch 285100: Loss = 0.6427\nEpoch 285200: Loss = 0.6427\nEpoch 285300: Loss = 0.6427\nEpoch 285400: Loss = 0.6427\nEpoch 285500: Loss = 0.6427\nEpoch 285600: Loss = 0.6427\nEpoch 285700: Loss = 0.6427\nEpoch 285800: Loss = 0.6427\nEpoch 285900: Loss = 0.6427\nEpoch 286000: Loss = 0.6427\nEpoch 286100: Loss = 0.6427\nEpoch 286200: Loss = 0.6427\nEpoch 286300: Loss = 0.6427\nEpoch 286400: Loss = 0.6427\nEpoch 286500: Loss = 0.6427\nEpoch 286600: Loss = 0.6427\nEpoch 286700: Loss = 0.6427\nEpoch 286800: Loss = 0.6427\nEpoch 286900: Loss = 0.6427\nEpoch 287000: Loss = 0.6427\nEpoch 287100: Loss = 0.6427\nEpoch 287200: Loss = 0.6427\nEpoch 287300: Loss = 0.6427\nEpoch 287400: Loss = 0.6427\nEpoch 287500: Loss = 0.6427\nEpoch 287600: Loss = 0.6427\nEpoch 287700: Loss = 0.6427\nEpoch 287800: Loss = 0.6427\nEpoch 287900: Loss = 0.6427\nEpoch 288000: Loss = 0.6427\nEpoch 288100: Loss = 0.6427\nEpoch 288200: Loss = 0.6427\nEpoch 288300: Loss = 0.6427\nEpoch 288400: Loss = 0.6427\nEpoch 288500: Loss = 0.6427\nEpoch 288600: Loss = 0.6427\nEpoch 288700: Loss = 0.6427\nEpoch 288800: Loss = 0.6427\nEpoch 288900: Loss = 0.6427\nEpoch 289000: Loss = 0.6427\nEpoch 289100: Loss = 0.6427\nEpoch 289200: Loss = 0.6427\nEpoch 289300: Loss = 0.6427\nEpoch 289400: Loss = 0.6427\nEpoch 289500: Loss = 0.6427\nEpoch 289600: Loss = 0.6427\nEpoch 289700: Loss = 0.6427\nEpoch 289800: Loss = 0.6427\nEpoch 289900: Loss = 0.6427\nEpoch 290000: Loss = 0.6427\nEpoch 290100: Loss = 0.6427\nEpoch 290200: Loss = 0.6427\nEpoch 290300: Loss = 0.6427\nEpoch 290400: Loss = 0.6427\nEpoch 290500: Loss = 0.6427\nEpoch 290600: Loss = 0.6427\nEpoch 290700: Loss = 0.6427\nEpoch 290800: Loss = 0.6427\nEpoch 290900: Loss = 0.6427\nEpoch 291000: Loss = 0.6427\nEpoch 291100: Loss = 0.6427\nEpoch 291200: Loss = 0.6427\nEpoch 291300: Loss = 0.6427\nEpoch 291400: Loss = 0.6427\nEpoch 291500: Loss = 0.6427\nEpoch 291600: Loss = 0.6427\nEpoch 291700: Loss = 0.6427\nEpoch 291800: Loss = 0.6427\nEpoch 291900: Loss = 0.6427\nEpoch 292000: Loss = 0.6427\nEpoch 292100: Loss = 0.6427\nEpoch 292200: Loss = 0.6427\nEpoch 292300: Loss = 0.6427\nEpoch 292400: Loss = 0.6427\nEpoch 292500: Loss = 0.6427\nEpoch 292600: Loss = 0.6427\nEpoch 292700: Loss = 0.6427\nEpoch 292800: Loss = 0.6427\nEpoch 292900: Loss = 0.6427\nEpoch 293000: Loss = 0.6427\nEpoch 293100: Loss = 0.6427\nEpoch 293200: Loss = 0.6427\nEpoch 293300: Loss = 0.6427\nEpoch 293400: Loss = 0.6427\nEpoch 293500: Loss = 0.6427\nEpoch 293600: Loss = 0.6427\nEpoch 293700: Loss = 0.6427\nEpoch 293800: Loss = 0.6427\nEpoch 293900: Loss = 0.6427\nEpoch 294000: Loss = 0.6427\nEpoch 294100: Loss = 0.6427\nEpoch 294200: Loss = 0.6427\nEpoch 294300: Loss = 0.6427\nEpoch 294400: Loss = 0.6427\nEpoch 294500: Loss = 0.6427\nEpoch 294600: Loss = 0.6427\nEpoch 294700: Loss = 0.6427\nEpoch 294800: Loss = 0.6427\nEpoch 294900: Loss = 0.6427\nEpoch 295000: Loss = 0.6427\nEpoch 295100: Loss = 0.6427\nEpoch 295200: Loss = 0.6427\nEpoch 295300: Loss = 0.6427\nEpoch 295400: Loss = 0.6427\nEpoch 295500: Loss = 0.6427\nEpoch 295600: Loss = 0.6427\nEpoch 295700: Loss = 0.6427\nEpoch 295800: Loss = 0.6427\nEpoch 295900: Loss = 0.6427\nEpoch 296000: Loss = 0.6427\nEpoch 296100: Loss = 0.6427\nEpoch 296200: Loss = 0.6427\nEpoch 296300: Loss = 0.6427\nEpoch 296400: Loss = 0.6427\nEpoch 296500: Loss = 0.6427\nEpoch 296600: Loss = 0.6427\nEpoch 296700: Loss = 0.6427\nEpoch 296800: Loss = 0.6427\nEpoch 296900: Loss = 0.6427\nEpoch 297000: Loss = 0.6427\nEpoch 297100: Loss = 0.6427\nEpoch 297200: Loss = 0.6427\nEpoch 297300: Loss = 0.6427\nEpoch 297400: Loss = 0.6427\nEpoch 297500: Loss = 0.6427\nEpoch 297600: Loss = 0.6427\nEpoch 297700: Loss = 0.6427\nEpoch 297800: Loss = 0.6427\nEpoch 297900: Loss = 0.6427\nEpoch 298000: Loss = 0.6427\nEpoch 298100: Loss = 0.6427\nEpoch 298200: Loss = 0.6427\nEpoch 298300: Loss = 0.6427\nEpoch 298400: Loss = 0.6427\nEpoch 298500: Loss = 0.6427\nEpoch 298600: Loss = 0.6427\nEpoch 298700: Loss = 0.6427\nEpoch 298800: Loss = 0.6427\nEpoch 298900: Loss = 0.6427\nEpoch 299000: Loss = 0.6427\nEpoch 299100: Loss = 0.6427\nEpoch 299200: Loss = 0.6427\nEpoch 299300: Loss = 0.6427\nEpoch 299400: Loss = 0.6427\nEpoch 299500: Loss = 0.6427\nEpoch 299600: Loss = 0.6427\nEpoch 299700: Loss = 0.6427\nEpoch 299800: Loss = 0.6427\nEpoch 299900: Loss = 0.6427\nEpoch 300000: Loss = 0.6427\nEpoch 300100: Loss = 0.6427\nEpoch 300200: Loss = 0.6427\nEpoch 300300: Loss = 0.6427\nEpoch 300400: Loss = 0.6427\nEpoch 300500: Loss = 0.6427\nEpoch 300600: Loss = 0.6426\nEpoch 300700: Loss = 0.6427\nEpoch 300800: Loss = 0.6427\nEpoch 300900: Loss = 0.6427\nEpoch 301000: Loss = 0.6426\nEpoch 301100: Loss = 0.6427\nEpoch 301200: Loss = 0.6426\nEpoch 301300: Loss = 0.6426\nEpoch 301400: Loss = 0.6426\nEpoch 301500: Loss = 0.6426\nEpoch 301600: Loss = 0.6426\nEpoch 301700: Loss = 0.6426\nEpoch 301800: Loss = 0.6426\nEpoch 301900: Loss = 0.6426\nEpoch 302000: Loss = 0.6426\nEpoch 302100: Loss = 0.6426\nEpoch 302200: Loss = 0.6426\nEpoch 302300: Loss = 0.6426\nEpoch 302400: Loss = 0.6426\nEpoch 302500: Loss = 0.6426\nEpoch 302600: Loss = 0.6426\nEpoch 302700: Loss = 0.6426\nEpoch 302800: Loss = 0.6426\nEpoch 302900: Loss = 0.6426\nEpoch 303000: Loss = 0.6426\nEpoch 303100: Loss = 0.6426\nEpoch 303200: Loss = 0.6426\nEpoch 303300: Loss = 0.6426\nEpoch 303400: Loss = 0.6426\nEpoch 303500: Loss = 0.6426\nEpoch 303600: Loss = 0.6426\nEpoch 303700: Loss = 0.6426\nEpoch 303800: Loss = 0.6426\nEpoch 303900: Loss = 0.6426\nEpoch 304000: Loss = 0.6426\nEpoch 304100: Loss = 0.6426\nEpoch 304200: Loss = 0.6426\nEpoch 304300: Loss = 0.6426\nEpoch 304400: Loss = 0.6426\nEpoch 304500: Loss = 0.6426\nEpoch 304600: Loss = 0.6426\nEpoch 304700: Loss = 0.6426\nEpoch 304800: Loss = 0.6426\nEpoch 304900: Loss = 0.6426\nEpoch 305000: Loss = 0.6426\nEpoch 305100: Loss = 0.6426\nEpoch 305200: Loss = 0.6426\nEpoch 305300: Loss = 0.6426\nEpoch 305400: Loss = 0.6426\nEpoch 305500: Loss = 0.6426\nEpoch 305600: Loss = 0.6426\nEpoch 305700: Loss = 0.6426\nEpoch 305800: Loss = 0.6426\nEpoch 305900: Loss = 0.6426\nEpoch 306000: Loss = 0.6426\nEpoch 306100: Loss = 0.6426\nEpoch 306200: Loss = 0.6426\nEpoch 306300: Loss = 0.6426\nEpoch 306400: Loss = 0.6426\nEpoch 306500: Loss = 0.6426\nEpoch 306600: Loss = 0.6426\nEpoch 306700: Loss = 0.6426\nEpoch 306800: Loss = 0.6426\nEpoch 306900: Loss = 0.6426\nEpoch 307000: Loss = 0.6426\nEpoch 307100: Loss = 0.6426\nEpoch 307200: Loss = 0.6426\nEpoch 307300: Loss = 0.6426\nEpoch 307400: Loss = 0.6426\nEpoch 307500: Loss = 0.6426\nEpoch 307600: Loss = 0.6426\nEpoch 307700: Loss = 0.6426\nEpoch 307800: Loss = 0.6426\nEpoch 307900: Loss = 0.6426\nEpoch 308000: Loss = 0.6426\nEpoch 308100: Loss = 0.6426\nEpoch 308200: Loss = 0.6426\nEpoch 308300: Loss = 0.6426\nEpoch 308400: Loss = 0.6426\nEpoch 308500: Loss = 0.6426\nEpoch 308600: Loss = 0.6426\nEpoch 308700: Loss = 0.6426\nEpoch 308800: Loss = 0.6426\nEpoch 308900: Loss = 0.6426\nEpoch 309000: Loss = 0.6426\nEpoch 309100: Loss = 0.6426\nEpoch 309200: Loss = 0.6426\nEpoch 309300: Loss = 0.6426\nEpoch 309400: Loss = 0.6426\nEpoch 309500: Loss = 0.6426\nEpoch 309600: Loss = 0.6426\nEpoch 309700: Loss = 0.6426\nEpoch 309800: Loss = 0.6426\nEpoch 309900: Loss = 0.6426\nEpoch 310000: Loss = 0.6426\nEpoch 310100: Loss = 0.6426\nEpoch 310200: Loss = 0.6426\nEpoch 310300: Loss = 0.6426\nEpoch 310400: Loss = 0.6426\nEpoch 310500: Loss = 0.6426\nEpoch 310600: Loss = 0.6426\nEpoch 310700: Loss = 0.6426\nEpoch 310800: Loss = 0.6426\nEpoch 310900: Loss = 0.6426\nEpoch 311000: Loss = 0.6426\nEpoch 311100: Loss = 0.6426\nEpoch 311200: Loss = 0.6426\nEpoch 311300: Loss = 0.6426\nEpoch 311400: Loss = 0.6426\nEpoch 311500: Loss = 0.6426\nEpoch 311600: Loss = 0.6426\nEpoch 311700: Loss = 0.6426\nEpoch 311800: Loss = 0.6426\nEpoch 311900: Loss = 0.6426\nEpoch 312000: Loss = 0.6426\nEpoch 312100: Loss = 0.6426\nEpoch 312200: Loss = 0.6426\nEpoch 312300: Loss = 0.6426\nEpoch 312400: Loss = 0.6426\nEpoch 312500: Loss = 0.6426\nEpoch 312600: Loss = 0.6426\nEpoch 312700: Loss = 0.6426\nEpoch 312800: Loss = 0.6426\nEpoch 312900: Loss = 0.6426\nEpoch 313000: Loss = 0.6426\nEpoch 313100: Loss = 0.6426\nEpoch 313200: Loss = 0.6426\nEpoch 313300: Loss = 0.6426\nEpoch 313400: Loss = 0.6426\nEpoch 313500: Loss = 0.6426\nEpoch 313600: Loss = 0.6426\nEpoch 313700: Loss = 0.6426\nEpoch 313800: Loss = 0.6426\nEpoch 313900: Loss = 0.6426\nEpoch 314000: Loss = 0.6426\nEpoch 314100: Loss = 0.6426\nEpoch 314200: Loss = 0.6426\nEpoch 314300: Loss = 0.6426\nEpoch 314400: Loss = 0.6426\nEpoch 314500: Loss = 0.6426\nEpoch 314600: Loss = 0.6426\nEpoch 314700: Loss = 0.6426\nEpoch 314800: Loss = 0.6426\nEpoch 314900: Loss = 0.6426\nEpoch 315000: Loss = 0.6426\nEpoch 315100: Loss = 0.6426\nEpoch 315200: Loss = 0.6426\nEpoch 315300: Loss = 0.6426\nEpoch 315400: Loss = 0.6426\nEpoch 315500: Loss = 0.6426\nEpoch 315600: Loss = 0.6426\nEpoch 315700: Loss = 0.6426\nEpoch 315800: Loss = 0.6426\nEpoch 315900: Loss = 0.6426\nEpoch 316000: Loss = 0.6426\nEpoch 316100: Loss = 0.6426\nEpoch 316200: Loss = 0.6426\nEpoch 316300: Loss = 0.6426\nEpoch 316400: Loss = 0.6426\nEpoch 316500: Loss = 0.6426\nEpoch 316600: Loss = 0.6426\nEpoch 316700: Loss = 0.6426\nEpoch 316800: Loss = 0.6426\nEpoch 316900: Loss = 0.6426\nEpoch 317000: Loss = 0.6426\nEpoch 317100: Loss = 0.6426\nEpoch 317200: Loss = 0.6426\nEpoch 317300: Loss = 0.6426\nEpoch 317400: Loss = 0.6426\nEpoch 317500: Loss = 0.6426\nEpoch 317600: Loss = 0.6426\nEpoch 317700: Loss = 0.6426\nEpoch 317800: Loss = 0.6426\nEpoch 317900: Loss = 0.6426\nEpoch 318000: Loss = 0.6426\nEpoch 318100: Loss = 0.6426\nEpoch 318200: Loss = 0.6426\nEpoch 318300: Loss = 0.6426\nEpoch 318400: Loss = 0.6426\nEpoch 318500: Loss = 0.6426\nEpoch 318600: Loss = 0.6426\nEpoch 318700: Loss = 0.6426\nEpoch 318800: Loss = 0.6426\nEpoch 318900: Loss = 0.6426\nEpoch 319000: Loss = 0.6426\nEpoch 319100: Loss = 0.6426\nEpoch 319200: Loss = 0.6426\nEpoch 319300: Loss = 0.6426\nEpoch 319400: Loss = 0.6426\nEpoch 319500: Loss = 0.6426\nEpoch 319600: Loss = 0.6426\nEpoch 319700: Loss = 0.6426\nEpoch 319800: Loss = 0.6426\nEpoch 319900: Loss = 0.6426\nEpoch 320000: Loss = 0.6426\nEpoch 320100: Loss = 0.6426\nEpoch 320200: Loss = 0.6426\nEpoch 320300: Loss = 0.6426\nEpoch 320400: Loss = 0.6426\nEpoch 320500: Loss = 0.6426\nEpoch 320600: Loss = 0.6426\nEpoch 320700: Loss = 0.6426\nEpoch 320800: Loss = 0.6426\nEpoch 320900: Loss = 0.6426\nEpoch 321000: Loss = 0.6426\nEpoch 321100: Loss = 0.6426\nEpoch 321200: Loss = 0.6426\nEpoch 321300: Loss = 0.6426\nEpoch 321400: Loss = 0.6426\nEpoch 321500: Loss = 0.6426\nEpoch 321600: Loss = 0.6426\nEpoch 321700: Loss = 0.6426\nEpoch 321800: Loss = 0.6426\nEpoch 321900: Loss = 0.6426\nEpoch 322000: Loss = 0.6426\nEpoch 322100: Loss = 0.6426\nEpoch 322200: Loss = 0.6426\nEpoch 322300: Loss = 0.6426\nEpoch 322400: Loss = 0.6426\nEpoch 322500: Loss = 0.6426\nEpoch 322600: Loss = 0.6426\nEpoch 322700: Loss = 0.6426\nEpoch 322800: Loss = 0.6426\nEpoch 322900: Loss = 0.6426\nEpoch 323000: Loss = 0.6426\nEpoch 323100: Loss = 0.6426\nEpoch 323200: Loss = 0.6426\nEpoch 323300: Loss = 0.6426\nEpoch 323400: Loss = 0.6426\nEpoch 323500: Loss = 0.6426\nEpoch 323600: Loss = 0.6426\nEpoch 323700: Loss = 0.6426\nEpoch 323800: Loss = 0.6426\nEpoch 323900: Loss = 0.6426\nEpoch 324000: Loss = 0.6426\nEpoch 324100: Loss = 0.6426\nEpoch 324200: Loss = 0.6426\nEpoch 324300: Loss = 0.6426\nEpoch 324400: Loss = 0.6426\nEpoch 324500: Loss = 0.6426\nEpoch 324600: Loss = 0.6426\nEpoch 324700: Loss = 0.6426\nEpoch 324800: Loss = 0.6426\nEpoch 324900: Loss = 0.6426\nEpoch 325000: Loss = 0.6426\nEpoch 325100: Loss = 0.6426\nEpoch 325200: Loss = 0.6426\nEpoch 325300: Loss = 0.6426\nEpoch 325400: Loss = 0.6426\nEpoch 325500: Loss = 0.6426\nEpoch 325600: Loss = 0.6426\nEpoch 325700: Loss = 0.6426\nEpoch 325800: Loss = 0.6426\nEpoch 325900: Loss = 0.6425\nEpoch 326000: Loss = 0.6426\nEpoch 326100: Loss = 0.6425\nEpoch 326200: Loss = 0.6426\nEpoch 326300: Loss = 0.6426\nEpoch 326400: Loss = 0.6426\nEpoch 326500: Loss = 0.6425\nEpoch 326600: Loss = 0.6425\nEpoch 326700: Loss = 0.6425\nEpoch 326800: Loss = 0.6425\nEpoch 326900: Loss = 0.6425\nEpoch 327000: Loss = 0.6425\nEpoch 327100: Loss = 0.6425\nEpoch 327200: Loss = 0.6425\nEpoch 327300: Loss = 0.6425\nEpoch 327400: Loss = 0.6425\nEpoch 327500: Loss = 0.6425\nEpoch 327600: Loss = 0.6425\nEpoch 327700: Loss = 0.6425\nEpoch 327800: Loss = 0.6425\nEpoch 327900: Loss = 0.6425\nEpoch 328000: Loss = 0.6425\nEpoch 328100: Loss = 0.6425\nEpoch 328200: Loss = 0.6425\nEpoch 328300: Loss = 0.6425\nEpoch 328400: Loss = 0.6425\nEpoch 328500: Loss = 0.6425\nEpoch 328600: Loss = 0.6425\nEpoch 328700: Loss = 0.6425\nEpoch 328800: Loss = 0.6425\nEpoch 328900: Loss = 0.6425\nEpoch 329000: Loss = 0.6425\nEpoch 329100: Loss = 0.6425\nEpoch 329200: Loss = 0.6425\nEpoch 329300: Loss = 0.6425\nEpoch 329400: Loss = 0.6425\nEpoch 329500: Loss = 0.6425\nEpoch 329600: Loss = 0.6425\nEpoch 329700: Loss = 0.6425\nEpoch 329800: Loss = 0.6425\nEpoch 329900: Loss = 0.6425\nEpoch 330000: Loss = 0.6425\nEpoch 330100: Loss = 0.6425\nEpoch 330200: Loss = 0.6425\nEpoch 330300: Loss = 0.6425\nEpoch 330400: Loss = 0.6425\nEpoch 330500: Loss = 0.6425\nEpoch 330600: Loss = 0.6425\nEpoch 330700: Loss = 0.6425\nEpoch 330800: Loss = 0.6425\nEpoch 330900: Loss = 0.6425\nEpoch 331000: Loss = 0.6425\nEpoch 331100: Loss = 0.6425\nEpoch 331200: Loss = 0.6425\nEpoch 331300: Loss = 0.6425\nEpoch 331400: Loss = 0.6425\nEpoch 331500: Loss = 0.6425\nEpoch 331600: Loss = 0.6425\nEpoch 331700: Loss = 0.6425\nEpoch 331800: Loss = 0.6425\nEpoch 331900: Loss = 0.6425\nEpoch 332000: Loss = 0.6425\nEpoch 332100: Loss = 0.6425\nEpoch 332200: Loss = 0.6425\nEpoch 332300: Loss = 0.6425\nEpoch 332400: Loss = 0.6425\nEpoch 332500: Loss = 0.6425\nEpoch 332600: Loss = 0.6425\nEpoch 332700: Loss = 0.6425\nEpoch 332800: Loss = 0.6425\nEpoch 332900: Loss = 0.6425\nEpoch 333000: Loss = 0.6425\nEpoch 333100: Loss = 0.6425\nEpoch 333200: Loss = 0.6425\nEpoch 333300: Loss = 0.6425\nEpoch 333400: Loss = 0.6425\nEpoch 333500: Loss = 0.6425\nEpoch 333600: Loss = 0.6425\nEpoch 333700: Loss = 0.6425\nEpoch 333800: Loss = 0.6425\nEpoch 333900: Loss = 0.6425\nEpoch 334000: Loss = 0.6425\nEpoch 334100: Loss = 0.6425\nEpoch 334200: Loss = 0.6425\nEpoch 334300: Loss = 0.6425\nEpoch 334400: Loss = 0.6425\nEpoch 334500: Loss = 0.6425\nEpoch 334600: Loss = 0.6425\nEpoch 334700: Loss = 0.6425\nEpoch 334800: Loss = 0.6425\nEpoch 334900: Loss = 0.6425\nEpoch 335000: Loss = 0.6425\nEpoch 335100: Loss = 0.6425\nEpoch 335200: Loss = 0.6425\nEpoch 335300: Loss = 0.6425\nEpoch 335400: Loss = 0.6425\nEpoch 335500: Loss = 0.6425\nEpoch 335600: Loss = 0.6425\nEpoch 335700: Loss = 0.6425\nEpoch 335800: Loss = 0.6425\nEpoch 335900: Loss = 0.6425\nEpoch 336000: Loss = 0.6425\nEpoch 336100: Loss = 0.6425\nEpoch 336200: Loss = 0.6425\nEpoch 336300: Loss = 0.6425\nEpoch 336400: Loss = 0.6425\nEpoch 336500: Loss = 0.6425\nEpoch 336600: Loss = 0.6425\nEpoch 336700: Loss = 0.6425\nEpoch 336800: Loss = 0.6425\nEpoch 336900: Loss = 0.6425\nEpoch 337000: Loss = 0.6425\nEpoch 337100: Loss = 0.6425\nEpoch 337200: Loss = 0.6425\nEpoch 337300: Loss = 0.6425\nEpoch 337400: Loss = 0.6425\nEpoch 337500: Loss = 0.6425\nEpoch 337600: Loss = 0.6425\nEpoch 337700: Loss = 0.6425\nEpoch 337800: Loss = 0.6425\nEpoch 337900: Loss = 0.6425\nEpoch 338000: Loss = 0.6425\nEpoch 338100: Loss = 0.6425\nEpoch 338200: Loss = 0.6425\nEpoch 338300: Loss = 0.6425\nEpoch 338400: Loss = 0.6425\nEpoch 338500: Loss = 0.6425\nEpoch 338600: Loss = 0.6425\nEpoch 338700: Loss = 0.6425\nEpoch 338800: Loss = 0.6425\nEpoch 338900: Loss = 0.6425\nEpoch 339000: Loss = 0.6425\nEpoch 339100: Loss = 0.6425\nEpoch 339200: Loss = 0.6425\nEpoch 339300: Loss = 0.6425\nEpoch 339400: Loss = 0.6425\nEpoch 339500: Loss = 0.6425\nEpoch 339600: Loss = 0.6425\nEpoch 339700: Loss = 0.6425\nEpoch 339800: Loss = 0.6425\nEpoch 339900: Loss = 0.6425\nEpoch 340000: Loss = 0.6425\nEpoch 340100: Loss = 0.6425\nEpoch 340200: Loss = 0.6425\nEpoch 340300: Loss = 0.6425\nEpoch 340400: Loss = 0.6425\nEpoch 340500: Loss = 0.6425\nEpoch 340600: Loss = 0.6425\nEpoch 340700: Loss = 0.6425\nEpoch 340800: Loss = 0.6425\nEpoch 340900: Loss = 0.6425\nEpoch 341000: Loss = 0.6425\nEpoch 341100: Loss = 0.6425\nEpoch 341200: Loss = 0.6425\nEpoch 341300: Loss = 0.6425\nEpoch 341400: Loss = 0.6425\nEpoch 341500: Loss = 0.6425\nEpoch 341600: Loss = 0.6425\nEpoch 341700: Loss = 0.6425\nEpoch 341800: Loss = 0.6425\nEpoch 341900: Loss = 0.6425\nEpoch 342000: Loss = 0.6425\nEpoch 342100: Loss = 0.6425\nEpoch 342200: Loss = 0.6425\nEpoch 342300: Loss = 0.6425\nEpoch 342400: Loss = 0.6425\nEpoch 342500: Loss = 0.6425\nEpoch 342600: Loss = 0.6425\nEpoch 342700: Loss = 0.6425\nEpoch 342800: Loss = 0.6425\nEpoch 342900: Loss = 0.6425\nEpoch 343000: Loss = 0.6425\nEpoch 343100: Loss = 0.6425\nEpoch 343200: Loss = 0.6425\nEpoch 343300: Loss = 0.6425\nEpoch 343400: Loss = 0.6425\nEpoch 343500: Loss = 0.6425\nEpoch 343600: Loss = 0.6425\nEpoch 343700: Loss = 0.6425\nEpoch 343800: Loss = 0.6425\nEpoch 343900: Loss = 0.6425\nEpoch 344000: Loss = 0.6425\nEpoch 344100: Loss = 0.6425\nEpoch 344200: Loss = 0.6425\nEpoch 344300: Loss = 0.6425\nEpoch 344400: Loss = 0.6425\nEpoch 344500: Loss = 0.6425\nEpoch 344600: Loss = 0.6425\nEpoch 344700: Loss = 0.6425\nEpoch 344800: Loss = 0.6425\nEpoch 344900: Loss = 0.6425\nEpoch 345000: Loss = 0.6425\nEpoch 345100: Loss = 0.6425\nEpoch 345200: Loss = 0.6425\nEpoch 345300: Loss = 0.6425\nEpoch 345400: Loss = 0.6425\nEpoch 345500: Loss = 0.6425\nEpoch 345600: Loss = 0.6425\nEpoch 345700: Loss = 0.6425\nEpoch 345800: Loss = 0.6425\nEpoch 345900: Loss = 0.6425\nEpoch 346000: Loss = 0.6425\nEpoch 346100: Loss = 0.6425\nEpoch 346200: Loss = 0.6425\nEpoch 346300: Loss = 0.6425\nEpoch 346400: Loss = 0.6425\nEpoch 346500: Loss = 0.6425\nEpoch 346600: Loss = 0.6425\nEpoch 346700: Loss = 0.6425\nEpoch 346800: Loss = 0.6425\nEpoch 346900: Loss = 0.6425\nEpoch 347000: Loss = 0.6425\nEpoch 347100: Loss = 0.6425\nEpoch 347200: Loss = 0.6425\nEpoch 347300: Loss = 0.6425\nEpoch 347400: Loss = 0.6425\nEpoch 347500: Loss = 0.6425\nEpoch 347600: Loss = 0.6425\nEpoch 347700: Loss = 0.6425\nEpoch 347800: Loss = 0.6425\nEpoch 347900: Loss = 0.6425\nEpoch 348000: Loss = 0.6425\nEpoch 348100: Loss = 0.6425\nEpoch 348200: Loss = 0.6425\nEpoch 348300: Loss = 0.6425\nEpoch 348400: Loss = 0.6425\nEpoch 348500: Loss = 0.6425\nEpoch 348600: Loss = 0.6425\nEpoch 348700: Loss = 0.6425\nEpoch 348800: Loss = 0.6425\nEpoch 348900: Loss = 0.6425\nEpoch 349000: Loss = 0.6425\nEpoch 349100: Loss = 0.6425\nEpoch 349200: Loss = 0.6425\nEpoch 349300: Loss = 0.6425\nEpoch 349400: Loss = 0.6425\nEpoch 349500: Loss = 0.6425\nEpoch 349600: Loss = 0.6425\nEpoch 349700: Loss = 0.6425\nEpoch 349800: Loss = 0.6425\nEpoch 349900: Loss = 0.6425\nEpoch 350000: Loss = 0.6425\nEpoch 350100: Loss = 0.6425\nEpoch 350200: Loss = 0.6425\nEpoch 350300: Loss = 0.6425\nEpoch 350400: Loss = 0.6425\nEpoch 350500: Loss = 0.6425\nEpoch 350600: Loss = 0.6425\nEpoch 350700: Loss = 0.6425\nEpoch 350800: Loss = 0.6425\nEpoch 350900: Loss = 0.6425\nEpoch 351000: Loss = 0.6425\nEpoch 351100: Loss = 0.6425\nEpoch 351200: Loss = 0.6425\nEpoch 351300: Loss = 0.6425\nEpoch 351400: Loss = 0.6425\nEpoch 351500: Loss = 0.6425\nEpoch 351600: Loss = 0.6425\nEpoch 351700: Loss = 0.6425\nEpoch 351800: Loss = 0.6425\nEpoch 351900: Loss = 0.6425\nEpoch 352000: Loss = 0.6425\nEpoch 352100: Loss = 0.6425\nEpoch 352200: Loss = 0.6425\nEpoch 352300: Loss = 0.6425\nEpoch 352400: Loss = 0.6425\nEpoch 352500: Loss = 0.6425\nEpoch 352600: Loss = 0.6425\nEpoch 352700: Loss = 0.6425\nEpoch 352800: Loss = 0.6425\nEpoch 352900: Loss = 0.6425\nEpoch 353000: Loss = 0.6425\nEpoch 353100: Loss = 0.6425\nEpoch 353200: Loss = 0.6425\nEpoch 353300: Loss = 0.6425\nEpoch 353400: Loss = 0.6425\nEpoch 353500: Loss = 0.6425\nEpoch 353600: Loss = 0.6425\nEpoch 353700: Loss = 0.6425\nEpoch 353800: Loss = 0.6425\nEpoch 353900: Loss = 0.6425\nEpoch 354000: Loss = 0.6425\nEpoch 354100: Loss = 0.6425\nEpoch 354200: Loss = 0.6425\nEpoch 354300: Loss = 0.6425\nEpoch 354400: Loss = 0.6425\nEpoch 354500: Loss = 0.6425\nEpoch 354600: Loss = 0.6425\nEpoch 354700: Loss = 0.6425\nEpoch 354800: Loss = 0.6425\nEpoch 354900: Loss = 0.6425\nEpoch 355000: Loss = 0.6424\nEpoch 355100: Loss = 0.6425\nEpoch 355200: Loss = 0.6424\nEpoch 355300: Loss = 0.6425\nEpoch 355400: Loss = 0.6424\nEpoch 355500: Loss = 0.6424\nEpoch 355600: Loss = 0.6424\nEpoch 355700: Loss = 0.6424\nEpoch 355800: Loss = 0.6424\nEpoch 355900: Loss = 0.6424\nEpoch 356000: Loss = 0.6425\nEpoch 356100: Loss = 0.6424\nEpoch 356200: Loss = 0.6424\nEpoch 356300: Loss = 0.6424\nEpoch 356400: Loss = 0.6424\nEpoch 356500: Loss = 0.6424\nEpoch 356600: Loss = 0.6424\nEpoch 356700: Loss = 0.6424\nEpoch 356800: Loss = 0.6424\nEpoch 356900: Loss = 0.6424\nEpoch 357000: Loss = 0.6424\nEpoch 357100: Loss = 0.6424\nEpoch 357200: Loss = 0.6424\nEpoch 357300: Loss = 0.6424\nEpoch 357400: Loss = 0.6424\nEpoch 357500: Loss = 0.6424\nEpoch 357600: Loss = 0.6424\nEpoch 357700: Loss = 0.6424\nEpoch 357800: Loss = 0.6424\nEpoch 357900: Loss = 0.6424\nEpoch 358000: Loss = 0.6424\nEpoch 358100: Loss = 0.6424\nEpoch 358200: Loss = 0.6424\nEpoch 358300: Loss = 0.6424\nEpoch 358400: Loss = 0.6424\nEpoch 358500: Loss = 0.6424\nEpoch 358600: Loss = 0.6424\nEpoch 358700: Loss = 0.6424\nEpoch 358800: Loss = 0.6424\nEpoch 358900: Loss = 0.6424\nEpoch 359000: Loss = 0.6424\nEpoch 359100: Loss = 0.6424\nEpoch 359200: Loss = 0.6424\nEpoch 359300: Loss = 0.6424\nEpoch 359400: Loss = 0.6424\nEpoch 359500: Loss = 0.6424\nEpoch 359600: Loss = 0.6424\nEpoch 359700: Loss = 0.6424\nEpoch 359800: Loss = 0.6424\nEpoch 359900: Loss = 0.6424\nEpoch 360000: Loss = 0.6424\nEpoch 360100: Loss = 0.6424\nEpoch 360200: Loss = 0.6424\nEpoch 360300: Loss = 0.6424\nEpoch 360400: Loss = 0.6424\nEpoch 360500: Loss = 0.6424\nEpoch 360600: Loss = 0.6424\nEpoch 360700: Loss = 0.6424\nEpoch 360800: Loss = 0.6424\nEpoch 360900: Loss = 0.6424\nEpoch 361000: Loss = 0.6424\nEpoch 361100: Loss = 0.6424\nEpoch 361200: Loss = 0.6424\nEpoch 361300: Loss = 0.6424\nEpoch 361400: Loss = 0.6424\nEpoch 361500: Loss = 0.6424\nEpoch 361600: Loss = 0.6424\nEpoch 361700: Loss = 0.6424\nEpoch 361800: Loss = 0.6424\nEpoch 361900: Loss = 0.6424\nEpoch 362000: Loss = 0.6424\nEpoch 362100: Loss = 0.6424\nEpoch 362200: Loss = 0.6424\nEpoch 362300: Loss = 0.6424\nEpoch 362400: Loss = 0.6424\nEpoch 362500: Loss = 0.6424\nEpoch 362600: Loss = 0.6424\nEpoch 362700: Loss = 0.6424\nEpoch 362800: Loss = 0.6424\nEpoch 362900: Loss = 0.6424\nEpoch 363000: Loss = 0.6424\nEpoch 363100: Loss = 0.6424\nEpoch 363200: Loss = 0.6424\nEpoch 363300: Loss = 0.6424\nEpoch 363400: Loss = 0.6424\nEpoch 363500: Loss = 0.6424\nEpoch 363600: Loss = 0.6424\nEpoch 363700: Loss = 0.6424\nEpoch 363800: Loss = 0.6424\nEpoch 363900: Loss = 0.6424\nEpoch 364000: Loss = 0.6424\nEpoch 364100: Loss = 0.6424\nEpoch 364200: Loss = 0.6424\nEpoch 364300: Loss = 0.6424\nEpoch 364400: Loss = 0.6424\nEpoch 364500: Loss = 0.6424\nEpoch 364600: Loss = 0.6424\nEpoch 364700: Loss = 0.6424\nEpoch 364800: Loss = 0.6424\nEpoch 364900: Loss = 0.6424\nEpoch 365000: Loss = 0.6424\nEpoch 365100: Loss = 0.6424\nEpoch 365200: Loss = 0.6424\nEpoch 365300: Loss = 0.6424\nEpoch 365400: Loss = 0.6424\nEpoch 365500: Loss = 0.6424\nEpoch 365600: Loss = 0.6424\nEpoch 365700: Loss = 0.6424\nEpoch 365800: Loss = 0.6424\nEpoch 365900: Loss = 0.6424\nEpoch 366000: Loss = 0.6424\nEpoch 366100: Loss = 0.6424\nEpoch 366200: Loss = 0.6424\nEpoch 366300: Loss = 0.6424\nEpoch 366400: Loss = 0.6424\nEpoch 366500: Loss = 0.6424\nEpoch 366600: Loss = 0.6424\nEpoch 366700: Loss = 0.6424\nEpoch 366800: Loss = 0.6424\nEpoch 366900: Loss = 0.6424\nEpoch 367000: Loss = 0.6424\nEpoch 367100: Loss = 0.6424\nEpoch 367200: Loss = 0.6424\nEpoch 367300: Loss = 0.6424\nEpoch 367400: Loss = 0.6424\nEpoch 367500: Loss = 0.6424\nEpoch 367600: Loss = 0.6424\nEpoch 367700: Loss = 0.6424\nEpoch 367800: Loss = 0.6424\nEpoch 367900: Loss = 0.6424\nEpoch 368000: Loss = 0.6424\nEpoch 368100: Loss = 0.6424\nEpoch 368200: Loss = 0.6424\nEpoch 368300: Loss = 0.6424\nEpoch 368400: Loss = 0.6424\nEpoch 368500: Loss = 0.6424\nEpoch 368600: Loss = 0.6424\nEpoch 368700: Loss = 0.6424\nEpoch 368800: Loss = 0.6424\nEpoch 368900: Loss = 0.6424\nEpoch 369000: Loss = 0.6424\nEpoch 369100: Loss = 0.6424\nEpoch 369200: Loss = 0.6424\nEpoch 369300: Loss = 0.6424\nEpoch 369400: Loss = 0.6424\nEpoch 369500: Loss = 0.6424\nEpoch 369600: Loss = 0.6424\nEpoch 369700: Loss = 0.6424\nEpoch 369800: Loss = 0.6424\nEpoch 369900: Loss = 0.6424\nEpoch 370000: Loss = 0.6424\nEpoch 370100: Loss = 0.6424\nEpoch 370200: Loss = 0.6424\nEpoch 370300: Loss = 0.6424\nEpoch 370400: Loss = 0.6424\nEpoch 370500: Loss = 0.6424\nEpoch 370600: Loss = 0.6424\nEpoch 370700: Loss = 0.6424\nEpoch 370800: Loss = 0.6424\nEpoch 370900: Loss = 0.6424\nEpoch 371000: Loss = 0.6424\nEpoch 371100: Loss = 0.6424\nEpoch 371200: Loss = 0.6424\nEpoch 371300: Loss = 0.6424\nEpoch 371400: Loss = 0.6424\nEpoch 371500: Loss = 0.6424\nEpoch 371600: Loss = 0.6424\nEpoch 371700: Loss = 0.6424\nEpoch 371800: Loss = 0.6424\nEpoch 371900: Loss = 0.6424\nEpoch 372000: Loss = 0.6424\nEpoch 372100: Loss = 0.6424\nEpoch 372200: Loss = 0.6424\nEpoch 372300: Loss = 0.6424\nEpoch 372400: Loss = 0.6424\nEpoch 372500: Loss = 0.6424\nEpoch 372600: Loss = 0.6424\nEpoch 372700: Loss = 0.6424\nEpoch 372800: Loss = 0.6424\nEpoch 372900: Loss = 0.6424\nEpoch 373000: Loss = 0.6424\nEpoch 373100: Loss = 0.6424\nEpoch 373200: Loss = 0.6424\nEpoch 373300: Loss = 0.6424\nEpoch 373400: Loss = 0.6424\nEpoch 373500: Loss = 0.6424\nEpoch 373600: Loss = 0.6424\nEpoch 373700: Loss = 0.6424\nEpoch 373800: Loss = 0.6424\nEpoch 373900: Loss = 0.6424\nEpoch 374000: Loss = 0.6424\nEpoch 374100: Loss = 0.6424\nEpoch 374200: Loss = 0.6424\nEpoch 374300: Loss = 0.6424\nEpoch 374400: Loss = 0.6424\nEpoch 374500: Loss = 0.6424\nEpoch 374600: Loss = 0.6424\nEpoch 374700: Loss = 0.6424\nEpoch 374800: Loss = 0.6424\nEpoch 374900: Loss = 0.6424\nEpoch 375000: Loss = 0.6424\nEpoch 375100: Loss = 0.6424\nEpoch 375200: Loss = 0.6424\nEpoch 375300: Loss = 0.6424\nEpoch 375400: Loss = 0.6424\nEpoch 375500: Loss = 0.6424\nEpoch 375600: Loss = 0.6424\nEpoch 375700: Loss = 0.6424\nEpoch 375800: Loss = 0.6424\nEpoch 375900: Loss = 0.6424\nEpoch 376000: Loss = 0.6424\nEpoch 376100: Loss = 0.6424\nEpoch 376200: Loss = 0.6424\nEpoch 376300: Loss = 0.6424\nEpoch 376400: Loss = 0.6424\nEpoch 376500: Loss = 0.6424\nEpoch 376600: Loss = 0.6424\nEpoch 376700: Loss = 0.6424\nEpoch 376800: Loss = 0.6424\nEpoch 376900: Loss = 0.6424\nEpoch 377000: Loss = 0.6424\nEpoch 377100: Loss = 0.6424\nEpoch 377200: Loss = 0.6424\nEpoch 377300: Loss = 0.6424\nEpoch 377400: Loss = 0.6424\nEpoch 377500: Loss = 0.6424\nEpoch 377600: Loss = 0.6424\nEpoch 377700: Loss = 0.6424\nEpoch 377800: Loss = 0.6424\nEpoch 377900: Loss = 0.6424\nEpoch 378000: Loss = 0.6424\nEpoch 378100: Loss = 0.6424\nEpoch 378200: Loss = 0.6424\nEpoch 378300: Loss = 0.6424\nEpoch 378400: Loss = 0.6424\nEpoch 378500: Loss = 0.6424\nEpoch 378600: Loss = 0.6424\nEpoch 378700: Loss = 0.6424\nEpoch 378800: Loss = 0.6424\nEpoch 378900: Loss = 0.6424\nEpoch 379000: Loss = 0.6424\nEpoch 379100: Loss = 0.6424\nEpoch 379200: Loss = 0.6424\nEpoch 379300: Loss = 0.6424\nEpoch 379400: Loss = 0.6424\nEpoch 379500: Loss = 0.6424\nEpoch 379600: Loss = 0.6424\nEpoch 379700: Loss = 0.6424\nEpoch 379800: Loss = 0.6424\nEpoch 379900: Loss = 0.6424\nEpoch 380000: Loss = 0.6424\nEpoch 380100: Loss = 0.6424\nEpoch 380200: Loss = 0.6424\nEpoch 380300: Loss = 0.6424\nEpoch 380400: Loss = 0.6424\nEpoch 380500: Loss = 0.6424\nEpoch 380600: Loss = 0.6424\nEpoch 380700: Loss = 0.6424\nEpoch 380800: Loss = 0.6424\nEpoch 380900: Loss = 0.6424\nEpoch 381000: Loss = 0.6424\nEpoch 381100: Loss = 0.6424\nEpoch 381200: Loss = 0.6424\nEpoch 381300: Loss = 0.6424\nEpoch 381400: Loss = 0.6424\nEpoch 381500: Loss = 0.6424\nEpoch 381600: Loss = 0.6424\nEpoch 381700: Loss = 0.6424\nEpoch 381800: Loss = 0.6424\nEpoch 381900: Loss = 0.6424\nEpoch 382000: Loss = 0.6424\nEpoch 382100: Loss = 0.6424\nEpoch 382200: Loss = 0.6424\nEpoch 382300: Loss = 0.6424\nEpoch 382400: Loss = 0.6424\nEpoch 382500: Loss = 0.6424\nEpoch 382600: Loss = 0.6424\nEpoch 382700: Loss = 0.6424\nEpoch 382800: Loss = 0.6424\nEpoch 382900: Loss = 0.6424\nEpoch 383000: Loss = 0.6424\nEpoch 383100: Loss = 0.6424\nEpoch 383200: Loss = 0.6424\nEpoch 383300: Loss = 0.6424\nEpoch 383400: Loss = 0.6424\nEpoch 383500: Loss = 0.6424\nEpoch 383600: Loss = 0.6424\nEpoch 383700: Loss = 0.6424\nEpoch 383800: Loss = 0.6424\nEpoch 383900: Loss = 0.6424\nEpoch 384000: Loss = 0.6424\nEpoch 384100: Loss = 0.6424\nEpoch 384200: Loss = 0.6424\nEpoch 384300: Loss = 0.6424\nEpoch 384400: Loss = 0.6424\nEpoch 384500: Loss = 0.6423\nEpoch 384600: Loss = 0.6424\nEpoch 384700: Loss = 0.6423\nEpoch 384800: Loss = 0.6423\nEpoch 384900: Loss = 0.6423\nEpoch 385000: Loss = 0.6423\nEpoch 385100: Loss = 0.6423\nEpoch 385200: Loss = 0.6423\nEpoch 385300: Loss = 0.6423\nEpoch 385400: Loss = 0.6423\nEpoch 385500: Loss = 0.6423\nEpoch 385600: Loss = 0.6423\nEpoch 385700: Loss = 0.6424\nEpoch 385800: Loss = 0.6423\nEpoch 385900: Loss = 0.6423\nEpoch 386000: Loss = 0.6423\nEpoch 386100: Loss = 0.6423\nEpoch 386200: Loss = 0.6423\nEpoch 386300: Loss = 0.6423\nEpoch 386400: Loss = 0.6423\nEpoch 386500: Loss = 0.6423\nEpoch 386600: Loss = 0.6423\nEpoch 386700: Loss = 0.6423\nEpoch 386800: Loss = 0.6423\nEpoch 386900: Loss = 0.6423\nEpoch 387000: Loss = 0.6423\nEpoch 387100: Loss = 0.6423\nEpoch 387200: Loss = 0.6423\nEpoch 387300: Loss = 0.6423\nEpoch 387400: Loss = 0.6423\nEpoch 387500: Loss = 0.6423\nEpoch 387600: Loss = 0.6423\nEpoch 387700: Loss = 0.6423\nEpoch 387800: Loss = 0.6423\nEpoch 387900: Loss = 0.6423\nEpoch 388000: Loss = 0.6423\nEpoch 388100: Loss = 0.6423\nEpoch 388200: Loss = 0.6423\nEpoch 388300: Loss = 0.6423\nEpoch 388400: Loss = 0.6423\nEpoch 388500: Loss = 0.6423\nEpoch 388600: Loss = 0.6423\nEpoch 388700: Loss = 0.6423\nEpoch 388800: Loss = 0.6423\nEpoch 388900: Loss = 0.6423\nEpoch 389000: Loss = 0.6423\nEpoch 389100: Loss = 0.6423\nEpoch 389200: Loss = 0.6423\nEpoch 389300: Loss = 0.6423\nEpoch 389400: Loss = 0.6423\nEpoch 389500: Loss = 0.6423\nEpoch 389600: Loss = 0.6423\nEpoch 389700: Loss = 0.6423\nEpoch 389800: Loss = 0.6423\nEpoch 389900: Loss = 0.6423\nEpoch 390000: Loss = 0.6423\nEpoch 390100: Loss = 0.6423\nEpoch 390200: Loss = 0.6423\nEpoch 390300: Loss = 0.6423\nEpoch 390400: Loss = 0.6423\nEpoch 390500: Loss = 0.6423\nEpoch 390600: Loss = 0.6423\nEpoch 390700: Loss = 0.6423\nEpoch 390800: Loss = 0.6423\nEpoch 390900: Loss = 0.6423\nEpoch 391000: Loss = 0.6423\nEpoch 391100: Loss = 0.6423\nEpoch 391200: Loss = 0.6423\nEpoch 391300: Loss = 0.6423\nEpoch 391400: Loss = 0.6423\nEpoch 391500: Loss = 0.6423\nEpoch 391600: Loss = 0.6423\nEpoch 391700: Loss = 0.6423\nEpoch 391800: Loss = 0.6423\nEpoch 391900: Loss = 0.6423\nEpoch 392000: Loss = 0.6423\nEpoch 392100: Loss = 0.6423\nEpoch 392200: Loss = 0.6423\nEpoch 392300: Loss = 0.6423\nEpoch 392400: Loss = 0.6423\nEpoch 392500: Loss = 0.6423\nEpoch 392600: Loss = 0.6423\nEpoch 392700: Loss = 0.6423\nEpoch 392800: Loss = 0.6423\nEpoch 392900: Loss = 0.6423\nEpoch 393000: Loss = 0.6423\nEpoch 393100: Loss = 0.6423\nEpoch 393200: Loss = 0.6423\nEpoch 393300: Loss = 0.6423\nEpoch 393400: Loss = 0.6423\nEpoch 393500: Loss = 0.6423\nEpoch 393600: Loss = 0.6423\nEpoch 393700: Loss = 0.6423\nEpoch 393800: Loss = 0.6423\nEpoch 393900: Loss = 0.6423\nEpoch 394000: Loss = 0.6423\nEpoch 394100: Loss = 0.6423\nEpoch 394200: Loss = 0.6423\nEpoch 394300: Loss = 0.6423\nEpoch 394400: Loss = 0.6423\nEpoch 394500: Loss = 0.6423\nEpoch 394600: Loss = 0.6423\nEpoch 394700: Loss = 0.6423\nEpoch 394800: Loss = 0.6423\nEpoch 394900: Loss = 0.6423\nEpoch 395000: Loss = 0.6423\nEpoch 395100: Loss = 0.6423\nEpoch 395200: Loss = 0.6423\nEpoch 395300: Loss = 0.6423\nEpoch 395400: Loss = 0.6423\nEpoch 395500: Loss = 0.6423\nEpoch 395600: Loss = 0.6423\nEpoch 395700: Loss = 0.6423\nEpoch 395800: Loss = 0.6423\nEpoch 395900: Loss = 0.6423\nEpoch 396000: Loss = 0.6423\nEpoch 396100: Loss = 0.6423\nEpoch 396200: Loss = 0.6423\nEpoch 396300: Loss = 0.6423\nEpoch 396400: Loss = 0.6423\nEpoch 396500: Loss = 0.6423\nEpoch 396600: Loss = 0.6423\nEpoch 396700: Loss = 0.6423\nEpoch 396800: Loss = 0.6423\nEpoch 396900: Loss = 0.6423\nEpoch 397000: Loss = 0.6423\nEpoch 397100: Loss = 0.6423\nEpoch 397200: Loss = 0.6423\nEpoch 397300: Loss = 0.6423\nEpoch 397400: Loss = 0.6423\nEpoch 397500: Loss = 0.6423\nEpoch 397600: Loss = 0.6423\nEpoch 397700: Loss = 0.6423\nEpoch 397800: Loss = 0.6423\nEpoch 397900: Loss = 0.6423\nEpoch 398000: Loss = 0.6423\nEpoch 398100: Loss = 0.6423\nEpoch 398200: Loss = 0.6423\nEpoch 398300: Loss = 0.6423\nEpoch 398400: Loss = 0.6423\nEpoch 398500: Loss = 0.6423\nEpoch 398600: Loss = 0.6423\nEpoch 398700: Loss = 0.6423\nEpoch 398800: Loss = 0.6423\nEpoch 398900: Loss = 0.6423\nEpoch 399000: Loss = 0.6423\nEpoch 399100: Loss = 0.6423\nEpoch 399200: Loss = 0.6423\nEpoch 399300: Loss = 0.6423\nEpoch 399400: Loss = 0.6423\nEpoch 399500: Loss = 0.6423\nEpoch 399600: Loss = 0.6423\nEpoch 399700: Loss = 0.6423\nEpoch 399800: Loss = 0.6423\nEpoch 399900: Loss = 0.6423\nEpoch 400000: Loss = 0.6423\nEpoch 400100: Loss = 0.6423\nEpoch 400200: Loss = 0.6423\nEpoch 400300: Loss = 0.6423\nEpoch 400400: Loss = 0.6423\nEpoch 400500: Loss = 0.6423\nEpoch 400600: Loss = 0.6423\nEpoch 400700: Loss = 0.6423\nEpoch 400800: Loss = 0.6423\nEpoch 400900: Loss = 0.6423\nEpoch 401000: Loss = 0.6423\nEpoch 401100: Loss = 0.6423\nEpoch 401200: Loss = 0.6423\nEpoch 401300: Loss = 0.6423\nEpoch 401400: Loss = 0.6423\nEpoch 401500: Loss = 0.6423\nEpoch 401600: Loss = 0.6423\nEpoch 401700: Loss = 0.6423\nEpoch 401800: Loss = 0.6423\nEpoch 401900: Loss = 0.6423\nEpoch 402000: Loss = 0.6423\nEpoch 402100: Loss = 0.6423\nEpoch 402200: Loss = 0.6423\nEpoch 402300: Loss = 0.6423\nEpoch 402400: Loss = 0.6423\nEpoch 402500: Loss = 0.6423\nEpoch 402600: Loss = 0.6423\nEpoch 402700: Loss = 0.6423\nEpoch 402800: Loss = 0.6423\nEpoch 402900: Loss = 0.6423\nEpoch 403000: Loss = 0.6423\nEpoch 403100: Loss = 0.6423\nEpoch 403200: Loss = 0.6423\nEpoch 403300: Loss = 0.6423\nEpoch 403400: Loss = 0.6423\nEpoch 403500: Loss = 0.6423\nEpoch 403600: Loss = 0.6423\nEpoch 403700: Loss = 0.6423\nEpoch 403800: Loss = 0.6423\nEpoch 403900: Loss = 0.6423\nEpoch 404000: Loss = 0.6423\nEpoch 404100: Loss = 0.6423\nEpoch 404200: Loss = 0.6423\nEpoch 404300: Loss = 0.6423\nEpoch 404400: Loss = 0.6423\nEpoch 404500: Loss = 0.6423\nEpoch 404600: Loss = 0.6423\nEpoch 404700: Loss = 0.6423\nEpoch 404800: Loss = 0.6423\nEpoch 404900: Loss = 0.6423\nEpoch 405000: Loss = 0.6423\nEpoch 405100: Loss = 0.6423\nEpoch 405200: Loss = 0.6423\nEpoch 405300: Loss = 0.6423\nEpoch 405400: Loss = 0.6423\nEpoch 405500: Loss = 0.6423\nEpoch 405600: Loss = 0.6423\nEpoch 405700: Loss = 0.6423\nEpoch 405800: Loss = 0.6423\nEpoch 405900: Loss = 0.6423\nEpoch 406000: Loss = 0.6423\nEpoch 406100: Loss = 0.6423\nEpoch 406200: Loss = 0.6423\nEpoch 406300: Loss = 0.6423\nEpoch 406400: Loss = 0.6423\nEpoch 406500: Loss = 0.6423\nEpoch 406600: Loss = 0.6423\nEpoch 406700: Loss = 0.6423\nEpoch 406800: Loss = 0.6423\nEpoch 406900: Loss = 0.6423\nEpoch 407000: Loss = 0.6423\nEpoch 407100: Loss = 0.6423\nEpoch 407200: Loss = 0.6423\nEpoch 407300: Loss = 0.6423\nEpoch 407400: Loss = 0.6423\nEpoch 407500: Loss = 0.6423\nEpoch 407600: Loss = 0.6423\nEpoch 407700: Loss = 0.6423\nEpoch 407800: Loss = 0.6423\nEpoch 407900: Loss = 0.6423\nEpoch 408000: Loss = 0.6423\nEpoch 408100: Loss = 0.6423\nEpoch 408200: Loss = 0.6423\nEpoch 408300: Loss = 0.6423\nEpoch 408400: Loss = 0.6423\nEpoch 408500: Loss = 0.6423\nEpoch 408600: Loss = 0.6423\nEpoch 408700: Loss = 0.6423\nEpoch 408800: Loss = 0.6423\nEpoch 408900: Loss = 0.6423\nEpoch 409000: Loss = 0.6423\nEpoch 409100: Loss = 0.6423\nEpoch 409200: Loss = 0.6423\nEpoch 409300: Loss = 0.6423\nEpoch 409400: Loss = 0.6423\nEpoch 409500: Loss = 0.6423\nEpoch 409600: Loss = 0.6423\nEpoch 409700: Loss = 0.6423\nEpoch 409800: Loss = 0.6423\nEpoch 409900: Loss = 0.6423\nEpoch 410000: Loss = 0.6423\nEpoch 410100: Loss = 0.6423\nEpoch 410200: Loss = 0.6423\nEpoch 410300: Loss = 0.6423\nEpoch 410400: Loss = 0.6423\nEpoch 410500: Loss = 0.6423\nEpoch 410600: Loss = 0.6423\nEpoch 410700: Loss = 0.6423\nEpoch 410800: Loss = 0.6423\nEpoch 410900: Loss = 0.6423\nEpoch 411000: Loss = 0.6423\nEpoch 411100: Loss = 0.6423\nEpoch 411200: Loss = 0.6423\nEpoch 411300: Loss = 0.6423\nEpoch 411400: Loss = 0.6423\nEpoch 411500: Loss = 0.6423\nEpoch 411600: Loss = 0.6423\nEpoch 411700: Loss = 0.6423\nEpoch 411800: Loss = 0.6423\nEpoch 411900: Loss = 0.6423\nEpoch 412000: Loss = 0.6423\nEpoch 412100: Loss = 0.6423\nEpoch 412200: Loss = 0.6423\nEpoch 412300: Loss = 0.6423\nEpoch 412400: Loss = 0.6423\nEpoch 412500: Loss = 0.6423\nEpoch 412600: Loss = 0.6423\nEpoch 412700: Loss = 0.6423\nEpoch 412800: Loss = 0.6423\nEpoch 412900: Loss = 0.6423\nEpoch 413000: Loss = 0.6423\nEpoch 413100: Loss = 0.6422\nEpoch 413200: Loss = 0.6423\nEpoch 413300: Loss = 0.6422\nEpoch 413400: Loss = 0.6422\nEpoch 413500: Loss = 0.6423\nEpoch 413600: Loss = 0.6423\nEpoch 413700: Loss = 0.6423\nEpoch 413800: Loss = 0.6422\nEpoch 413900: Loss = 0.6423\nEpoch 414000: Loss = 0.6422\nEpoch 414100: Loss = 0.6422\nEpoch 414200: Loss = 0.6422\nEpoch 414300: Loss = 0.6422\nEpoch 414400: Loss = 0.6422\nEpoch 414500: Loss = 0.6422\nEpoch 414600: Loss = 0.6422\nEpoch 414700: Loss = 0.6422\nEpoch 414800: Loss = 0.6422\nEpoch 414900: Loss = 0.6422\nEpoch 415000: Loss = 0.6422\nEpoch 415100: Loss = 0.6422\nEpoch 415200: Loss = 0.6422\nEpoch 415300: Loss = 0.6422\nEpoch 415400: Loss = 0.6422\nEpoch 415500: Loss = 0.6422\nEpoch 415600: Loss = 0.6422\nEpoch 415700: Loss = 0.6422\nEpoch 415800: Loss = 0.6422\nEpoch 415900: Loss = 0.6422\nEpoch 416000: Loss = 0.6422\nEpoch 416100: Loss = 0.6422\nEpoch 416200: Loss = 0.6422\nEpoch 416300: Loss = 0.6422\nEpoch 416400: Loss = 0.6422\nEpoch 416500: Loss = 0.6422\nEpoch 416600: Loss = 0.6422\nEpoch 416700: Loss = 0.6422\nEpoch 416800: Loss = 0.6422\nEpoch 416900: Loss = 0.6422\nEpoch 417000: Loss = 0.6422\nEpoch 417100: Loss = 0.6422\nEpoch 417200: Loss = 0.6422\nEpoch 417300: Loss = 0.6422\nEpoch 417400: Loss = 0.6422\nEpoch 417500: Loss = 0.6422\nEpoch 417600: Loss = 0.6422\nEpoch 417700: Loss = 0.6422\nEpoch 417800: Loss = 0.6422\nEpoch 417900: Loss = 0.6422\nEpoch 418000: Loss = 0.6422\nEpoch 418100: Loss = 0.6422\nEpoch 418200: Loss = 0.6422\nEpoch 418300: Loss = 0.6422\nEpoch 418400: Loss = 0.6422\nEpoch 418500: Loss = 0.6422\nEpoch 418600: Loss = 0.6422\nEpoch 418700: Loss = 0.6422\nEpoch 418800: Loss = 0.6422\nEpoch 418900: Loss = 0.6422\nEpoch 419000: Loss = 0.6422\nEpoch 419100: Loss = 0.6422\nEpoch 419200: Loss = 0.6422\nEpoch 419300: Loss = 0.6422\nEpoch 419400: Loss = 0.6422\nEpoch 419500: Loss = 0.6422\nEpoch 419600: Loss = 0.6422\nEpoch 419700: Loss = 0.6422\nEpoch 419800: Loss = 0.6422\nEpoch 419900: Loss = 0.6422\nEpoch 420000: Loss = 0.6422\nEpoch 420100: Loss = 0.6422\nEpoch 420200: Loss = 0.6422\nEpoch 420300: Loss = 0.6422\nEpoch 420400: Loss = 0.6422\nEpoch 420500: Loss = 0.6422\nEpoch 420600: Loss = 0.6422\nEpoch 420700: Loss = 0.6422\nEpoch 420800: Loss = 0.6422\nEpoch 420900: Loss = 0.6422\nEpoch 421000: Loss = 0.6422\nEpoch 421100: Loss = 0.6422\nEpoch 421200: Loss = 0.6422\nEpoch 421300: Loss = 0.6422\nEpoch 421400: Loss = 0.6422\nEpoch 421500: Loss = 0.6422\nEpoch 421600: Loss = 0.6422\nEpoch 421700: Loss = 0.6422\nEpoch 421800: Loss = 0.6422\nEpoch 421900: Loss = 0.6422\nEpoch 422000: Loss = 0.6422\nEpoch 422100: Loss = 0.6422\nEpoch 422200: Loss = 0.6422\nEpoch 422300: Loss = 0.6422\nEpoch 422400: Loss = 0.6422\nEpoch 422500: Loss = 0.6422\nEpoch 422600: Loss = 0.6422\nEpoch 422700: Loss = 0.6422\nEpoch 422800: Loss = 0.6422\nEpoch 422900: Loss = 0.6422\nEpoch 423000: Loss = 0.6422\nEpoch 423100: Loss = 0.6422\nEpoch 423200: Loss = 0.6422\nEpoch 423300: Loss = 0.6422\nEpoch 423400: Loss = 0.6422\nEpoch 423500: Loss = 0.6422\nEpoch 423600: Loss = 0.6422\nEpoch 423700: Loss = 0.6422\nEpoch 423800: Loss = 0.6422\nEpoch 423900: Loss = 0.6422\nEpoch 424000: Loss = 0.6422\nEpoch 424100: Loss = 0.6422\nEpoch 424200: Loss = 0.6422\nEpoch 424300: Loss = 0.6422\nEpoch 424400: Loss = 0.6422\nEpoch 424500: Loss = 0.6422\nEpoch 424600: Loss = 0.6422\nEpoch 424700: Loss = 0.6422\nEpoch 424800: Loss = 0.6422\nEpoch 424900: Loss = 0.6422\nEpoch 425000: Loss = 0.6422\nEpoch 425100: Loss = 0.6422\nEpoch 425200: Loss = 0.6422\nEpoch 425300: Loss = 0.6422\nEpoch 425400: Loss = 0.6422\nEpoch 425500: Loss = 0.6422\nEpoch 425600: Loss = 0.6422\nEpoch 425700: Loss = 0.6422\nEpoch 425800: Loss = 0.6422\nEpoch 425900: Loss = 0.6422\nEpoch 426000: Loss = 0.6422\nEpoch 426100: Loss = 0.6422\nEpoch 426200: Loss = 0.6422\nEpoch 426300: Loss = 0.6422\nEpoch 426400: Loss = 0.6422\nEpoch 426500: Loss = 0.6422\nEpoch 426600: Loss = 0.6422\nEpoch 426700: Loss = 0.6422\nEpoch 426800: Loss = 0.6422\nEpoch 426900: Loss = 0.6422\nEpoch 427000: Loss = 0.6422\nEpoch 427100: Loss = 0.6422\nEpoch 427200: Loss = 0.6422\nEpoch 427300: Loss = 0.6422\nEpoch 427400: Loss = 0.6422\nEpoch 427500: Loss = 0.6422\nEpoch 427600: Loss = 0.6422\nEpoch 427700: Loss = 0.6422\nEpoch 427800: Loss = 0.6422\nEpoch 427900: Loss = 0.6422\nEpoch 428000: Loss = 0.6422\nEpoch 428100: Loss = 0.6422\nEpoch 428200: Loss = 0.6422\nEpoch 428300: Loss = 0.6422\nEpoch 428400: Loss = 0.6422\nEpoch 428500: Loss = 0.6422\nEpoch 428600: Loss = 0.6422\nEpoch 428700: Loss = 0.6422\nEpoch 428800: Loss = 0.6422\nEpoch 428900: Loss = 0.6422\nEpoch 429000: Loss = 0.6422\nEpoch 429100: Loss = 0.6422\nEpoch 429200: Loss = 0.6422\nEpoch 429300: Loss = 0.6422\nEpoch 429400: Loss = 0.6422\nEpoch 429500: Loss = 0.6422\nEpoch 429600: Loss = 0.6422\nEpoch 429700: Loss = 0.6422\nEpoch 429800: Loss = 0.6422\nEpoch 429900: Loss = 0.6422\nEpoch 430000: Loss = 0.6422\nEpoch 430100: Loss = 0.6422\nEpoch 430200: Loss = 0.6422\nEpoch 430300: Loss = 0.6422\nEpoch 430400: Loss = 0.6422\nEpoch 430500: Loss = 0.6422\nEpoch 430600: Loss = 0.6422\nEpoch 430700: Loss = 0.6422\nEpoch 430800: Loss = 0.6422\nEpoch 430900: Loss = 0.6422\nEpoch 431000: Loss = 0.6422\nEpoch 431100: Loss = 0.6422\nEpoch 431200: Loss = 0.6422\nEpoch 431300: Loss = 0.6422\nEpoch 431400: Loss = 0.6422\nEpoch 431500: Loss = 0.6422\nEpoch 431600: Loss = 0.6422\nEpoch 431700: Loss = 0.6422\nEpoch 431800: Loss = 0.6422\nEpoch 431900: Loss = 0.6422\nEpoch 432000: Loss = 0.6422\nEpoch 432100: Loss = 0.6422\nEpoch 432200: Loss = 0.6422\nEpoch 432300: Loss = 0.6422\nEpoch 432400: Loss = 0.6422\nEpoch 432500: Loss = 0.6422\nEpoch 432600: Loss = 0.6422\nEpoch 432700: Loss = 0.6422\nEpoch 432800: Loss = 0.6422\nEpoch 432900: Loss = 0.6422\nEpoch 433000: Loss = 0.6422\nEpoch 433100: Loss = 0.6422\nEpoch 433200: Loss = 0.6422\nEpoch 433300: Loss = 0.6422\nEpoch 433400: Loss = 0.6422\nEpoch 433500: Loss = 0.6422\nEpoch 433600: Loss = 0.6422\nEpoch 433700: Loss = 0.6422\nEpoch 433800: Loss = 0.6422\nEpoch 433900: Loss = 0.6422\nEpoch 434000: Loss = 0.6422\nEpoch 434100: Loss = 0.6422\nEpoch 434200: Loss = 0.6422\nEpoch 434300: Loss = 0.6422\nEpoch 434400: Loss = 0.6422\nEpoch 434500: Loss = 0.6422\nEpoch 434600: Loss = 0.6422\nEpoch 434700: Loss = 0.6422\nEpoch 434800: Loss = 0.6422\nEpoch 434900: Loss = 0.6422\nEpoch 435000: Loss = 0.6422\nEpoch 435100: Loss = 0.6422\nEpoch 435200: Loss = 0.6422\nEpoch 435300: Loss = 0.6422\nEpoch 435400: Loss = 0.6422\nEpoch 435500: Loss = 0.6422\nEpoch 435600: Loss = 0.6422\nEpoch 435700: Loss = 0.6422\nEpoch 435800: Loss = 0.6422\nEpoch 435900: Loss = 0.6422\nEpoch 436000: Loss = 0.6422\nEpoch 436100: Loss = 0.6422\nEpoch 436200: Loss = 0.6422\nEpoch 436300: Loss = 0.6422\nEpoch 436400: Loss = 0.6422\nEpoch 436500: Loss = 0.6422\nEpoch 436600: Loss = 0.6422\nEpoch 436700: Loss = 0.6422\nEpoch 436800: Loss = 0.6422\nEpoch 436900: Loss = 0.6422\nEpoch 437000: Loss = 0.6422\nEpoch 437100: Loss = 0.6422\nEpoch 437200: Loss = 0.6422\nEpoch 437300: Loss = 0.6422\nEpoch 437400: Loss = 0.6422\nEpoch 437500: Loss = 0.6422\nEpoch 437600: Loss = 0.6422\nEpoch 437700: Loss = 0.6422\nEpoch 437800: Loss = 0.6422\nEpoch 437900: Loss = 0.6422\nEpoch 438000: Loss = 0.6422\nEpoch 438100: Loss = 0.6422\nEpoch 438200: Loss = 0.6422\nEpoch 438300: Loss = 0.6422\nEpoch 438400: Loss = 0.6422\nEpoch 438500: Loss = 0.6422\nEpoch 438600: Loss = 0.6422\nEpoch 438700: Loss = 0.6422\nEpoch 438800: Loss = 0.6422\nEpoch 438900: Loss = 0.6422\nEpoch 439000: Loss = 0.6422\nEpoch 439100: Loss = 0.6422\nEpoch 439200: Loss = 0.6422\nEpoch 439300: Loss = 0.6422\nEpoch 439400: Loss = 0.6422\nEpoch 439500: Loss = 0.6422\nEpoch 439600: Loss = 0.6422\nEpoch 439700: Loss = 0.6422\nEpoch 439800: Loss = 0.6422\nEpoch 439900: Loss = 0.6422\nEpoch 440000: Loss = 0.6422\nEpoch 440100: Loss = 0.6422\nEpoch 440200: Loss = 0.6422\nEpoch 440300: Loss = 0.6422\nEpoch 440400: Loss = 0.6422\nEpoch 440500: Loss = 0.6422\nEpoch 440600: Loss = 0.6422\nEpoch 440700: Loss = 0.6422\nEpoch 440800: Loss = 0.6422\nEpoch 440900: Loss = 0.6422\nEpoch 441000: Loss = 0.6422\nEpoch 441100: Loss = 0.6422\nEpoch 441200: Loss = 0.6422\nEpoch 441300: Loss = 0.6422\nEpoch 441400: Loss = 0.6422\nEpoch 441500: Loss = 0.6422\nEpoch 441600: Loss = 0.6422\nEpoch 441700: Loss = 0.6422\nEpoch 441800: Loss = 0.6422\nEpoch 441900: Loss = 0.6422\nEpoch 442000: Loss = 0.6422\nEpoch 442100: Loss = 0.6422\nEpoch 442200: Loss = 0.6422\nEpoch 442300: Loss = 0.6422\nEpoch 442400: Loss = 0.6422\nEpoch 442500: Loss = 0.6422\nEpoch 442600: Loss = 0.6422\nEpoch 442700: Loss = 0.6422\nEpoch 442800: Loss = 0.6422\nEpoch 442900: Loss = 0.6422\nEpoch 443000: Loss = 0.6422\nEpoch 443100: Loss = 0.6422\nEpoch 443200: Loss = 0.6422\nEpoch 443300: Loss = 0.6422\nEpoch 443400: Loss = 0.6422\nEpoch 443500: Loss = 0.6422\nEpoch 443600: Loss = 0.6422\nEpoch 443700: Loss = 0.6422\nEpoch 443800: Loss = 0.6422\nEpoch 443900: Loss = 0.6422\nEpoch 444000: Loss = 0.6422\nEpoch 444100: Loss = 0.6422\nEpoch 444200: Loss = 0.6422\nEpoch 444300: Loss = 0.6422\nEpoch 444400: Loss = 0.6422\nEpoch 444500: Loss = 0.6422\nEpoch 444600: Loss = 0.6422\nEpoch 444700: Loss = 0.6422\nEpoch 444800: Loss = 0.6422\nEpoch 444900: Loss = 0.6422\nEpoch 445000: Loss = 0.6422\nEpoch 445100: Loss = 0.6422\nEpoch 445200: Loss = 0.6422\nEpoch 445300: Loss = 0.6422\nEpoch 445400: Loss = 0.6422\nEpoch 445500: Loss = 0.6422\nEpoch 445600: Loss = 0.6422\nEpoch 445700: Loss = 0.6422\nEpoch 445800: Loss = 0.6422\nEpoch 445900: Loss = 0.6422\nEpoch 446000: Loss = 0.6422\nEpoch 446100: Loss = 0.6422\nEpoch 446200: Loss = 0.6422\nEpoch 446300: Loss = 0.6422\nEpoch 446400: Loss = 0.6422\nEpoch 446500: Loss = 0.6422\nEpoch 446600: Loss = 0.6422\nEpoch 446700: Loss = 0.6422\nEpoch 446800: Loss = 0.6422\nEpoch 446900: Loss = 0.6422\nEpoch 447000: Loss = 0.6422\nEpoch 447100: Loss = 0.6422\nEpoch 447200: Loss = 0.6422\nEpoch 447300: Loss = 0.6422\nEpoch 447400: Loss = 0.6422\nEpoch 447500: Loss = 0.6422\nEpoch 447600: Loss = 0.6422\nEpoch 447700: Loss = 0.6422\nEpoch 447800: Loss = 0.6422\nEpoch 447900: Loss = 0.6422\nEpoch 448000: Loss = 0.6422\nEpoch 448100: Loss = 0.6422\nEpoch 448200: Loss = 0.6422\nEpoch 448300: Loss = 0.6422\nEpoch 448400: Loss = 0.6422\nEpoch 448500: Loss = 0.6422\nEpoch 448600: Loss = 0.6422\nEpoch 448700: Loss = 0.6422\nEpoch 448800: Loss = 0.6422\nEpoch 448900: Loss = 0.6422\nEpoch 449000: Loss = 0.6422\nEpoch 449100: Loss = 0.6422\nEpoch 449200: Loss = 0.6422\nEpoch 449300: Loss = 0.6422\nEpoch 449400: Loss = 0.6422\nEpoch 449500: Loss = 0.6422\nEpoch 449600: Loss = 0.6422\nEpoch 449700: Loss = 0.6422\nEpoch 449800: Loss = 0.6422\nEpoch 449900: Loss = 0.6422\nEpoch 450000: Loss = 0.6422\nEpoch 450100: Loss = 0.6422\nEpoch 450200: Loss = 0.6422\nEpoch 450300: Loss = 0.6422\nEpoch 450400: Loss = 0.6422\nEpoch 450500: Loss = 0.6422\nEpoch 450600: Loss = 0.6422\nEpoch 450700: Loss = 0.6422\nEpoch 450800: Loss = 0.6422\nEpoch 450900: Loss = 0.6421\nEpoch 451000: Loss = 0.6422\nEpoch 451100: Loss = 0.6422\nEpoch 451200: Loss = 0.6421\nEpoch 451300: Loss = 0.6422\nEpoch 451400: Loss = 0.6421\nEpoch 451500: Loss = 0.6421\nEpoch 451600: Loss = 0.6422\nEpoch 451700: Loss = 0.6422\nEpoch 451800: Loss = 0.6421\nEpoch 451900: Loss = 0.6422\nEpoch 452000: Loss = 0.6421\nEpoch 452100: Loss = 0.6422\nEpoch 452200: Loss = 0.6421\nEpoch 452300: Loss = 0.6421\nEpoch 452400: Loss = 0.6421\nEpoch 452500: Loss = 0.6421\nEpoch 452600: Loss = 0.6422\nEpoch 452700: Loss = 0.6421\nEpoch 452800: Loss = 0.6421\nEpoch 452900: Loss = 0.6421\nEpoch 453000: Loss = 0.6421\nEpoch 453100: Loss = 0.6421\nEpoch 453200: Loss = 0.6421\nEpoch 453300: Loss = 0.6421\nEpoch 453400: Loss = 0.6421\nEpoch 453500: Loss = 0.6421\nEpoch 453600: Loss = 0.6421\nEpoch 453700: Loss = 0.6421\nEpoch 453800: Loss = 0.6421\nEpoch 453900: Loss = 0.6421\nEpoch 454000: Loss = 0.6421\nEpoch 454100: Loss = 0.6421\nEpoch 454200: Loss = 0.6421\nEpoch 454300: Loss = 0.6421\nEpoch 454400: Loss = 0.6421\nEpoch 454500: Loss = 0.6421\nEpoch 454600: Loss = 0.6421\nEpoch 454700: Loss = 0.6421\nEpoch 454800: Loss = 0.6421\nEpoch 454900: Loss = 0.6421\nEpoch 455000: Loss = 0.6421\nEpoch 455100: Loss = 0.6421\nEpoch 455200: Loss = 0.6421\nEpoch 455300: Loss = 0.6421\nEpoch 455400: Loss = 0.6421\nEpoch 455500: Loss = 0.6421\nEpoch 455600: Loss = 0.6421\nEpoch 455700: Loss = 0.6421\nEpoch 455800: Loss = 0.6421\nEpoch 455900: Loss = 0.6421\nEpoch 456000: Loss = 0.6421\nEpoch 456100: Loss = 0.6421\nEpoch 456200: Loss = 0.6421\nEpoch 456300: Loss = 0.6421\nEpoch 456400: Loss = 0.6421\nEpoch 456500: Loss = 0.6421\nEpoch 456600: Loss = 0.6421\nEpoch 456700: Loss = 0.6421\nEpoch 456800: Loss = 0.6421\nEpoch 456900: Loss = 0.6421\nEpoch 457000: Loss = 0.6421\nEpoch 457100: Loss = 0.6421\nEpoch 457200: Loss = 0.6421\nEpoch 457300: Loss = 0.6421\nEpoch 457400: Loss = 0.6421\nEpoch 457500: Loss = 0.6421\nEpoch 457600: Loss = 0.6421\nEpoch 457700: Loss = 0.6421\nEpoch 457800: Loss = 0.6421\nEpoch 457900: Loss = 0.6421\nEpoch 458000: Loss = 0.6421\nEpoch 458100: Loss = 0.6421\nEpoch 458200: Loss = 0.6421\nEpoch 458300: Loss = 0.6421\nEpoch 458400: Loss = 0.6421\nEpoch 458500: Loss = 0.6421\nEpoch 458600: Loss = 0.6421\nEpoch 458700: Loss = 0.6421\nEpoch 458800: Loss = 0.6421\nEpoch 458900: Loss = 0.6421\nEpoch 459000: Loss = 0.6421\nEpoch 459100: Loss = 0.6421\nEpoch 459200: Loss = 0.6421\nEpoch 459300: Loss = 0.6421\nEpoch 459400: Loss = 0.6421\nEpoch 459500: Loss = 0.6421\nEpoch 459600: Loss = 0.6421\nEpoch 459700: Loss = 0.6421\nEpoch 459800: Loss = 0.6421\nEpoch 459900: Loss = 0.6421\nEpoch 460000: Loss = 0.6421\nEpoch 460100: Loss = 0.6421\nEpoch 460200: Loss = 0.6421\nEpoch 460300: Loss = 0.6421\nEpoch 460400: Loss = 0.6421\nEpoch 460500: Loss = 0.6421\nEpoch 460600: Loss = 0.6421\nEpoch 460700: Loss = 0.6421\nEpoch 460800: Loss = 0.6421\nEpoch 460900: Loss = 0.6421\nEpoch 461000: Loss = 0.6421\nEpoch 461100: Loss = 0.6421\nEpoch 461200: Loss = 0.6421\nEpoch 461300: Loss = 0.6421\nEpoch 461400: Loss = 0.6421\nEpoch 461500: Loss = 0.6421\nEpoch 461600: Loss = 0.6421\nEpoch 461700: Loss = 0.6421\nEpoch 461800: Loss = 0.6421\nEpoch 461900: Loss = 0.6421\nEpoch 462000: Loss = 0.6421\nEpoch 462100: Loss = 0.6421\nEpoch 462200: Loss = 0.6421\nEpoch 462300: Loss = 0.6421\nEpoch 462400: Loss = 0.6421\nEpoch 462500: Loss = 0.6421\nEpoch 462600: Loss = 0.6421\nEpoch 462700: Loss = 0.6421\nEpoch 462800: Loss = 0.6421\nEpoch 462900: Loss = 0.6421\nEpoch 463000: Loss = 0.6421\nEpoch 463100: Loss = 0.6421\nEpoch 463200: Loss = 0.6421\nEpoch 463300: Loss = 0.6421\nEpoch 463400: Loss = 0.6421\nEpoch 463500: Loss = 0.6421\nEpoch 463600: Loss = 0.6421\nEpoch 463700: Loss = 0.6421\nEpoch 463800: Loss = 0.6421\nEpoch 463900: Loss = 0.6421\nEpoch 464000: Loss = 0.6421\nEpoch 464100: Loss = 0.6421\nEpoch 464200: Loss = 0.6421\nEpoch 464300: Loss = 0.6421\nEpoch 464400: Loss = 0.6421\nEpoch 464500: Loss = 0.6421\nEpoch 464600: Loss = 0.6421\nEpoch 464700: Loss = 0.6421\nEpoch 464800: Loss = 0.6421\nEpoch 464900: Loss = 0.6421\nEpoch 465000: Loss = 0.6421\nEpoch 465100: Loss = 0.6421\nEpoch 465200: Loss = 0.6421\nEpoch 465300: Loss = 0.6421\nEpoch 465400: Loss = 0.6421\nEpoch 465500: Loss = 0.6421\nEpoch 465600: Loss = 0.6421\nEpoch 465700: Loss = 0.6421\nEpoch 465800: Loss = 0.6421\nEpoch 465900: Loss = 0.6421\nEpoch 466000: Loss = 0.6421\nEpoch 466100: Loss = 0.6421\nEpoch 466200: Loss = 0.6421\nEpoch 466300: Loss = 0.6421\nEpoch 466400: Loss = 0.6421\nEpoch 466500: Loss = 0.6421\nEpoch 466600: Loss = 0.6421\nEpoch 466700: Loss = 0.6421\nEpoch 466800: Loss = 0.6421\nEpoch 466900: Loss = 0.6421\nEpoch 467000: Loss = 0.6421\nEpoch 467100: Loss = 0.6421\nEpoch 467200: Loss = 0.6421\nEpoch 467300: Loss = 0.6421\nEpoch 467400: Loss = 0.6421\nEpoch 467500: Loss = 0.6421\nEpoch 467600: Loss = 0.6421\nEpoch 467700: Loss = 0.6421\nEpoch 467800: Loss = 0.6421\nEpoch 467900: Loss = 0.6421\nEpoch 468000: Loss = 0.6421\nEpoch 468100: Loss = 0.6421\nEpoch 468200: Loss = 0.6421\nEpoch 468300: Loss = 0.6421\nEpoch 468400: Loss = 0.6421\nEpoch 468500: Loss = 0.6421\nEpoch 468600: Loss = 0.6421\nEpoch 468700: Loss = 0.6421\nEpoch 468800: Loss = 0.6421\nEpoch 468900: Loss = 0.6421\nEpoch 469000: Loss = 0.6421\nEpoch 469100: Loss = 0.6421\nEpoch 469200: Loss = 0.6421\nEpoch 469300: Loss = 0.6421\nEpoch 469400: Loss = 0.6421\nEpoch 469500: Loss = 0.6421\nEpoch 469600: Loss = 0.6421\nEpoch 469700: Loss = 0.6421\nEpoch 469800: Loss = 0.6421\nEpoch 469900: Loss = 0.6421\nEpoch 470000: Loss = 0.6421\nEpoch 470100: Loss = 0.6421\nEpoch 470200: Loss = 0.6421\nEpoch 470300: Loss = 0.6421\nEpoch 470400: Loss = 0.6421\nEpoch 470500: Loss = 0.6421\nEpoch 470600: Loss = 0.6421\nEpoch 470700: Loss = 0.6421\nEpoch 470800: Loss = 0.6421\nEpoch 470900: Loss = 0.6421\nEpoch 471000: Loss = 0.6421\nEpoch 471100: Loss = 0.6421\nEpoch 471200: Loss = 0.6421\nEpoch 471300: Loss = 0.6421\nEpoch 471400: Loss = 0.6421\nEpoch 471500: Loss = 0.6421\nEpoch 471600: Loss = 0.6421\nEpoch 471700: Loss = 0.6421\nEpoch 471800: Loss = 0.6421\nEpoch 471900: Loss = 0.6421\nEpoch 472000: Loss = 0.6421\nEpoch 472100: Loss = 0.6421\nEpoch 472200: Loss = 0.6421\nEpoch 472300: Loss = 0.6421\nEpoch 472400: Loss = 0.6421\nEpoch 472500: Loss = 0.6421\nEpoch 472600: Loss = 0.6421\nEpoch 472700: Loss = 0.6421\nEpoch 472800: Loss = 0.6421\nEpoch 472900: Loss = 0.6421\nEpoch 473000: Loss = 0.6421\nEpoch 473100: Loss = 0.6421\nEpoch 473200: Loss = 0.6421\nEpoch 473300: Loss = 0.6421\nEpoch 473400: Loss = 0.6421\nEpoch 473500: Loss = 0.6421\nEpoch 473600: Loss = 0.6421\nEpoch 473700: Loss = 0.6421\nEpoch 473800: Loss = 0.6421\nEpoch 473900: Loss = 0.6421\nEpoch 474000: Loss = 0.6421\nEpoch 474100: Loss = 0.6421\nEpoch 474200: Loss = 0.6421\nEpoch 474300: Loss = 0.6421\nEpoch 474400: Loss = 0.6421\nEpoch 474500: Loss = 0.6421\nEpoch 474600: Loss = 0.6421\nEpoch 474700: Loss = 0.6421\nEpoch 474800: Loss = 0.6421\nEpoch 474900: Loss = 0.6421\nEpoch 475000: Loss = 0.6421\nEpoch 475100: Loss = 0.6421\nEpoch 475200: Loss = 0.6421\nEpoch 475300: Loss = 0.6421\nEpoch 475400: Loss = 0.6421\nEpoch 475500: Loss = 0.6421\nEpoch 475600: Loss = 0.6421\nEpoch 475700: Loss = 0.6421\nEpoch 475800: Loss = 0.6421\nEpoch 475900: Loss = 0.6421\nEpoch 476000: Loss = 0.6421\nEpoch 476100: Loss = 0.6421\nEpoch 476200: Loss = 0.6421\nEpoch 476300: Loss = 0.6421\nEpoch 476400: Loss = 0.6421\nEpoch 476500: Loss = 0.6421\nEpoch 476600: Loss = 0.6421\nEpoch 476700: Loss = 0.6421\nEpoch 476800: Loss = 0.6421\nEpoch 476900: Loss = 0.6421\nEpoch 477000: Loss = 0.6421\nEpoch 477100: Loss = 0.6421\nEpoch 477200: Loss = 0.6421\nEpoch 477300: Loss = 0.6421\nEpoch 477400: Loss = 0.6421\nEpoch 477500: Loss = 0.6421\nEpoch 477600: Loss = 0.6421\nEpoch 477700: Loss = 0.6421\nEpoch 477800: Loss = 0.6421\nEpoch 477900: Loss = 0.6421\nEpoch 478000: Loss = 0.6421\nEpoch 478100: Loss = 0.6421\nEpoch 478200: Loss = 0.6421\nEpoch 478300: Loss = 0.6421\nEpoch 478400: Loss = 0.6421\nEpoch 478500: Loss = 0.6421\nEpoch 478600: Loss = 0.6421\nEpoch 478700: Loss = 0.6421\nEpoch 478800: Loss = 0.6421\nEpoch 478900: Loss = 0.6421\nEpoch 479000: Loss = 0.6421\nEpoch 479100: Loss = 0.6421\nEpoch 479200: Loss = 0.6421\nEpoch 479300: Loss = 0.6421\nEpoch 479400: Loss = 0.6421\nEpoch 479500: Loss = 0.6421\nEpoch 479600: Loss = 0.6421\nEpoch 479700: Loss = 0.6421\nEpoch 479800: Loss = 0.6421\nEpoch 479900: Loss = 0.6421\nEpoch 480000: Loss = 0.6421\nEpoch 480100: Loss = 0.6421\nEpoch 480200: Loss = 0.6421\nEpoch 480300: Loss = 0.6421\nEpoch 480400: Loss = 0.6421\nEpoch 480500: Loss = 0.6421\nEpoch 480600: Loss = 0.6421\nEpoch 480700: Loss = 0.6421\nEpoch 480800: Loss = 0.6421\nEpoch 480900: Loss = 0.6421\nEpoch 481000: Loss = 0.6421\nEpoch 481100: Loss = 0.6421\nEpoch 481200: Loss = 0.6421\nEpoch 481300: Loss = 0.6421\nEpoch 481400: Loss = 0.6421\nEpoch 481500: Loss = 0.6421\nEpoch 481600: Loss = 0.6421\nEpoch 481700: Loss = 0.6421\nEpoch 481800: Loss = 0.6421\nEpoch 481900: Loss = 0.6421\nEpoch 482000: Loss = 0.6421\nEpoch 482100: Loss = 0.6421\nEpoch 482200: Loss = 0.6421\nEpoch 482300: Loss = 0.6421\nEpoch 482400: Loss = 0.6421\nEpoch 482500: Loss = 0.6421\nEpoch 482600: Loss = 0.6421\nEpoch 482700: Loss = 0.6421\nEpoch 482800: Loss = 0.6421\nEpoch 482900: Loss = 0.6421\nEpoch 483000: Loss = 0.6421\nEpoch 483100: Loss = 0.6421\nEpoch 483200: Loss = 0.6421\nEpoch 483300: Loss = 0.6421\nEpoch 483400: Loss = 0.6421\nEpoch 483500: Loss = 0.6421\nEpoch 483600: Loss = 0.6421\nEpoch 483700: Loss = 0.6421\nEpoch 483800: Loss = 0.6421\nEpoch 483900: Loss = 0.6421\nEpoch 484000: Loss = 0.6421\nEpoch 484100: Loss = 0.6421\nEpoch 484200: Loss = 0.6421\nEpoch 484300: Loss = 0.6421\nEpoch 484400: Loss = 0.6421\nEpoch 484500: Loss = 0.6421\nEpoch 484600: Loss = 0.6421\nEpoch 484700: Loss = 0.6421\nEpoch 484800: Loss = 0.6421\nEpoch 484900: Loss = 0.6421\nEpoch 485000: Loss = 0.6421\nEpoch 485100: Loss = 0.6421\nEpoch 485200: Loss = 0.6421\nEpoch 485300: Loss = 0.6421\nEpoch 485400: Loss = 0.6421\nEpoch 485500: Loss = 0.6421\nEpoch 485600: Loss = 0.6421\nEpoch 485700: Loss = 0.6421\nEpoch 485800: Loss = 0.6421\nEpoch 485900: Loss = 0.6421\nEpoch 486000: Loss = 0.6421\nEpoch 486100: Loss = 0.6421\nEpoch 486200: Loss = 0.6421\nEpoch 486300: Loss = 0.6421\nEpoch 486400: Loss = 0.6421\nEpoch 486500: Loss = 0.6421\nEpoch 486600: Loss = 0.6421\nEpoch 486700: Loss = 0.6421\nEpoch 486800: Loss = 0.6421\nEpoch 486900: Loss = 0.6421\nEpoch 487000: Loss = 0.6421\nEpoch 487100: Loss = 0.6421\nEpoch 487200: Loss = 0.6421\nEpoch 487300: Loss = 0.6421\nEpoch 487400: Loss = 0.6421\nEpoch 487500: Loss = 0.6421\nEpoch 487600: Loss = 0.6421\nEpoch 487700: Loss = 0.6421\nEpoch 487800: Loss = 0.6421\nEpoch 487900: Loss = 0.6421\nEpoch 488000: Loss = 0.6421\nEpoch 488100: Loss = 0.6421\nEpoch 488200: Loss = 0.6421\nEpoch 488300: Loss = 0.6421\nEpoch 488400: Loss = 0.6421\nEpoch 488500: Loss = 0.6421\nEpoch 488600: Loss = 0.6421\nEpoch 488700: Loss = 0.6421\nEpoch 488800: Loss = 0.6421\nEpoch 488900: Loss = 0.6421\nEpoch 489000: Loss = 0.6421\nEpoch 489100: Loss = 0.6421\nEpoch 489200: Loss = 0.6421\nEpoch 489300: Loss = 0.6421\nEpoch 489400: Loss = 0.6421\nEpoch 489500: Loss = 0.6421\nEpoch 489600: Loss = 0.6421\nEpoch 489700: Loss = 0.6421\nEpoch 489800: Loss = 0.6421\nEpoch 489900: Loss = 0.6421\nEpoch 490000: Loss = 0.6421\nEpoch 490100: Loss = 0.6421\nEpoch 490200: Loss = 0.6421\nEpoch 490300: Loss = 0.6421\nEpoch 490400: Loss = 0.6421\nEpoch 490500: Loss = 0.6421\nEpoch 490600: Loss = 0.6421\nEpoch 490700: Loss = 0.6421\nEpoch 490800: Loss = 0.6421\nEpoch 490900: Loss = 0.6421\nEpoch 491000: Loss = 0.6421\nEpoch 491100: Loss = 0.6421\nEpoch 491200: Loss = 0.6421\nEpoch 491300: Loss = 0.6421\nEpoch 491400: Loss = 0.6421\nEpoch 491500: Loss = 0.6421\nEpoch 491600: Loss = 0.6421\nEpoch 491700: Loss = 0.6421\nEpoch 491800: Loss = 0.6421\nEpoch 491900: Loss = 0.6421\nEpoch 492000: Loss = 0.6421\nEpoch 492100: Loss = 0.6421\nEpoch 492200: Loss = 0.6421\nEpoch 492300: Loss = 0.6421\nEpoch 492400: Loss = 0.6421\nEpoch 492500: Loss = 0.6421\nEpoch 492600: Loss = 0.6421\nEpoch 492700: Loss = 0.6421\nEpoch 492800: Loss = 0.6421\nEpoch 492900: Loss = 0.6421\nEpoch 493000: Loss = 0.6421\nEpoch 493100: Loss = 0.6421\nEpoch 493200: Loss = 0.6421\nEpoch 493300: Loss = 0.6421\nEpoch 493400: Loss = 0.6421\nEpoch 493500: Loss = 0.6421\nEpoch 493600: Loss = 0.6421\nEpoch 493700: Loss = 0.6421\nEpoch 493800: Loss = 0.6421\nEpoch 493900: Loss = 0.6421\nEpoch 494000: Loss = 0.6421\nEpoch 494100: Loss = 0.6421\nEpoch 494200: Loss = 0.6421\nEpoch 494300: Loss = 0.6421\nEpoch 494400: Loss = 0.6421\nEpoch 494500: Loss = 0.6421\nEpoch 494600: Loss = 0.6421\nEpoch 494700: Loss = 0.6421\nEpoch 494800: Loss = 0.6421\nEpoch 494900: Loss = 0.6421\nEpoch 495000: Loss = 0.6421\nEpoch 495100: Loss = 0.6421\nEpoch 495200: Loss = 0.6421\nEpoch 495300: Loss = 0.6421\nEpoch 495400: Loss = 0.6421\nEpoch 495500: Loss = 0.6421\nEpoch 495600: Loss = 0.6421\nEpoch 495700: Loss = 0.6421\nEpoch 495800: Loss = 0.6421\nEpoch 495900: Loss = 0.6421\nEpoch 496000: Loss = 0.6421\nEpoch 496100: Loss = 0.6421\nEpoch 496200: Loss = 0.6421\nEpoch 496300: Loss = 0.6421\nEpoch 496400: Loss = 0.6421\nEpoch 496500: Loss = 0.6421\nEpoch 496600: Loss = 0.6421\nEpoch 496700: Loss = 0.6421\nEpoch 496800: Loss = 0.6421\nEpoch 496900: Loss = 0.6421\nEpoch 497000: Loss = 0.6421\nEpoch 497100: Loss = 0.6421\nEpoch 497200: Loss = 0.6421\nEpoch 497300: Loss = 0.6421\nEpoch 497400: Loss = 0.6421\nEpoch 497500: Loss = 0.6421\nEpoch 497600: Loss = 0.6421\nEpoch 497700: Loss = 0.6421\nEpoch 497800: Loss = 0.6421\nEpoch 497900: Loss = 0.6421\nEpoch 498000: Loss = 0.6421\nEpoch 498100: Loss = 0.6421\nEpoch 498200: Loss = 0.6421\nEpoch 498300: Loss = 0.6421\nEpoch 498400: Loss = 0.6421\nEpoch 498500: Loss = 0.6421\nEpoch 498600: Loss = 0.6421\nEpoch 498700: Loss = 0.6421\nEpoch 498800: Loss = 0.6421\nEpoch 498900: Loss = 0.6421\nEpoch 499000: Loss = 0.6421\nEpoch 499100: Loss = 0.6421\nEpoch 499200: Loss = 0.6421\nEpoch 499300: Loss = 0.6421\nEpoch 499400: Loss = 0.6421\nEpoch 499500: Loss = 0.6421\nEpoch 499600: Loss = 0.6421\nEpoch 499700: Loss = 0.6421\nEpoch 499800: Loss = 0.6421\nEpoch 499900: Loss = 0.6421\n\n\nInspecting ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍weight ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍vector KR.a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍shows ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍most ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍entries ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍are ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍very ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍close ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍zero.\n\n(1.0*(KR.a &gt; 0.001)).mean()\n\ntensor(0.0800)\n\n\nThe ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍code ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍block ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍below ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍plots ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍scores ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍along ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍with ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍training ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍will ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍highlight ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍pieces ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍training ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍which ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍have ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍weights ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍distinguishable ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍from ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍0.\n\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n# ax.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\ndef plot_decision_boundary(model, X, y, title=\"\", ax=None):\n    h = 0.1\n    x_min, x_max = X[:, 0].min().item() - 1, X[:, 0].max().item() + 1\n    y_min, y_max = X[:, 1].min().item() - 1, X[:, 1].max().item() + 1\n    xx, yy = torch.meshgrid(torch.arange(x_min, x_max, h),\n                            torch.arange(y_min, y_max, h), indexing=\"xy\")\n    grid = torch.cat((xx.reshape(-1, 1), yy.reshape(-1, 1)), dim=1)\n    Z = model.predict(grid).reshape(xx.shape)\n\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.contourf(xx.numpy(), yy.numpy(), Z.numpy(), alpha=0.4, cmap=\"RdBu\")\n    plot_classification_data(X, y, ax)\n    \n    # Highlight points with non-zero weights (support vectors)\n    nonzero = model.a.abs() &gt; 1e-3\n    ax.scatter(X[nonzero, 0], X[nonzero, 1], s=80, edgecolors='black',\n               facecolors='none', linewidths=1.5, label='Support Vectors')\n\n    ax.set_title(title)\n    ax.legend()\n\n# Recreate training data\nX, y = classification_data(n_points=100, noise=0.4)\n\n# Moderate λ\nKR_mod = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=1)\nKR_mod.fit(X, y, epochs=1000, lr=0.01)\n\n# Large λ\nKR_large = KernelLogisticRegression(rbf_kernel, lam=100.0, gamma=1)\nKR_large.fit(X, y, epochs=1000, lr=0.01)\n\n# Plot decision boundaries\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nplot_decision_boundary(KR_mod, X, y, title=\"Moderate λ = 0.1\", ax=axes[0])\nplot_decision_boundary(KR_large, X, y, title=\"Large λ = 100.0\", ax=axes[1])\nplt.tight_layout()\nplt.show()\n\n# Plot distribution of weights a\nfig, ax = plt.subplots(1, 2, figsize=(12, 3))\nax[0].stem(KR_mod.a.detach().numpy(), use_line_collection=True)\nax[0].set_title(\"Weights (Moderate λ)\")\nax[1].stem(KR_large.a.detach().numpy(), use_line_collection=True)\nax[1].set_title(\"Weights (Large λ)\")\nplt.tight_layout()\nplt.show()\n\nEpoch 0: Loss = 0.6892\nEpoch 100: Loss = 0.6423\nEpoch 200: Loss = 0.6359\nEpoch 300: Loss = 0.6335\nEpoch 400: Loss = 0.6322\nEpoch 500: Loss = 0.6307\nEpoch 600: Loss = 0.6300\nEpoch 700: Loss = 0.6291\nEpoch 800: Loss = 0.6290\nEpoch 900: Loss = 0.6284\nEpoch 0: Loss = 12.8850\nEpoch 100: Loss = 1425.3674\nEpoch 200: Loss = 2660.0278\nEpoch 300: Loss = 3883.7214\nEpoch 400: Loss = 5105.1763\nEpoch 500: Loss = 6324.9033\nEpoch 600: Loss = 7492.2520\nEpoch 700: Loss = 7732.1611\nEpoch 800: Loss = 7286.4023\nEpoch 900: Loss = 6585.9546\n\n\n\n\n\n\n\n\n\n/var/folders/jj/kkhsq80d2pv7cj0ph5cb4rs40000gn/T/ipykernel_40986/433974894.py:45: MatplotlibDeprecationWarning: The 'use_line_collection' parameter of stem() was deprecated in Matplotlib 3.6 and will be removed two minor releases later. If any parameter follows 'use_line_collection', they should be passed as keyword, not positionally.\n  ax[0].stem(KR_mod.a.detach().numpy(), use_line_collection=True)\n/var/folders/jj/kkhsq80d2pv7cj0ph5cb4rs40000gn/T/ipykernel_40986/433974894.py:47: MatplotlibDeprecationWarning: The 'use_line_collection' parameter of stem() was deprecated in Matplotlib 3.6 and will be removed two minor releases later. If any parameter follows 'use_line_collection', they should be passed as keyword, not positionally.\n  ax[1].stem(KR_large.a.detach().numpy(), use_line_collection=True)"
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Abstract\nIn this project, I implement the perceptron algorithm from scratch using PyTorch to explore how linear classifiers work in both two-dimensional and higher-dimensional spaces. The perceptron is one of the earliest and most foundational machine learning algorithms, and this blog post walks through its logic, gradient updates, and training behavior. I apply the perceptron to synthetic datasets to examine its performance on linearly separable versus non-linearly separable data, and I visualize how the decision boundary and loss evolve over time. The goal is to gain an intuitive and practical understanding of how the perceptron learns from data and where its limitations lie.\n\n\nMy Perceptron Implementation\nThis project uses the perceptron.py script. The LinearModel class defines the basic structure, including methods to initialize the weight vector w, compute linear scores for input data (score), and make binary predictions (predict). The Perceptron class inherits from LinearModel and adds a loss method that calculates the misclassification rate. Finally, the PerceptronOptimizer class defines a step method that updates the model’s weights by applying the perceptron rule to a single data point.\nThe grad() function computes the update for the perceptron algorithm. The perceptron algorithm updates the weights only when a data point is misclassified. A point is misclassified when: \\[\\langle \\textbf{w},\\textbf{x}_i \\rangle \\cdot y_i &lt; 0\\] In the grad function, I convert the labels of \\(\\{0,1\\}\\) to \\(\\{-1,1\\}\\) by using \\[y' = 2y-1\\] Then, it computes the score \\(s = \\langle \\textbf{w},\\textbf{x}_i \\rangle\\), and if \\(s \\cdot y' &lt; 0\\), the point is misclassified. In that case, we return the update \\[-y' \\cdot \\textbf{x}_i\\] This pushes the weights in a direction that would correctly classify the point. If the point is already classified correctly, no update is needed, and the function returns a zero vector.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer, LinearModel\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nThe code above re-reads the contents of the perceptron.py file, so that changes made in that file will be reflected in this notebook.\n\n\nCreating and Visualizing Sample Data Points\nThe code below generates and visualizes synthetic 2D data for training a perceptron model. It uses PyTorch to create n_points (default 300) split into two classes, with half labeled as 0 and the other half as 1. Each data point is generated by adding Gaussian noise to the class label, creating two slightly overlapping clusters in a 2D space. An additional bias term (column of ones) is added to the feature matrix X. The plot_perceptron_data function then visualizes the data using Matplotlib, plotting class 0 and class 1 points with different markers and slight color variations.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\n\nTesting My Perceptron Algorithm\nNow that we have implemented our perceptron algorithm and have some simple data to work with, it’s time to check our implementation by running this “minimal training loop” below. This code initializes a perceptron and its optimizer, then enters a loop where it continually calculates the loss over the entire dataset and records this loss. In each iteration, a random data point is selected and used to update the perceptron’s weights. The loop continues until the loss reaches zero, implying that the data is linearly separable and the model has perfectly classified the training examples.\n\nfrom perceptron import Perceptron, PerceptronOptimizer\nimport torch\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nWe can track the progress of our training by checking the values of the loss function over time. The code below visualizes the evolution of the loss during perceptron training. It first plots a line graph of the loss values stored in loss_vec using a slategrey color. Then, it overlays a scatter plot to mark each individual loss measurement at each iteration (with the x-values generated by torch.arange(len(loss_vec))). Finally, it sets the x-axis label to “Perceptron Iteration (Updates Only)” and the y-axis label to “loss,” clearly labeling the plot to show how the loss changes with each update step.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAs we can see, over the course of the iterations, the loss value (y-axis) goes to 0, which indicates perfect training accuracy. The perceptron experiences some initial fluctuations in the loss (with a few notable spikes), but eventually drives the loss to zero and keeps it there for the remainder of the updates. This indicates that the dataset is linearly separable, and the perceptron successfully found a separation that classifies all training examples correctly. The spikes represent moments when a particular data point was misclassified and the weights were adjusted, but after enough updates, the model converges to perfect classification.\n\n\nExperiments\n\nEvolution of Loss Function During Training: Linearly Separable Data\nUsing 2D data that is linearly separable, the perceptron algorithm converges to weight vector \\(\\vec{w}\\), describing a separating line.\nThe code below repeatedly performs perceptron updates on randomly selected misclassified points from the dataset until the model achieves zero classification error. For each update, it plots the old decision boundary (before the weight update) as a dashed line and the new decision boundary (after the update) as a solid line on a subplot. The misclassified point used for the update is also highlighted. These plots are arranged in a 2x3 grid, showing the evolution of the model’s decision boundary over successive updates. This gives a clear visual demonstration of how the perceptron learns to separate the data through linear updates. Citation: this code was provided from these lecture notes.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nNon-Linearly Separable Data\nNow for 2D data that is not linearly separable, the perceptron algorithm will not settle on a final value of \\(\\vec{w}\\), but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy.\nThe code below trains a perceptron on 2D data that is not linearly separable by adding significant noise to the dataset. It runs the training loop for a fixed number of iterations (1000), since the perceptron can’t perfectly separate the classes in this case. At each step, it selects a random data point and updates the model weights if the point is misclassified. After training, it generates two plots: one showing the data and the final decision boundary, and another showing how the perceptron’s misclassification rate (loss) evolved over time.\n\nimport torch\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# Helper: draw decision boundary line from weight vector\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    ax.plot(x, y, **kwargs)\n\n# Generate NON-linearly separable data\nX, y = perceptron_data(n_points=300, noise=0.8)  # Increase noise for overlap\nn = X.size(0)\n\n# Initialize model + optimizer\ntorch.manual_seed(42)\np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# Training loop with max iterations\nmax_iters = 1000\nloss_vec = []\n\nfor iteration in range(max_iters):\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\n    # pick random point\n    i = torch.randint(n, size=(1,))\n    x_i = X[i[0]]  # shape: (p,)\n    y_i = y[i[0]]  # scalar\n\n    opt.step(x_i, y_i)\n\n# --- Visualization ---\n\n# 1. Plot the data with the final decision boundary\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: Data + final line\nplot_perceptron_data(X, y, ax[0])\ndraw_line(p.w, x_min=-1, x_max=2, ax=ax[0], color='black')\nax[0].set_title(\"Data with Final Decision Boundary\")\n\n# Right: Loss over time\nax[1].plot(loss_vec, color='darkred')\nax[1].set_title(\"Loss over Training\")\nax[1].set_xlabel(\"Iteration\")\nax[1].set_ylabel(\"Misclassification Rate\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMore Than Two Features\nThe perceptron algorithm is also able to work in more than 2 dimensions.\n\nimport torch\nimport matplotlib.pyplot as plt\n\n# Generate higher-dimensional data (5D + bias)\ndef perceptron_data_highdim(n_points=300, noise=0.2, p_dims=5):\n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), dim=1)  # Add bias column\n    return X, y\n\n# Generate data\nX, y = perceptron_data_highdim(p_dims=5)\nn = X.size(0)\n\n# Initialize model and optimizer\ntorch.manual_seed(42)\np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# Train the model\nmax_iters = 1000\nloss_vec = []\n\nfor _ in range(max_iters):\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    \n    i = torch.randint(n, size=(1,))\n    x_i = X[i[0]]\n    y_i = y[i[0]]\n    \n    opt.step(x_i, y_i)\n\n# Plot loss over time\nplt.figure(figsize=(6, 4))\nplt.plot(loss_vec, color='navy')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Misclassification Rate\")\nplt.title(\"Loss During Training (5D Data)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see from the graph above, the loss consistently drops and eventually reaches zero and stays there. This suggests that the data is linearly separable in the 5D space, because we can eventually reach a loss of 0 which is perfect accuracy.\n\n\nRuntimes\n\nQ: What is the runtime complexity of a single iteration of the perceptron algorithm? Does the runtime complexity of a single iteration depend on the number of data points \\(n\\)? What about the number of features \\(p\\)? If you implemented minibatch perceptron, what is the runtime complexity of a single iteration of the minibatch perceptron algorithm?\n\nA single iteration of of the perceptron algorithm involves the following steps:\n\nSelect one data point \\((x_i,y_i)\\), which takes constant time \\(O(1)\\).\nCompute the score \\(s = \\langle \\textbf{w},\\textbf{x}_i \\rangle\\). This is a dot product of two length-\\(p\\) vectors, so the run-time is \\(O(p)\\).\nCheck if misclassified and, if so, compute and apply the update. Computing the update is \\(-y_i \\cdot x_i\\), which again has runtime \\(O(p)\\). Updating the weights vector is vector addition and therefore has runtime \\(O(p)\\).\n\nThus, the total complexity per iteration is \\(O(p)\\).\nThis runtime does not depend on \\(n\\) because only one data point is processed. However, both the dot product and weight vector update involve vectors of length \\(p\\), so the runtime does depend on \\(p\\).\nIn minibatch perceptron, a batch of \\(b\\) data points is processed together. So, for every \\(b\\), the program do the same process outlined above that had a runtime of \\(O(p)\\).\nSo the total runtime per iteration of minibatch perceptron is \\(O(b \\cdot p)\\)\n\n\nConclusion\nThrough hands-on experimentation with the perceptron algorithm, I observed how effectively it learns a linear decision boundary when the data is linearly separable, converging to zero classification error over time. However, in cases where the data is not linearly separable, the perceptron fails to converge and continues to update the weights without settling, highlighting its limitations in more complex real-world scenarios. Extending the perceptron to higher dimensions confirmed that the algorithm can scale beyond 2D, with the loss curve providing insight into whether linear separation is likely. Overall, this project deepened my understanding of how simple learning algorithms operate and set the stage for exploring more advanced models like logistic regression or support vector machines."
  },
  {
    "objectID": "posts/loans/credit-risk.html",
    "href": "posts/loans/credit-risk.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Abstract\nIn this project, I developed a machine learning-based loan approval system designed to predict the likelihood of loan default and optimize lending decisions for a financial institution. Using a dataset containing borrower information such as age, income, employment length, homeownership status, and credit history, I first created data visualizations and summary tables to explore trends in the data. Then, I implemented a logistic regression model to classify applicants as either high-risk (default) or low-risk (non-default) and create a score function with a weight vector to weight each feature into the decision process. Then, to decide on a threshold value for my score function, I conducted a profit optimization analysis, adjusting approval thresholds to maximize expected returns. Through this study, I evaluated how loan approval rates varied across age groups, income levels, and loan purposes, ultimately assessing the fairness and efficiency of an automated credit decision system.\n\n\nData Extraction\nWe begin by downloading the data from the source. The columns in this data include extensive information about the prospective borrower – age, income, home owndership, loan intent, etc.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nData Exploration\nNow I will create some visualizations to explore patterns in the data.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='loan_intent', y='person_age', hue='person_home_ownership', data=df_train)\nplt.title(\"Age Distribution by Loan Intent and Homeownership Status\")\nplt.xlabel(\"Loan Intent\")\nplt.ylabel(\"Age\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Homeownership\")\n\n\n\n\n\n\n\n\nThis boxplot helps visualize the distribution of borrowers’ ages across different loan intents, while also distinguishing between different homeownership statuses. An immediate observation that I have is that the interquartile range for every loan type and homeownership status falls around 20-30 years of age, meaning that is the main age group seeking all types of loans. Another notable observation is the outliers per category. There are a lot more y-axis outliers (i.e. older people) that rent their homes, meaning it is common for older renters to be seeking a loan in almost all loan categories as opposed to older homeowners or mortgagers. Further, the largest age outlier category falls in the blue medical section of the x-axis, meaning that older-age renters are often seeking medical loans.\n\nplt.figure(figsize=(12, 6))\nsns.scatterplot(x='person_income', y='loan_int_rate', hue='loan_status', data=df_train, alpha=0.7)\nplt.xscale(\"log\")  # Log scale to handle wide income range\nplt.title(\"Interest Rate vs. Income by Loan Status\")\nplt.xlabel(\"Annual Income (Log Scale)\")\nplt.ylabel(\"Interest Rate (%)\")\nplt.legend(title=\"Loan Defaulted\")\nplt.show()\n\n\n\n\n\n\n\n\nThis scatter plot helps us visualize the relationship between a person’s income and the interest rate they are offered, and whether or not that loan was defaulted. As we can see by the scattering, there is no clear relationship between income and interest rate; there are both high income people with low and high interest rates, and there are low income people with both high and low interest rates. However, the abundance of orange to the top and left of the plot tells us that those with lower annual incomes tend to default on their loans much more often as opposed to higher-income individuals.\n\nsummary_table = df_train.groupby('person_emp_length').agg({\n    'loan_amnt': 'mean',\n    'loan_int_rate': 'mean',\n    'loan_percent_income': 'mean'\n}).reset_index()\n\n# Rename columns for clarity\nsummary_table.columns = ['Employment Length (Years)', 'Avg Loan Amount', 'Avg Interest Rate (%)', 'Avg Loan as % of Income']\n\n# Display the summary table\ndisplay(summary_table)\n\n\n\n\n\n\n\n\nEmployment Length (Years)\nAvg Loan Amount\nAvg Interest Rate (%)\nAvg Loan as % of Income\n\n\n\n\n0\n0.0\n8572.957492\n11.212005\n0.174275\n\n\n1\n1.0\n9196.537307\n11.356443\n0.178396\n\n\n2\n2.0\n9142.199217\n11.302131\n0.174799\n\n\n3\n3.0\n9448.739572\n11.095799\n0.170120\n\n\n4\n4.0\n9392.330416\n11.215608\n0.169326\n\n\n5\n5.0\n9596.843434\n10.857377\n0.171835\n\n\n6\n6.0\n9735.197368\n10.775047\n0.172580\n\n\n7\n7.0\n10095.776060\n10.930345\n0.163076\n\n\n8\n8.0\n10439.578714\n10.577886\n0.162084\n\n\n9\n9.0\n10799.319419\n10.904209\n0.160971\n\n\n10\n10.0\n10989.527629\n10.994492\n0.163850\n\n\n11\n11.0\n10832.043478\n10.936378\n0.146348\n\n\n12\n12.0\n10988.591703\n10.562252\n0.155087\n\n\n13\n13.0\n10824.500000\n10.692848\n0.161286\n\n\n14\n14.0\n11635.018727\n10.776500\n0.155393\n\n\n15\n15.0\n10270.750000\n10.474341\n0.156400\n\n\n16\n16.0\n11522.674419\n9.895086\n0.157364\n\n\n17\n17.0\n9693.181818\n10.468352\n0.140101\n\n\n18\n18.0\n10328.481013\n10.228333\n0.136329\n\n\n19\n19.0\n13135.795455\n11.061190\n0.153864\n\n\n20\n20.0\n12926.973684\n11.654412\n0.175526\n\n\n21\n21.0\n10059.166667\n12.068889\n0.144333\n\n\n22\n22.0\n10470.000000\n12.206429\n0.144000\n\n\n23\n23.0\n9700.000000\n11.513333\n0.188333\n\n\n24\n24.0\n10045.000000\n9.986250\n0.140000\n\n\n25\n25.0\n7625.000000\n10.353333\n0.111667\n\n\n26\n26.0\n12000.000000\n8.540000\n0.130000\n\n\n27\n27.0\n13200.000000\n9.938000\n0.186000\n\n\n28\n28.0\n17666.666667\n15.330000\n0.220000\n\n\n29\n29.0\n25000.000000\n13.430000\n0.340000\n\n\n30\n30.0\n24000.000000\n10.380000\n0.155000\n\n\n31\n31.0\n13500.000000\n10.450000\n0.075000\n\n\n32\n34.0\n7500.000000\n13.550000\n0.150000\n\n\n33\n38.0\n20000.000000\n9.880000\n0.190000\n\n\n34\n41.0\n3000.000000\n7.510000\n0.060000\n\n\n35\n123.0\n27500.000000\n11.280000\n0.345000\n\n\n\n\n\n\n\nFrom this summary table, we can see that typically those who have been employed longer get access to larger lines of credit. While the correlation is not super strong, it can be said that those who have been employed a short time (0-6) years are much more likely to get a loan that is worth less than $10,000.\n\n\nBuilding a Model to Find a Weight Vector\nBelow, I have trained a logistic regression model to predict whether a borrower will default on a loan based on all of the given features in the data set aside from loan grade and loan status (the target variable). I first separate the features into numeric (e.g., income, loan amount) and categorical (e.g., homeownership, loan intent). The numeric features are standardized using StandardScaler, while categorical features are one-hot encoded using OneHotEncoder, all managed through a ColumnTransformer. A Pipeline is then created to preprocess the data and train a logistic regression model. Finally, the weight vector (model coefficients) is retrieved, and cross-validation is performed to assess the model’s accuracy across five folds. The accuracy scores and mean accuracy are printed to evaluate the model’s predictive performance.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import cross_val_score\n\n\n\n# List of features (excluding forbidden ones)\nfeatures = [\n    \"person_age\", \"person_income\", \"person_home_ownership\", \"person_emp_length\",\n    \"loan_intent\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\",\n    \"cb_person_default_on_file\", \"cb_person_cred_hist_length\"\n]\n\n# Target variable\ntarget = \"loan_status\"\n\n# Example placeholder for DataFrame\n\n# Splitting numeric and categorical features\nnumeric_features = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\ncategorical_features = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\n\n# Preprocessing\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ]\n)\ndf = df_train.dropna()\n# Define logistic regression model\nmodel = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n])\n\n# Prepare data\nX = df[features]\ny = df[target]\n\n# Ensure target variable is binary (1 = Default, 0 = Non-Default)\ny = y.map({'Default': 1, 'Non-Default': 0}) if y.dtype == 'object' else y\n\n# Fit the model\nmodel.fit(X, y)\n\n# Retrieve the weight vector\nweights = model.named_steps['classifier'].coef_\nprint(\"Weight vector (w):\", weights)\n\n# Cross-validation to evaluate model accuracy\ncv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\nprint(\"Cross-validation accuracy scores:\", cv_scores)\nprint(\"Mean accuracy:\", np.mean(cv_scores))\n\nWeight vector (w): [[-0.0383707   0.04510115 -0.02445074 -0.58076891  1.03715014  1.3244998\n  -0.01195797  0.12828473  0.37763254 -1.36247275  0.86221669  0.42979327\n  -0.37518745  0.50616243  0.24713985 -0.19722704 -0.60501984 -0.0351749\n   0.04083611]]\nCross-validation accuracy scores: [0.85268442 0.85421213 0.84566688 0.84654006 0.84981445]\nMean accuracy: 0.8497835888866307\n\n\nFrom this we can see that our weight vector for all of the features is w = [-0.0383707 0.04510115 -0.02445074 -0.58076891 1.03715014 1.3244998 -0.01195797 0.12828473 0.37763254 -1.36247275 0.86221669 0.42979327 -0.37518745 0.50616243 0.24713985 -0.19722704 -0.60501984 -0.0351749 0.04083611]\nAnd our mean accuracy across our cross-validations is 0.849, meaning we predicted around 85% of loan approval decisions to be correct (did not default).\n\n\nFinding a Threshold\nNow we must find a threshold. To do this, the code below first generates predicted probabilities of loan default from the logistic regression model and defines a range of threshold values between 0 and 1. The profit if a loan is repaid and the loss if a loan defaults are computed using the following assumptions:\n\nIf ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loan ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍repaid ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍full, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍profit ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bank ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍equal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loan_amnt(1 + 0.25loan_int_rate)**10 - loan_amnt. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍This ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍formula ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍assumes ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍profit ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍earned ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bank ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍on ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍10-year ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loan ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍equal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍25% ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍interest ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍rate ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍each ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍year, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍with ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍other ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍75% ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍interest ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍going ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍things ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍like ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍salaries ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍people ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍who ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍manage ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bank. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍It ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍extremely ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍simplistic ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍does ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍not ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍account ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍inflation, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍amortization ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍over ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍time, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍opportunity ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍costs, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍etc.\nIf ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍borrower ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍defaults ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍on ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loan, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍“profit” ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bank ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍equal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loan_amnt(1 + 0.25loan_int_rate)**3 - 1.7*loan_amnt. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍This ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍formula ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍corresponds ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍same ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍profit-earning ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍mechanism ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍as ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍above, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍but ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍assumes ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍borrower ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍defaults ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍three ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍years ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍into ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loan ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bank ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loses ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍70% ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍principal.\n\nFor each threshold, the model classifies loan applicants, determining whether they are approved or denied. The total expected profit is then computed by summing profits from repaid loans and losses from defaulted loans. The optimal threshold is selected as the one that yields the highest total profit.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate predicted probabilities from the logistic regression model\ny_prob = model.predict_proba(X)[:, 1]\n\n# Define a range of threshold values to test\nthresholds = np.linspace(0, 1, 100)\n\n# Extract loan amounts and interest rates as numpy arrays\nloan_amounts = X[\"loan_amnt\"].to_numpy()\ninterest_rates = X[\"loan_int_rate\"].to_numpy()\n\n# Ensure interest rates are correctly scaled (divided by 100 if necessary)\nif interest_rates.max() &gt; 1:  # If the max value is greater than 1, assume percentages need scaling\n    interest_rates /= 100\n\n# Compute profit if loan is repaid and loss if defaulted (vectorized)\nprofit_if_repaid = loan_amounts * ((1 + 0.25 * interest_rates) ** 10) - loan_amounts\nloss_if_defaulted = loan_amounts * ((1 + 0.25 * interest_rates) ** 3) - (1.7 * loan_amounts)\n\n# Ensure y is a NumPy array\ny_np = y.to_numpy()\n\n# Vectorized computation of profit for each threshold\nprofits = np.zeros_like(thresholds)\n\nfor i, t in enumerate(thresholds):\n    predictions = (y_prob &gt;= t).astype(int)  # Convert probabilities to binary predictions\n    approved_loans = predictions == 0  # Loans that are approved\n\n    # Compute total profit\n    total_profit = np.sum(profit_if_repaid[(y_np == 0) & approved_loans]) + np.sum(loss_if_defaulted[(y_np == 1) & approved_loans])\n    profits[i] = total_profit\n\n# Find the threshold that maximizes profit\noptimal_threshold = thresholds[np.argmax(profits)]\nmax_profit = profits.max()\nexpected_profit_per_borrower = max_profit / len(y_prob)\n\n# Plot the profit curve\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, profits, label=\"Total Profit\", color='blue')\nplt.axvline(optimal_threshold, color='red', linestyle=\"--\", label=f\"Optimal Threshold: {optimal_threshold:.4f}\")\nplt.xlabel(\"Decision Threshold\")\nplt.ylabel(\"Total Profit\")\nplt.title(\"Profit Optimization for Loan Approval\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Output the results\nprint(f\"Optimal Threshold: {optimal_threshold:.4f}\")\nprint(f\"Maximum Profit: {max_profit:.2f}\")\nprint(f\"Expected Profit per Borrower: {expected_profit_per_borrower:.2f}\")\n\n\n\n\n\n\n\n\nOptimal Threshold: 0.4040\nMaximum Profit: 32086998.37\nExpected Profit per Borrower: 1400.75\n\n\nAbove, I have plotted a profit curve to visualize how profit changes with different decision thresholds, with the optimal threshold marked in red. As we can see, the threshold that optimizes profits is t = 0.4040, generating $1400.75 in profit per borrower.\n\n\nModel Evaluation: Bank’s Perspective\nNow that I have finalized our weight vector w and threshold t, I am going to evaluate the performance of the logistic regression model on the test dataset. The code below loads and cleans the test data by dropping missing values, extracts relevant features, and the target variable (loan_status) is mapped to binary values.\nUnder the financial formula assumptions provided above, the optimal threshold of t = 0.4040 is applied to classify loans as approved (0) or denied (1). The total profit is computed by summing profits from repaid loans and losses from defaulted loans within the approved subset. Finally, the expected profit per borrower is derived by dividing total profit by the number of test borrowers.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test_dirty = pd.read_csv(url)\ndf_test = df_test_dirty.dropna()\n\nimport pandas as pd\nimport numpy as np\n\n# Extract relevant features from the test set (excluding forbidden ones)\nX_test = df_test[[\"person_age\", \"person_income\", \"person_home_ownership\", \"person_emp_length\",\n                  \"loan_intent\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\",\n                  \"cb_person_default_on_file\", \"cb_person_cred_hist_length\"]]\n\n# Extract true labels\ny_test = df_test[\"loan_status\"]\n\n# Ensure target variable is binary (1 = Default, 0 = Non-Default)\ny_test = y_test.map({'Default': 1, 'Non-Default': 0}) if y_test.dtype == 'object' else y_test\n\n# Get predicted probabilities for the positive class (default) using trained model\ny_prob_test = model.predict_proba(X_test)[:, 1]\n\n# Extract loan amounts and interest rates as numpy arrays\nloan_amounts_test = X_test[\"loan_amnt\"].to_numpy()\ninterest_rates_test = X_test[\"loan_int_rate\"].to_numpy()\n\n# Ensure interest rates are correctly scaled (divided by 100 if necessary)\nif interest_rates_test.max() &gt; 1:  # If the max value is greater than 1, assume percentages need scaling\n    interest_rates_test /= 100\n\n# Compute profit if loan is repaid and loss if defaulted (vectorized)\nprofit_if_repaid_test = loan_amounts_test * ((1 + 0.25 * interest_rates_test) ** 10) - loan_amounts_test\nloss_if_defaulted_test = loan_amounts_test * ((1 + 0.25 * interest_rates_test) ** 3) - (1.7 * loan_amounts_test)\n\n# Ensure y_test is a NumPy array\ny_test_np = y_test.to_numpy()\n\n# Apply the chosen threshold to make loan approval decisions\npredictions_test = (y_prob_test &gt;= optimal_threshold).astype(int)\napproved_loans_test = predictions_test == 0  # Loans that are approved\n\n# Compute total profit on the test set\ntotal_profit_test = np.sum(profit_if_repaid_test[(y_test_np == 0) & approved_loans_test]) + \\\n                    np.sum(loss_if_defaulted_test[(y_test_np == 1) & approved_loans_test])\n\n# Compute expected profit per borrower\nexpected_profit_per_borrower_test = total_profit_test / len(y_test_np)\n\nprint(f\"Total Profit on Test Set: {total_profit_test:.2f}\")\nprint(f\"Expected Profit per Borrower on Test Set: {expected_profit_per_borrower_test:.2f}\")\n\nTotal Profit on Test Set: 7684515.98\nExpected Profit per Borrower on Test Set: 1340.87\n\n\nAs we can see, the expected profit per borrower on the test set is $1340.87, which is very similar to the value of $1400.75 that was calculated on the training set. From a Bank’s perspective, our model is reasonably profitable and accurate.\n\n\nModel Evaluation: Borrower’s Perspective\nNow we will evaluate our model from the borrower’s perspective. This code analyzes the loan approval rates under the model’s decision system across different demographic and financial groups. The analysis is divided into three key perspectives:\nAge Groups: Borrowers are categorized into age brackets, and approval rates are computed for each group.\nLoan Type (Medical vs. Other): Loans are grouped into Medical and Other categories, and their respective approval and default rates are calculated.\nIncome Levels: Borrowers are divided into five quintile income brackets, and approval rates for each group are determined.\n\nimport pandas as pd\nimport numpy as np\n\n# Create a copy to avoid modifying the original DataFrame\ndf_test = df_test.copy()\n\n# Assign approval results safely\ndf_test.loc[:, \"approved\"] = approved_loans_test\n\n# Analyze approval rates across age groups\nage_groups = pd.cut(df_test[\"person_age\"], bins=[18, 25, 35, 50, 65, 100], labels=[\"18-24\", \"25-34\", \"35-49\", \"50-64\", \"65+\"])\napproval_rate_by_age = df_test.groupby(age_groups)[\"approved\"].mean()\n\n# Create a new column to categorize loans as 'Medical' or 'Other'\ndf_test.loc[:, \"loan_category\"] = df_test[\"loan_intent\"].apply(lambda x: \"Medical\" if x == \"MEDICAL\" else \"Other\")\n\n# Compute approval rates\napproval_rate_medical = df_test[df_test[\"loan_category\"] == \"Medical\"][\"approved\"].mean()\napproval_rate_other = df_test[df_test[\"loan_category\"] == \"Other\"][\"approved\"].mean()\n\n# Compute default rates\ndefault_rate_medical = df_test[df_test[\"loan_category\"] == \"Medical\"][\"loan_status\"].mean()\ndefault_rate_other = df_test[df_test[\"loan_category\"] == \"Other\"][\"loan_status\"].mean()\n\n# Analyze impact of income on loan approvals\nincome_groups = pd.qcut(df_test[\"person_income\"], q=5, labels=[\"Low\", \"Lower-Middle\", \"Middle\", \"Upper-Middle\", \"High\"])\napproval_rate_by_income = df_test.groupby(income_groups)[\"approved\"].mean()\n\n# Display results\nprint(\"Loan Approval Rates by Age Group:\")\nprint(approval_rate_by_age)\n\nprint(\"\\nLoan Approval and Default Rates by Loan Type:\")\nprint(f\"Medical - Approval Rate: {approval_rate_medical:.2f}, Default Rate: {default_rate_medical:.2f}\")\nprint(f\"Other - Approval Rate: {approval_rate_other:.2f}, Default Rate: {default_rate_other:.2f}\")\n\nprint(\"\\nLoan Approval Rates by Income Group:\")\nprint(approval_rate_by_income)\n\nLoan Approval Rates by Age Group:\nperson_age\n18-24    0.779661\n25-34    0.815186\n35-49    0.848432\n50-64    0.731707\n65+      1.000000\nName: approved, dtype: float64\n\nLoan Approval and Default Rates by Loan Type:\n\nLoan Approval and Default Rates for Medical vs. Other Loans:\nMedical - Approval Rate: 0.74, Default Rate: 0.28\nOther - Approval Rate: 0.82, Default Rate: 0.21\n\nLoan Approval Rates by Income Group:\nperson_income\nLow             0.575152\nLower-Middle    0.723222\nMiddle          0.835861\nUpper-Middle    0.901482\nHigh            0.973799\nName: approved, dtype: float64\n\n\nFrom the first table we can see that my model predicts that the group with the lowest loan approval rates is 50-64. Aside from that group, the younger age groups tend to have lower approval ratings. This means, according to my model, it is most difficult for people aged 50-64 to get loans. Whereas people above the age of 65 are guaranteed to be approved.\nFrom the second table we can see that my model predicts medical loans have a lower approval rating and a higher default rate when compared to other loans. So, according to my model, it is more difficult for people to get loans for medical expenses than it is for business, education, homes, etc., but it is also more common for people to default on medical loans.\nFrom the third table, we can see that my model predicts the loan approval rate to be worse for lower income individuals, and as someone moves into a higher income group, they have a higher chance of loan approval. Thus, it is much easer for a wealthier person to get access to credit under my system.\n\n\nReflection\nThoughout this project, I learned a lot about automated decision systems. While the model was designed to maximize lender profitability and reduce financial risk, these systems have far-reaching consequences beyond financial efficiency. Decisions made by models like mine directly impact people’s access to credit—a critical resource for economic mobility, education, healthcare, and entrepreneurship. By relying on historical data and statistical relationships, machine learning models can often reinforce existing inequalities in financial systems. For example, my study revealed that younger applicants and lower-income individuals had lower approval rates, reflecting biases where those already struggling financially find it even harder to secure credit. It is not feasible for banks to give out loans to people who are likely to default on them, and machine learning models can very accurately predict whether someone is high or low risk. However, these models are rarely 100% accurate (mine was 85%). For someone in a high-risk group that will be able to pay back a loan but never gets accepted, this decision-making system seems unfair and negatively impacts people from these high-risk groups.\nAnother controversy arises when discussing medical loans. Applicants seeking loans for medical expenses faced higher rejection rates due to their higher historical default rates. Considering ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍people ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍seeking ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loans ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍medical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍expense ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍have ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍high ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍rates ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍default, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍it ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍fair ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍it ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍more ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍difficult ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍them ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍obtain ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍access ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍credit? In the context of credit, I would define a fair system as one that uses objective and relevant financial factors without discrimination on vulnerable groups. However, sick people are often vulnerable as they often cannot work and have no control over their financial situation. This highlights the flaws in purely data-driven credit decisions. While lenders must manage risk, rejecting medical borrowers based solely on historical default rates ignores the uncontrollable nature of medical crises. True fairness should account for context, distinguishing between defaults caused by financial irresponsibility and those driven by hardship. A more ethical system would incorporate alternative lending structures—such as flexible repayment plans or government-backed medical loans—to ensure vulnerable borrowers aren’t automatically excluded from financial support. The challenge is not just predicting risk, but balancing objective financial metrics with contextual and individual nuance in credit decisions."
  },
  {
    "objectID": "posts/logistic/index.html",
    "href": "posts/logistic/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Abstract\n\nimport torch\nimport matplotlib.pyplot as plt\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\n\n\nPart A: Implementing Logistic Regression\nIn my logistic.py script, I have implemented a few classes to perform logistic regression on a data set.\nThe LinearModel class provides the foundation for linear models by maintaining a weight vector \\(w\\) and defining core functionality to compute scores and predictions. The score method calculates the raw output (dot product) for each data point in the feature matrix \\(X\\) by multiplying it with the model’s weight vector \\(w\\). If \\(w\\) has not been initialized yet, it randomly initializes it to a vector of the correct shape. The predict method then uses these scores to produce binary predictions by thresholding the score at zero; any value greater than zero becomes a 1.0 , otherwise 0.0.\nThe LogisticRegression class inherits from LinearModel and implements functionality specific to binary classification using the logistic loss. It introduces a loss method that calculates the average logistic loss over the dataset by applying the sigmoid function to the model’s scores to obtain probabilities, and then computing the cross-entropy loss with the true labels y. The grad method computes the gradient of this loss with respect to the weight vector \\(w\\), which is necessary for optimization. It does this by taking the derivative of the loss function, resulting in the formula \\[\\frac{1}{n} \\cdot X^{T} @ (\\sigma(a)-y)\\] Where \\(\\sigma(s)\\) is the vector of predicted probabilities. These methods enable learning the optimal weights for classification tasks through gradient-based updates.\nThe GradientDescentOptimizer class is responsible for updating the model’s weights using gradient descent with momentum. Initialized with a reference to a model (e.g., LogisticRegression), it keeps track of the previous weight vector to compute the momentum term. The step method performs a single optimization step using the formula \\[w_{k+1} = w_k - \\alpha \\cdot \\nabla L(w_k) + \\beta(w_k -w_{k-1})\\] Where \\(\\alpha\\) is the learning rate and \\(\\beta\\) is the momentum coefficient. If it’s the first step, the previous weight vector is simply initialized to the current weights. The momentum term helps smooth out updates and can accelerate convergence, especially in cases where the loss surface has narrow valleys or noisy gradients.\n\n\nPart B: Experiments\nBelow is I have written a few functions that will allow us to perform and vizualize various experiments.\nThe run_experiment function runs a logistic regression training loop using gradient descent with a specified learning rate (alpha) and momentum parameter (beta) for a set number of iterations. It initializes a LogisticRegression model and a GradientDescentOptimizer. During each iteration, the function computes the current loss and performs a gradient descent step using the specified parameters. The loss at each iteration is stored in a list, allowing for later analysis of convergence behavior. It returns both the loss history and the trained model.\nThe plot_decision_boundary function visualizes the decision boundary learned by a trained logistic regression model over a 2D feature space. It first defines a grid of (x, y) coordinates that spans the input data range and constructs a feature matrix that includes a bias term. It then uses the model to compute sigmoid probabilities over this grid and reshapes the results for contour plotting. The function plots a contour at the decision threshold (probability = 0.5), which represents the decision boundary. It overlays this boundary on a scatter plot of the original dataset, clearly showing how well the model separates the two classes.\nThe plot_loss_curve function plots the progression of logistic loss values recorded during the training process. It takes a list of loss values (one per iteration) and creates a line plot with iteration number on the x-axis and logistic loss on the y-axis. The resulting graph helps visualize whether the loss is decreasing consistently—a key indicator of successful convergence during training. A smooth, monotonically decreasing curve suggests that the gradient descent algorithm is working correctly.\n\ndef run_experiment(X, y, beta, alpha, iterations):\n    \"\"\"\n    Run gradient descent on the provided classification data for a given beta (momentum)\n    and learning rate alpha, returning the loss history and final model.\n    \"\"\"\n    model = LogisticRegression()\n    optimizer = GradientDescentOptimizer(model)\n    losses = []\n    for i in range(iterations):\n        loss = model.loss(X, y).item()\n        losses.append(loss)\n        optimizer.step(X, y, alpha=alpha, beta=beta)\n    return losses, model\n\n# === Visualization Functions ===\n\ndef plot_decision_boundary(model, X, y):\n    \"\"\"\n    Plot the decision boundary learned by the model along with the data.\n    We compute the sigmoid outputs on a grid and then plot a contour where\n    the decision boundary (σ(s)=0.5) lies.\n    \"\"\"\n    plt.figure(figsize=(8,6))\n    \n    # Determine grid boundaries\n    x_min, x_max = X[:,0].min() - 0.5, X[:,0].max() + 0.5\n    y_min, y_max = X[:,1].min() - 0.5, X[:,1].max() + 0.5\n    \n    # Create a meshgrid of points covering the input space\n    xx, yy = torch.meshgrid(torch.linspace(x_min, x_max, 100), \n                            torch.linspace(y_min, y_max, 100), indexing='xy')\n    # Note: bias term is 1 for all grid points\n    grid = torch.stack([xx.reshape(-1), yy.reshape(-1), torch.ones(xx.numel())], dim=1)\n    \n    # Compute probabilities on the grid\n    probs = torch.sigmoid(model.score(grid)).reshape(xx.shape)\n    \n    # Plot contour at probability 0.5 (decision boundary)\n    plt.contourf(xx, yy, probs.detach().numpy(), levels=[0, 0.5, 1], alpha=0.3, cmap='gray')\n    plt.contour(xx, yy, probs.detach().numpy(), levels=[0.5], colors='black')\n    \n    # Scatter plot of original data\n    plt.scatter(X[:,0][y==0], X[:,1][y==0], label=\"Class 0\", edgecolor='k')\n    plt.scatter(X[:,0][y==1], X[:,1][y==1], label=\"Class 1\", edgecolor='k')\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.title(\"Decision Boundary after Training (Vanilla Gradient Descent)\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef plot_loss_curve(losses):\n    \"\"\"\n    Plot the loss value as a function of iterations.\n    A monotonically decreasing curve is expected for a converging algorithm.\n    \"\"\"\n    plt.figure(figsize=(8,6))\n    plt.plot(losses, marker='o')\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"Logistic Loss\")\n    plt.title(\"Loss over Gradient Descent Iterations (Vanilla Gradient Descent)\")\n    plt.grid(True)\n    plt.show()\n\n\nExperiment 1: Vanilla Gradient Descent (\\(\\beta=0\\))\nFirst let’s create some sample data to work with by running the following code\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nNow we want to run a gradient descent for logistic regression with \\(\\alpha=0.1\\) and \\(\\beta=0\\), i.e no momentum. The code below runs my run_experiment function with those values and the above synthetic data, plotting the decision boundary for classification after training and the loss over the course of iterations.\n\niterations = 1000\nalpha = 0.1\n\n# Vanilla GD (β = 0)\nlosses_vanilla, model_vanilla = run_experiment(X, y, beta=0.0, alpha=alpha, iterations=iterations)\nplot_decision_boundary(model_vanilla, X, y)\nplot_loss_curve(losses_vanilla)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we can see from the first graph, the gradient ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍descent ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍logistic ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍regression ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍converges ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍weight ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍vector ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍\\(w\\) ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍looks ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍visually ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍correct, as it is mostly separating the blue data from the red data correctly.\nAs we can from the second graph, the loss decreases monotonically over the course of the iterations.\n\n\nExperiment 2: Gradient Descent with Momentum (\\(\\beta = 0.9\\))\nNow we want to use the same data set as we did in the first experiment, but this time we will implement momentum, setting \\(\\beta=0.9\\). The code below runs the experiemnt on the same data but with \\(\\beta=0.9\\) and \\(\\alpha=0.1\\) still.\n\niterations = 1000\nalpha = 0.1\n\n# Gradient Descent with momentum (β = 0.9)\nlosses_momentum, model_momentum = run_experiment(X, y, beta=0.9, alpha=alpha, iterations=iterations)\n\n# Plotting the Loss Curves\nplt.figure(figsize=(10, 6))\nplt.plot(losses_vanilla, marker='o', label=\"Vanilla GD (β = 0)\")\nplt.plot(losses_momentum, marker='o', label=\"Momentum GD (β = 0.9)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Logistic Loss\")\nplt.title(\"Loss over Iterations: Vanilla GD vs. Momentum GD\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see by the plot, gradient descent with momention can converge to the correct weight vector (minimal loss) in fewer iterations than vanilla gradient descent.\n\n\nExperiment 3: Overfitting\nNow we want to test Linear Regression on data with more than two dimensions. The code below generates and trains a logistic regression model on synthetic high-dimensional data using vanilla gradient descent. The generate_high_dim_data function creates a dataset with 50 data points (n_points) and 100 features (p_dim). Each data point has p_dim - 1 features sampled from a standard normal distribution, and a final feature of constant ones is appended as a bias term. Labels (y) are randomly assigned as 0 or 1 with equal probability. Two separate datasets are generated: one for training and one for testing. A logistic regression model is then trained on the training data using a custom GradientDescentOptimizer, which performs 200 iterations of optimization with a learning rate (alpha) of 0.1 and no momentum (beta = 0.0). During training, the loss is recorded at each iteration. Finally, the model’s predictions are evaluated on both the training and test sets, and the corresponding accuracies are printed.\n\ndef generate_high_dim_data(n_points=50, p_dim=100):\n    \"\"\"\n    Generates a dataset with n_points data and p_dim features.\n    The first (p_dim - 1) features are drawn from a standard normal distribution,\n    and the last column is a constant bias (ones).\n    Labels are randomly assigned (0 or 1) with equal probability.\n    \"\"\"\n    # p_dim includes the bias column.\n    X_raw = torch.randn(n_points, p_dim - 1)\n    bias = torch.ones(n_points, 1)\n    X = torch.cat([X_raw, bias], dim=1)\n    y = torch.randint(0, 2, (n_points,)).float()\n    return X, y\n\n# Generate the two datasets independently but with the same parameters.\nX_train, y_train = generate_high_dim_data(n_points=50, p_dim=100)\nX_test, y_test = generate_high_dim_data(n_points=50, p_dim=100)\n\n# === Training the Logistic Regression Model ===\n\nmodel = LogisticRegression()\noptimizer = GradientDescentOptimizer(model)\n\nnum_iterations = 200\ntrain_losses = []\n\nfor i in range(num_iterations):\n    loss = model.loss(X_train, y_train)\n    train_losses.append(loss.item())\n    # Vanilla gradient descent (beta = 0).\n    optimizer.step(X_train, y_train, alpha=0.1, beta=0.0)\n\n# === Evaluate the Model on Training and Test Data ===\n\ntrain_preds = model.predict(X_train)\ntrain_accuracy = (train_preds == y_train).float().mean().item()\n\ntest_preds = model.predict(X_test)\ntest_accuracy = (test_preds == y_test).float().mean().item()\n\nprint(\"Training Accuracy: {:.2f}%\".format(train_accuracy * 100))\nprint(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))\n\nTraining Accuracy: 100.00%\nTest Accuracy: 48.00%\n\n\nAs we can see, while the Logistic Regression classified the data with 100% accuracy on the training set, it only performed with 48% accuracy on the test set. This suggests that the model has overfit the training data—essentially memorizing the patterns in the training set rather than learning generalizable features. Since the dataset is high-dimensional (with 100 features and only 50 data points), the model has enough flexibility to perfectly separate the training points. However, this does not translate to good performance on unseen data, which explains the low accuracy on the test set.\n\n\nExperiment 4: Performance on Empirical Data\nNow we want to test our implementation of Logistic Algorithm on real-world, empirical data. From Kaggle, I found a data set on diabetes on other physical metrics created by GeeksForGeeks. The link to the data set can be found here. The dataset includes key metrics necessary for predictins diabetes such as age, glucose levels, insulin, BMI, etc.\nMy goal is train a Logistic Regression model to predict/classify whether a student passed or failed their final exam based on the academic data.\nI downloaded the data as a .csv file, now I must read it into a data frame.\n\nimport pandas as pd\n\ndf = pd.read_csv('/Users/evanflaks/Downloads/diabetes-dataset.csv')\ndf.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\nNow we prepare our dataset for logistic regression. First, it separates the feature matrix X from the binary target variable y (which indicates whether a person has diabetes). It then standardizes the features using StandardScaler to ensure each has a mean of 0 and standard deviation of 1, which helps the model train more effectively. A column of ones is appended to the feature matrix to act as a bias (intercept) term. Finally, both the feature matrix and target vector are converted into PyTorch tensors to be used in training a logistic regression model.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport torch\n\n# Split features and target\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Add bias column (intercept term)\nimport numpy as np\nX_scaled = np.hstack([X_scaled, np.ones((X_scaled.shape[0], 1))])\n\n# Convert to torch tensors\nX_tensor = torch.tensor(X_scaled, dtype=torch.float32)\ny_tensor = torch.tensor(y.values, dtype=torch.float32)\n\nNow we split our data into training, validation, and testing sets, with a 60%, 20%, and 20% split.\n\n# First: split into 80% train_val and 20% test\nX_temp, X_test, y_temp, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n\n# Then split train_val into 60% train and 20% val =&gt; 0.25 * 0.8 = 0.2\nX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n\nThis function trains a logistic regression model using gradient descent with optional momentum. It takes the model, optimizer, training and validation data, and hyperparameters like the number of iterations (n_iter), learning rate (alpha), and momentum (beta). Before training begins, it calls model.score(X_train) to ensure the model’s weights are initialized. In each iteration of training, the optimizer updates the model weights, and the function records the loss on both the training and validation sets. After all iterations, it returns two lists containing the training and validation losses over time, which will be used to visualize the model’s learning progress.\n\ndef train(model, optimizer, X_train, y_train, X_val, y_val, n_iter=100, alpha=0.1, beta=0.0):\n    train_losses = []\n    val_losses = []\n\n    # 🔧 Force initialization of model.w before optimizer.step()\n    model.score(X_train)\n\n    for i in range(n_iter):\n        optimizer.step(X_train, y_train, alpha, beta)\n        train_loss = model.loss(X_train, y_train).item()\n        val_loss = model.loss(X_val, y_val).item()\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n    return train_losses, val_losses\n\nThe code below trains two logistic regression models—one using standard gradient descent and the other using gradient descent with momentum—and compares their performance by plotting the training and validation loss over 100 iterations. It first initializes and trains model1 without momentum (\\(\\beta = 0.0\\)), then initializes and trains model2 with momentum (\\(\\beta = 0.9\\)). The recorded loss values for both models are plotted using Matplotlib.\n\nimport matplotlib.pyplot as plt\n\n# Without momentum\nmodel1 = LogisticRegression()\noptimizer1 = GradientDescentOptimizer(model1)\ntrain1, val1 = train(model1, optimizer1, X_train, y_train, X_val, y_val, n_iter=100, alpha=0.1, beta=0.0)\n\n# With momentum\nmodel2 = LogisticRegression()\noptimizer2 = GradientDescentOptimizer(model2)\ntrain2, val2 = train(model2, optimizer2, X_train, y_train, X_val, y_val, n_iter=100, alpha=0.1, beta=0.9)\n\n# Plotting\nplt.plot(train1, label='Train Loss (No Momentum)')\nplt.plot(val1, label='Val Loss (No Momentum)')\nplt.plot(train2, label='Train Loss (With Momentum)')\nplt.plot(val2, label='Val Loss (With Momentum)')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSimilar to our earlier experiment, in both the validation and training sets, loss monotonically decreases over the course of iterations. For both sets, the decrease is more rapid with Momentum, leveling out earlier. Without momentum, loss decreases more gradually but eventually reaches similar loss levels as the models with momentum.\n\ndef accuracy(model, X, y):\n    preds = model.predict(X)\n    return (preds == y).float().mean().item()\n\nfinal_test_loss = model2.loss(X_test, y_test).item()\nfinal_test_accuracy = accuracy(model2, X_test, y_test)\n\nprint(f\"Test Loss: {final_test_loss:.4f}\")\nprint(f\"Test Accuracy: {final_test_accuracy*100:.2f}%\")\n\nTest Loss: 0.5299\nTest Accuracy: 72.73%\n\n\nOur logistic regression model found a final loss of 0.5299 and an accuracy of 72.73% on the data test set.\n\n\n\nConclusion\nIn this project, I implemented logistic regression from scratch in PyTorch and designed a complete training pipeline with gradient descent, including optional momentum. I began by building three core components: a base LinearModel class for linear predictions, a LogisticRegression class that computes logistic loss and gradients, and a GradientDescentOptimizer that updates weights using gradient descent with momentum. After verifying these implementations, I created synthetic classification data and ran multiple experiments to understand how different optimization settings influence training performance. Specifically, I trained models both with and without momentum, plotted the loss curves over iterations, and visualized the decision boundaries. These visualizations showed that adding momentum accelerated convergence and helped the model better separate the two classes, especially when the data had higher noise. I also tested the model on high-dimensional data, where it achieved perfect accuracy on the training set but performed poorly on unseen test data—highlighting the classic problem of overfitting in high-dimensional spaces with small sample sizes. I then transitioned to a real-world dataset on diabetes and evaluated the model’s predictive performance after preprocessing, scaling, and splitting the data into training, validation, and test sets. The model achieved a much more realistic 72.73% accuracy on the empirical test set, showing that it can generalize better when trained on structured, real-world data. I also analyzed the impact of momentum in gradient descent and found that adding momentum allowed the model to converge more quickly, with smoother and faster decreases in training and validation loss curves. Overall, this project helped me deepen my understanding of loss functions, overfitting, optimization techniques, and the practical challenges of applying machine learning models to real-world data."
  },
  {
    "objectID": "posts/penguins/penguins.html",
    "href": "posts/penguins/penguins.html",
    "title": "Palmer Penguins Classification",
    "section": "",
    "text": "Abstract\nThis goal of this project was to develop a machine learning model to classify penguin species based on quantitative and qualititave characteristics using the Palmer Penguins dataset. After accessing the data and splitting it up into training and testing sets, I created visualizations on the training data to better understand which species exhibited particular traits. Then, I used a feature selection system to determine which features (two quantitative and one qualitiative) would be the best predictor of a penguin’s species. From there, I used these features to train a logistic regression model. The model’s performance is assessed using training and test accuracy and decision region visualizations. The goal of this assignment was to take a penguin with unknown species, analyze three of its given features, and with 100% accuracy, predict its species.\n\n\nData Preparation\nFirst I must access and read the data from the source.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nNow, I must prepare the dataset for machine learning by encoding the target variable (Species) into numerical values, dropping unnecessary columns, removing missing values and invalid entries, converting categorical columns into numerical representations via one-hot encoding, and splitting up training and testing sets.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\n# Initialize LabelEncoder for the target variable\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n# Function to preprocess data\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]  # Remove rows with invalid 'Sex' values\n    df = df.dropna()  # Remove any remaining missing values\n    y = le.transform(df[\"Species\"])  # Encode the target variable\n    df = df.drop([\"Species\"], axis=1)  # Remove the target column from features\n    df = pd.get_dummies(df)  # One-hot encode categorical features\n    return df, y\n\n# Prepare the dataset\nX, y = prepare_data(train)\n\n# Split the dataset into 80% training and 20% testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Convert training data back into a DataFrame for visualization purposes\ntrain_data = X_train.copy()\ntrain_data[\"Species\"] = le.inverse_transform(y_train)  # Convert encoded labels back to species names\n\n# Reconstruct the 'Island' column from one-hot encoded values\nisland_columns = ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\ntrain_data[\"Island\"] = train_data[island_columns].idxmax(axis=1).str.replace(\"Island_\", \"\")\n\n# Check the sizes of the splits\nprint(f\"Training Set Size: {X_train.shape[0]}\")\nprint(f\"Test Set Size: {X_test.shape[0]}\")\n\nTraining Set Size: 204\nTest Set Size: 52\n\n\n\n\nVisualizations\nNow, to visualize the dataset, I have created a scatter plot that plots flipper length vs. body mass of the three species and a bar chart showing the distribution of Penguins found on the three different islands. Finally, I created a summary table that shows each species’ average Culmen Length and Culmen Depth.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=train_data, x='Flipper Length (mm)', y='Body Mass (g)', hue='Species', style='Species')\n\n# Labels and title\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Body Mass (g)')\nplt.title('Flipper Length vs. Body Mass by Species (Training Data Only)')\nplt.legend(title='Species')\nplt.show()\n\n\n\n\n\n\n\n\nAbove, I have created a scatter plot that plots the flipper length and body mass of each penguin observed in the training set. I created a key with different symbols to plot each species so I could visualize the physical differences between each species. This visualization gave me valuable insight, informing me that Gentoo penguins are much larger in terms of body mass and flipper length than both Adelie and Chinstrap penguins. Adelie and Chinstrap penguins, as represented by the blue circles and green squares, share very similar sizes in these metrics. From this, we can conclude that body mass and flipper length would be excellent features with which to train our model in order to distinguish between Gentoo or not-gentoo, but these features would not help our model distinguish between Adelie and Chinstrap.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create a count plot for species distribution by island (training data only)\nplt.figure(figsize=(8, 6))\nsns.countplot(data=train_data, x='Island', hue='Species')\n\n# Labels and title\nplt.xlabel('Island')\nplt.ylabel('Count')\nplt.title('Penguin Species Distribution by Island (Training Data Only)')\nplt.legend(title='Species')\nplt.show()\n\n\n\n\n\n\n\n\nFor this visualization, I created a bar chart to see the island where each observed penguin was found. Here, we can see that Gentoo penguins are only found on Biscoe Island and Chinstrap penguins are only found on Dream Island. Adelie penguins, on the other hand, are found on all three islands. This bar chart tells me that Island location could help train my model to distinguish between Adelie and Chinstrap because Chinstrap penguins seem to only be found on Dream Island. So, if we see a penguin with quantitaive measurables that could fall under Adelie or Chinstrap, but the penguin was found on Torgersen Island, the model would correctly predict that the penguin is Adelie. This is especially helpful considering the quantitative features observed in my first visualization had a lot of overlap between Adelie and Chinstrap.\n\nimport pandas as pd\n\n# Compute the average Culmen Length & Depth in the training set only\nsummary_table = train_data.groupby(\"Species\")[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].mean()\n\n# Display the summary table\nprint(summary_table)\n\n\n                                           Culmen Length (mm)  \\\nSpecies                                                         \nAdelie Penguin (Pygoscelis adeliae)                 39.048837   \nChinstrap penguin (Pygoscelis antarctica)           48.697778   \nGentoo penguin (Pygoscelis papua)                   47.031507   \n\n                                           Culmen Depth (mm)  \nSpecies                                                       \nAdelie Penguin (Pygoscelis adeliae)                18.412791  \nChinstrap penguin (Pygoscelis antarctica)          18.406667  \nGentoo penguin (Pygoscelis papua)                  14.957534  \n\n\nHere, I created a table to see the average culmen length and culmen depth of each penguin species from the data set. From this, we can see that culmen length would be a great feature to distinguish Adelie penguins from the other two species because they have considerably shorter culmens. However, culmen length would not be very helpful in distinguishing Gentoo from Chinstrap because they have very similar measurements. As for culmen depth, we see that Adelie and Chinstrap have very similar average measurements while Gentoo penguins have considerably smaller culmen depths. This means culmen depth would be a good feature for my model to distinguish Gentoo penguins from the other two species.\n\n\nData Visualization Conclusions\nAfter creating three data visualizations, I have intuition for some of the features that would help train a successful model. From my scatter plot, I saw that body mass and flipper length would both be great features to help my model distinguish between Gentoo and Adelie or Chinstrap, but would not be very helpful in distinguishing Adelie from Chinstrap. Then, from the summary table, we could see that Adelie penguins, on average, have around a 20% shorter culmen length than Chinstrap penguins, so that would be a great feature to help my model distinguish between those two species. Finally, since Chinstrap penguins were only found on Dream Island and Gentoo Penguins were only found on Biscoe Island as represented by my bar chart, I figured this would be a nice qualitative feature with which to train my model.\n\n\nFeature Selection\nNow we must choose which three features we want to use to predict the penguin species. I gained some valuable insight from my visualizations but now want to use a systematic approach to select the three best features. To do this, I used a Random Forest Classifier by measuring how much each feature contributes to making accurate predictions. When splitting a node in a decision tree, features that provide better separation between classes are preferred. The model tracks how often a feature is used in important splits and how much it improves classification accuracy. The importance of each feature is calculated as the total reduction in prediction error (impurity) it provides across all trees. Features with higher scores contribute more to the model’s decision-making.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train a random forest to assess feature importance\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Get feature importance scores\nfeature_importance = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': model.feature_importances_\n})\n\n# Sort by importance\nfeature_importance = feature_importance.sort_values(by='Importance', ascending=False)\nprint(feature_importance)\n\n                     Feature  Importance\n0         Culmen Length (mm)    0.234636\n2        Flipper Length (mm)    0.179364\n5          Delta 13 C (o/oo)    0.161621\n1          Culmen Depth (mm)    0.148071\n4          Delta 15 N (o/oo)    0.075136\n7               Island_Dream    0.063007\n6              Island_Biscoe    0.059049\n3              Body Mass (g)    0.058736\n8           Island_Torgersen    0.012969\n10      Clutch Completion_No    0.002622\n12                Sex_FEMALE    0.002250\n13                  Sex_MALE    0.001822\n11     Clutch Completion_Yes    0.000717\n9   Stage_Adult, 1 Egg Stage    0.000000\n\n\nFrom this, we can conclude that the best qualitative category is Island and the best quantitative categories are Culmen Length and Flipper Length. So, these are the features we will train our model on.\n\n\nLogistic Regression Model Testing\nThe model below uses logistic regression to classify data based on flipper length, culmen length, and island. First, it standardizes the numerical features using StandardScaler() to ensure consistent scaling. Then, it trains a logistic regression model with increased iterations (1000) to ensure convergence. Finally, it evaluates performance using training and test accuracy, helping assess how well the model generalizes.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# Define selected features\ncols = ['Flipper Length (mm)', 'Culmen Length (mm)','Island_Biscoe', 'Island_Dream', 'Island_Torgersen',]\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[cols]), columns=cols)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test[cols]), columns=cols)\n\n\n# Initialize and train logistic regression with increased iterations\nLR = LogisticRegression(max_iter=1000)  # Increased max_iter to ensure convergence\nLR.fit(X_train_scaled, y_train)\n\n# Evaluate model\nprint(\"Training Accuracy:\", LR.score(X_train_scaled, y_train))\nprint(\"Test Accuracy:\", LR.score(X_test_scaled, y_test))\n\n\n\nTraining Accuracy: 0.9754901960784313\nTest Accuracy: 1.0\n\n\n\n\nPlotting Decision Regions\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, scaler):\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(7, 3))\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n\n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({X.columns[0]: XX, X.columns[1]: YY})\n\n        # Initialize categorical features as zeros\n        for j in qual_features:\n            XY[j] = 0\n\n        # Set the specific category feature to 1\n        XY[qual_features[i]] = 1\n\n        # Standardize XY to match model input\n        XY_scaled = pd.DataFrame(scaler.transform(XY), columns=X.columns)\n\n        # Predict decision boundary\n        p = model.predict(XY_scaled)\n        p = p.reshape(xx.shape)\n\n        # Use contour plot to visualize the predictions\n        decision_cmap = ListedColormap([\"blue\", \"green\", \"orange\"])  # Match species colors\n        axarr[i].contourf(xx, yy, p, cmap=decision_cmap, alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n\n        # Plot the actual training data points\n        species_cmap = ListedColormap([\"blue\", \"green\", \"orange\"])  # blue = Adelie, Green = Chinstrap, orange = Gentoo\n        axarr[i].scatter(x0[ix], x1[ix], c=y[ix], cmap=species_cmap, vmin=0, vmax=2)\n\n        axarr[i].set(\n            xlabel=X.columns[0],\n            ylabel=X.columns[1],\n            title=qual_features[i]\n        )\n\n        patches = []\n        for color, spec in zip([\"blue\", \"green\", \"orange\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color=color, label=spec))\n\n        axarr[i].legend(handles=patches, title=\"Species\", loc=\"best\")\n\n    plt.tight_layout()\n\n# Call the function with scaler applied\nplot_regions(LR, X_train[cols], y_train, scaler)\nplot_regions(LR, X_test[cols], y_test, scaler)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbove, I have plotted my model’s decision regions on both the training and test sets, showing the thresholds of prediction for each species split up by each island.\nFor both the training and test set of penguins on Torgersen Island, my model predicted with 100% success as there were no other penguin found on this island besides Adelies.\nDream Island contained both Adelie and Chinstrap penguins, which both have very similar flipper length, so the decision regions for that island are based almost entirely by culmen length, as can be seen by the near-horizontal divide between the green and blue regions. My model did have some error on the training set for Dream Island as there were some outliers of the decision regions – Chinstraps with below average culmen lengths and Adelies with above average culmen lengths. The test set did not have as many outliers and therefore still predicted with 100% accuracy on Dream Island.\nAs for Biscoe Island, there were both Gentoo and Adelie penguins. As observed in my data visualizations, Gentoo penguins are considerably larger than Adelie in both flipper length and culmen length so the regions are divided diagonally. There were no overlaps in the decision regions on Biscoe in either the test or training set.\nIt is interesting to me that even though no Gentoo or Chinstrap penguins were found on Torgersen in the entire data set, there are still orange and green regions. Similarly, no Gentoos are found on Dream or Torgerson, but my model still has an orange decision region for both of those islands. This makes sense to me because, for instance, if the model found a penguin with a 230 mm flipper length and 60 mm culmen depth on Torgersen, it would be a reasonable prediction for that penguin to be a Gentoo, even if there are no other Gentoos on the island.\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test_scaled)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[22,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 19]])\n\n\nAbove is my models confusion matrix for the test set. The rows represent the actual species of a penguin while the columns represent the predicted species. As we can see, my model predicted all 22 Adelies to be Adelie, all 11 Chinstraps to be Chinstraps, and all 19 Gentoos to be gentoos. Therefore, my model correctly predicted the species of all 52 penguins in the test set with no errors.\n\n\nDiscussion\nThroughout this project, I gained valuable insights into the entire machine learning process. First, it was essential to prepare the data into training and test sections, encode qualitative regions into quantitave values, drop unessecary columns, and simplify column names. Then, the data visualizations helped me uncover trends in the dataset. This allowed me to get a foundational understanding of the variance between the species’ characteristics. A key takeaway was the importance of feature selection. I initially just analyzed my data visualizations and chose features I thought would be strong predictors. Even though this was somewhat accurate, when I automated my feature selection with a RandomForestClassifier and chose the most statistically important feautures, I was able to bring my model’s prediction success up to 100% for the test set. I also gained experience with model selection and scaling. After trying out a few options, I chose the Logistic Regression model because it was consistently high performing. During this process, I also realized that I had to standardize the numerical features to ensure consistent scaling. Perhaps the most interesting discovery of the entire process was plotting the decision regions to see where my model numerically distinguished between the species’ and to see the entire regions where a particular species would fall."
  },
  {
    "objectID": "posts/bias/bias.html",
    "href": "posts/bias/bias.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\nThis project investigates the use of machine learning to predict employment status based on demographic and socioeconomic variables from the U.S. Census PUMS data. Using the folktables library, we sourced data from the state of Maryland and preprocessed it to extract relevant features. A Random Forest model was trained to classify employment status, and its performance was evaluated on the general Maryland population. Then, a bias audit was conducted to assess potential disparities in model predictions across different racial and demographic groups. The results highlight both the predictive capabilities of the model and the ethical concerns related to bias in employment predictions.\n\n\nDownloading Data\nFirst we will download some PUMS data from the state of Maryland using folktables.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MD\"\n\ndata_source = ACSDataSource(survey_year='2023', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nSTATE\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2023GQ0000068\n5\n1\n201\n3\n24\n1019518\n27\n62\n...\n27\n29\n24\n25\n29\n27\n25\n29\n28\n28\n\n\n1\nP\n2023GQ0000079\n5\n1\n502\n3\n24\n1019518\n13\n21\n...\n13\n26\n17\n15\n13\n32\n0\n2\n2\n13\n\n\n2\nP\n2023GQ0000088\n5\n1\n1400\n3\n24\n1019518\n25\n35\n...\n32\n25\n70\n54\n42\n24\n2\n34\n49\n23\n\n\n3\nP\n2023GQ0000093\n5\n1\n1300\n3\n24\n1019518\n19\n61\n...\n14\n19\n23\n27\n2\n22\n27\n3\n20\n2\n\n\n4\nP\n2023GQ0000100\n5\n1\n802\n3\n24\n1019518\n106\n73\n...\n136\n206\n9\n15\n106\n209\n115\n210\n211\n15\n\n\n\n\n5 rows × 287 columns\n\n\n\n\n\nData Cleaning\nThe dataset has a lot of features. For our modeling task, we will only use the following possible features\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n62\n17.0\n5\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n1\n1.0\n1\n1\n6.0\n\n\n1\n21\n19.0\n5\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n2\n9\n6.0\n\n\n2\n35\n16.0\n5\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n1\n2\n6.0\n\n\n3\n61\n18.0\n3\n1\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n1.0\n1\n1\n6.0\n\n\n4\n73\n13.0\n5\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n\n\n\n\n\nFor documentation on what these features mean, please consult the appendix of this paper that introduced the package.\nI am going to train my model on all of the possible features aside from race, because we will be auditing for racial bias later. I will use these features to predict employment status (ESR), so that column of data will also be discluded from my training.\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\nNow we can construct a BasicProblem that expresses our wish to use these feautures to predict employment status (ESR), using race (RAC1P) as the group label.\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nBefore we touch the data anymore, we should perform a train-test split, training our model on 80% of the data, and testing on the remaining 20%.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nData Exploration\nNow we want to answer some basic questions about the data we are working with. We can answer these questions by turning our training data into a data frame for easy analysis.\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\ndf.head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\ngroup\nlabel\n\n\n\n\n0\n4.0\n1.0\n5.0\n2.0\n5.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n0.0\n1.0\n1\nFalse\n\n\n1\n2.0\n0.0\n5.0\n2.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n0.0\n1.0\n1\nFalse\n\n\n2\n49.0\n21.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1\nFalse\n\n\n3\n47.0\n15.0\n5.0\n1.0\n0.0\n3.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n1.0\n2.0\n1\nTrue\n\n\n4\n0.0\n0.0\n5.0\n2.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n2.0\n2.0\n0.0\n1.0\n1\nFalse\n\n\n\n\n\n\n\n\n1. How many individuals are in the data?\n\ndf.shape[0]\n\n49624\n\n\n\n\n2. Of these individuals, what proportion are employed?\n\ndf[\"label\"].mean()\n\n0.4898839271320329\n\n\n\n\n3. Of these individuals, how many are in each racial group?\n\n# Count the number of employed people in each racial group\ndf[df[\"label\"] == 1].groupby(\"group\")[\"label\"].count()\n\ngroup\n1    14026\n2     5131\n3       93\n4        1\n5       19\n6     2046\n7       12\n8     1247\n9     1735\nName: label, dtype: int64\n\n\nThe race groups are broken down as follows: – 1: White alone  – 2: Black or African American alone  – 3: American Indian alone – 4: Alaska Native alone – 5: American Indian and Alaska Native – 6: Asian alone – 7: Native Hawaiian and Other Pacific Islander alone – 8: Some Other Race alone – 9: Two or More Races\n\n\n4. In each group, what proportion of individuals have target label equal to 1, i.e. what proportion of people in each race is employed?\n\n# Compute the proportion of employed individuals in each racial group\ndf.groupby(\"group\")[\"label\"].mean()\n\ngroup\n1    0.491761\n2    0.478415\n3    0.547059\n4    0.333333\n5    0.372549\n6    0.544874\n7    0.666667\n8    0.513169\n9    0.439241\nName: label, dtype: float64\n\n\n\n\nNow I will look for some intersectional trends.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Create a DataFrame with the training features\ndf = pd.DataFrame(X_train, columns=features_to_use)\n\n# Add the race and employment status columns separately\ndf[\"Race\"] = group_train  # This is the group label provided separately\ndf[\"Employment Status\"] = y_train  # 1 = Employed, 0 = Not employed\n\n# Extract Sex feature\ndf[\"Sex\"] = X_train[:, features_to_use.index(\"SEX\")]\n# Replace numeric values with \"Male\" and \"Female\"\ndf[\"Sex\"] = df[\"Sex\"].replace({1: \"Male\", 2: \"Female\"})\n\n# Rename race labels dynamically\nunique_races = df[\"Race\"].unique()\nrace_labels = {race: f\"Race {race}\" for race in unique_races}\ndf[\"Race\"] = df[\"Race\"].replace(race_labels)\n\n# Compute the proportion of employed individuals by race and sex\nintersectional_proportions = df.groupby([\"Race\", \"Sex\"])[\"Employment Status\"].mean().reset_index()\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nax = sns.barplot(data=intersectional_proportions, x=\"Race\", y=\"Employment Status\", hue=\"Sex\")\n\n# Add numerical labels on top of each bar\nfor p in ax.patches:\n    if p.get_height() &gt; 0:  # Only annotate bars with nonzero height\n        ax.annotate(f'{p.get_height():.2f}',  # Format proportion to 2 decimal places\n                    (p.get_x() + p.get_width() / 2., p.get_height()), \n                    ha='center', va='bottom', fontsize=10, color='black')\n\n# Adjust plot aesthetics\nplt.title(\"Proportion of Employed Individuals by Sex and Race\")\nplt.ylabel(\"Proportion Employed\")\nplt.xlabel(\"Race\")\nplt.xticks(rotation=45)  # Rotate labels for better visibility\nplt.legend(title=\"Sex\")\nplt.ylim(0, 1)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nThis visualization gives us some valuable insights. The main outliers are only 30% of female Alaskan Natives (race 5) are employed and around 83% of male Pacific Islanders (race 7) are employed. Another thing to note is there are 0 employed male Alaskan natives (race 4), and this is because the training data set only includes one employed Alaskan native, and that person is female. Aside from those outliers, members from all racial groups have similar proportion of employed individuals (0.42-0.59), with males being employed at higher rates than women for every racial group besides Black people (race 2).\n\n\n\nTraining a Model\nThis Random Forest model undergoes hyperparameter tuning using GridSearchCV, where different values for max_depth (the maximum depth of each tree) are tested through 5-fold cross-validation to determine the best-performing configuration based on accuracy. Once the optimal max_depth is found, the best model is evaluated using cross-validation to estimate its generalization performance. Finally, the model is retrained on the full training dataset to maximize learning before deployment.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\n# Define the Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=0)\n\n# Define hyperparameters to tune\nparam_grid_rf = {'max_depth': [3, 5, 10, None]}  # Tune max depth\n\n# Perform Grid Search with Cross-Validation\ngrid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy')\ngrid_rf.fit(X_train, y_train)\n\n# Get the best model from Grid Search\nbest_rf_model = grid_rf.best_estimator_\n\n# Print the best max_depth value\nprint(\"Best max_depth for Random Forest:\", grid_rf.best_params_['max_depth'])\n\n# Evaluate the model with cross-validation\nrf_score = cross_val_score(best_rf_model, X_train, y_train, cv=5).mean()\nprint(f\"Cross-validation accuracy: {rf_score:.4f}\")\n\n# Train the final model on the full training set\nbest_rf_model.fit(X_train, y_train)\n\nBest max_depth for Random Forest: 10\nCross-validation accuracy: 0.8287\n\n\nRandomForestClassifier(max_depth=10, random_state=0)\n\n\nBased on our cross-validation results, the optimal accuracy of our model is achieved with a max_depth of 10.\nWith a max_depth of 10, our Random Forest model predicted employment with an accuracy of approximately 82% on the training data.\n\n\nTesting My Model\n\nOverall Accuracy and Precision\nFirst let’s see the overall accuracy of our model on the entire test set, without considering different racial groups.\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import confusion_matrix\n\n# Make predictions on the test set\ny_pred = best_rf_model.predict(X_test)\ndef compute_metrics(y_true, y_pred):\n    C = confusion_matrix(y_true, y_pred)\n\n    # Extract confusion matrix values manually\n    tn = C[0, 0] \n    fp = C[0, 1] \n    fn = C[1, 0] \n    tp = C[1, 1] \n\n    # Compute metrics\n    accuracy = (y_pred == y_true).mean()\n    ppv = tp / (tp + fp) \n    fnr = fn / (fn + tp) \n    fpr = fp / (fp + tn)\n\n    return pd.Series({\"Accuracy\": accuracy, \"PPV\": ppv, \"FNR\": fnr, \"FPR\": fpr})\n\n\ncompute_metrics(y_test, y_pred)\n\nAccuracy    0.826388\nPPV         0.792269\nFNR         0.121975\nFPR         0.223812\ndtype: float64\n\n\n\n\nTesting By Racial Group\nNow let’s check out some of the metrics split up by racial group.\n\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\n\n# Create DataFrame with predictions and actual values\ndf_results = pd.DataFrame({\"true_employment\": y_test, \"predicted_employment\": y_pred, \"race\": group_test})\n\n# Apply function to each subgroup\nresults_df = df_results.groupby(\"race\").apply(lambda g: compute_metrics(g[\"true_employment\"], g[\"predicted_employment\"]))\nresults_df\n\n\n\n\n\n\n\n\nAccuracy\nPPV\nFNR\nFPR\n\n\nrace\n\n\n\n\n\n\n\n\n1\n0.827839\n0.801902\n0.134550\n0.208914\n\n\n2\n0.814081\n0.770961\n0.131336\n0.235789\n\n\n3\n0.822222\n0.777778\n0.086957\n0.272727\n\n\n5\n0.850000\n0.700000\n0.000000\n0.230769\n\n\n6\n0.806999\n0.768116\n0.073786\n0.336449\n\n\n7\n0.818182\n0.750000\n0.250000\n0.142857\n\n\n8\n0.838879\n0.808260\n0.089701\n0.240741\n\n\n9\n0.860887\n0.803435\n0.076754\n0.192164\n\n\n\n\n\n\n\n\n\n\nAuditing For Racial Bias\nNow let’s analyze our data through the lens of various bias measures to see how it impacts different racial groups differently. To measure these biases, I will refer to the above results_df data frame that includes the model’s metrics divied up by racial group.\n\nCalibration\nNow I will preforming a check to see if my model is calibrated with respect to racial group. the code below calculates the actual true employment rate for each group, but only for cases where the model predicted employment. When I group by race and take the mean of Predicted Employment, it measures how often a positive prediction was actually correct within each racial group. This helps determine if the model’s positive predictions are equally reliable across racial groups.\n\nimport numpy as np\n\n# Compute actual employment rate when model predicts 1, by racial group\ncalibration_check = df_results[df_results[\"predicted_employment\"] == 1].groupby(\"race\")[\"true_employment\"].mean()\ncalibration_check\n\nrace\n1    0.801902\n2    0.770961\n3    0.777778\n5    0.700000\n6    0.768116\n7    0.750000\n8    0.808260\n9    0.803435\nName: true_employment, dtype: float64\n\n\nWhile the model seems to be generally well calibrated, there is an outlier for racial group 5 (Alaska Native), which has 0.05 lower calibration than the next lowest group. This is most likely due to this group having much less data than the other groups, as there were only 19 employed Alaskan natives in the data set. Aside from this group, the model is generally well calibrated as actual employment rates for individuals predicted as employed is fairly consistent, ranging from 0.75 to 0.80. While it is not perfectly calibrated because the values have variance, the level of consistency and a small range indicates that there is no severe calibration bias.\n\n\nError Rate Balance\nError rate balance requires that FPR and FNR be approximately equal across all groups. In my model, there is significant variation in both FNR and FPR across groups. FPR ranges from 0.143 to to 0.336 and FNR ranges between 0.000 and 0.25. This means that some groups are disproportionately classified as false positives or false negatives, and our model does not have balanced error-rates.\n\n\nStatistical Parity\nNow I will check for statistical parity, which is a fairness metric to determine whether different demographic groups receive favorable outcomes at the same rate. The code below checks the proportion of individuals predicted as employed (favorable outcome) within each group by grouping the dataset by Race and computing the mean of the predicted values. This tells us whether the model’s predictions are distributed equally across different groups.\n\n# Compute the proportion of individuals predicted as employed (y_hat == 1) per group\nstatistical_parity_check = df_results.groupby(\"Race\")[\"Predicted Employment\"].mean()\nstatistical_parity_check\n\nRace\n1    0.533390\n2    0.537954\n3    0.600000\n5    0.500000\n6    0.658537\n7    0.363636\n8    0.593695\n9    0.528226\nName: Predicted Employment, dtype: float64\n\n\nA model satisfies statistical parity if each racial group receives the positive prediction at the same rate. Looking at our values from above, the predicted employment rates range from 0.363 (group 7) to 0.658 (group 6), with significant variation among all racial groups. Thus, my model does not achieve statistical parity.\n\n\n\nFeasible FNR and FPR Rates\nAlexandra Chouldechova’s work on fair predictions provides us with the equation \\[\n  FPR = \\frac{1}{1-p}\\frac{1-PPV}{PPV}(1-FNR)\n\\]\nNow I will calculate feasible false positive rates for each group using the above equation by factoring in the prevelance (p). The code computes the prevalence for each group by averaging the true employment rates and then merges this dataframe with our previous results from above. Then, using the equation from above, it calculates a feasible FPR for each group.\n\nimport pandas as pd\n\n# Assuming results_df already contains 'group', 'PPV', 'FNR', and 'FPR'\n# Compute prevalence (p) for each group using y_test from df_results (individual-level data)\nprevalence = df_results.groupby(\"race\")[\"true_employment\"].mean().reset_index()\nprevalence.columns = [\"race\", \"Prevalence\"]\n\n# Merge prevalence into results_df\nresults_df = results_df.merge(prevalence, on=\"race\", how=\"left\")\n\n# Compute Feasible FPR using Equation (2.6)\nresults_df[\"Feasible_FPR\"] = (results_df[\"Prevalence\"] / (1 - results_df[\"Prevalence\"])) * \\\n                             ((1 - results_df[\"PPV\"]) / results_df[\"PPV\"]) * \\\n                             (1 - results_df[\"FNR\"])\n\nresults_df\n\n\n\n\n\n\n\n\nrace\nAccuracy\nPPV\nFNR\nFPR\nPrevalence\nFeasible_FPR\n\n\n\n\n0\n1\n0.827839\n0.801902\n0.134550\n0.208914\n0.494224\n0.208914\n\n\n1\n2\n0.814081\n0.770961\n0.131336\n0.235789\n0.477448\n0.235789\n\n\n2\n3\n0.822222\n0.777778\n0.086957\n0.272727\n0.511111\n0.272727\n\n\n3\n5\n0.850000\n0.700000\n0.000000\n0.230769\n0.350000\n0.230769\n\n\n4\n6\n0.806999\n0.768116\n0.073786\n0.336449\n0.546129\n0.336449\n\n\n5\n7\n0.818182\n0.750000\n0.250000\n0.142857\n0.363636\n0.142857\n\n\n6\n8\n0.838879\n0.808260\n0.089701\n0.240741\n0.527145\n0.240741\n\n\n7\n9\n0.860887\n0.803435\n0.076754\n0.192164\n0.459677\n0.192164\n\n\n\n\n\n\n\nNow I will recreate Chouldechova’s figure 5 from the reading by plotting the observed false negative rates (FNR) and false positive rates (FPR) for each group with the theoretically feasible FNR-FPR tradeoff lines derived from Equation 2.6. The tradeoff lines are calculated using the prevalence-adjusted feasible FPR values, showing the relationship between error rates across groups.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Extract observed values from results_df\ngroups = results_df[\"race\"]\nFNR = results_df[\"FNR\"]\nFPR = results_df[\"FPR\"]\nFeasible_FPR = results_df[\"Feasible_FPR\"]  # Precomputed from the equation\n\n# Define colors for each group\ncolors = plt.cm.tab10(np.linspace(0, 1, len(groups)))  # Using tab10 colormap\n\n# Generate feasible FNR-FPR tradeoff line using equation\nfnr_range = np.linspace(0, 1, 100)\n\n# Compute target FPR (equalizing across all groups)\ntarget_fpr = np.mean(FPR)\n\n# Define prevalence (p) and positive predictive value (PPV) - Assumed fixed\np = 0.5  # Example prevalence\nppv = 0.6  # Example PPV\n\n# Compute required FNR adjustments using Equation (2.6)\nRequired_FNR_Adjustment = 1 - (target_fpr * (1 - p) * ppv) / (p * (1 - ppv)) - FNR\n\n# Store adjustments back into results_df\nresults_df[\"Required_FNR_Adjustment\"] = Required_FNR_Adjustment\n\n# Create figure\nplt.figure(figsize=(7, 5))\n\n# Plot feasible tradeoff lines and corresponding observed points\nfor i, group in enumerate(groups):\n    color = colors[i]  # Assign color to group\n    plt.plot(fnr_range, Feasible_FPR[i] * (1 - fnr_range), color=color, label=f\"Feasible for Group {group}\")\n    plt.scatter(FNR[i], FPR[i], color=color, edgecolors=\"black\", s=50, zorder=3)  # Matching color with a black outline\n\n# Labels and styling\nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible (FNR, FPR) Combinations with Prevalence Constraint\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nresults_df[\"Required_FNR_Adjustment\"]\n\n0    0.516623\n1    0.519837\n2    0.564217\n3    0.651173\n4    0.577387\n5    0.401173\n6    0.561472\n7    0.574419\nName: Required_FNR_Adjustment, dtype: float64\n\n\nBy analyzing the relationship between false negative rates (FNR) and false positive rates (FPR) through the lens of equation (2.6), we gain a clearer picture of how the model’s predictive fairness varies across different groups. The plot juxtaposes actual FNR-FPR values with theoretically feasible ones under fixed prevalence and positive predictive value (PPV), revealing where disparities exist. While some groups align closely with the expected trade-offs, others display noticeable deviations, indicating imbalances in error distribution. Achieving fairness by equalizing FPR across all groups requires altering FNR values, as indicated by the Required_FNR_Adjustment metric. The degree of necessary modification varies, with certain groups needing only minor shifts while others require substantial recalibration. For instance, Group 3 exhibits the most significant adjustment, suggesting that its current FNR is considerably lower than what would be necessary for parity. Conversely, Group 5 requires minimal changes, implying it is already near the target threshold. These discrepancies highlight the challenge of enforcing fairness constraints while preserving model accuracy.\n\n\nConcluding Discussion\nThe ability to predict employment status based on demographic features presents significant opportunities for companies and government agencies seeking to improve hiring decisions, workforce planning, and policy implementation. Businesses in recruitment, human resources, and economic forecasting could leverage such a model to streamline hiring processes or assess labor market trends. However, the deployment of such a system requires careful consideration of ethical and fairness concerns.\nThe bias audit conducted in this project revealed disparities in the model’s predictions across racial groups, suggesting that certain demographic segments might be disproportionately misclassified. This raises concerns about the potential reinforcement of existing societal inequalities if the model is applied at scale without proper fairness interventions. In this model there were calibration disparities, higher error rates for certain groups, and a lack of statistical parity across groups.\nBeyond bias, additional concerns arise when deploying predictive models in high-stakes scenarios. The risk of automation bias, where decision-makers over-rely on model predictions without critical assessment, can exacerbate discriminatory outcomes. Additionally, the model’s reliance on historical data means that it may inherit and perpetuate past biases embedded in employment practices.\nTo address these challenges, future improvements should include fairness-aware training techniques, such as reweighting methods, or post-processing corrections. Moreover, continuous monitoring and auditing should be implemented to assess model performance across demographic groups regularly. Transparent reporting and explainability techniques should also be adopted to ensure that stakeholders understand the basis of predictions and can intervene where necessary.\nUltimately, while machine learning models offer powerful tools for employment status prediction, their real-world application must be guided by ethical considerations and fairness audits, to prevent harmful unintended consequences."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Evan Flaks’ Machine Learning Blog",
    "section": "",
    "text": "Sparse Kernelized Logistic Regression\n\n\n\n\n\nImplementing Kernelized Logistic Regression and Performing Various Data Experiments\n\n\n\n\n\nApr 16, 2025\n\n\nEvan Flaks\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nImplementing Gradient Descent for Logistic Regression and Performing Data Experiments with Varying Momentum\n\n\n\n\n\nApr 8, 2025\n\n\nEvan Flaks\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nImplementating the perceptron algorithm and testing it in several experiments.\n\n\n\n\n\nMar 23, 2025\n\n\nEvan Flaks\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nBuilding a model that predicts employment status and auditing the model’s racial bias\n\n\n\n\n\nMar 9, 2025\n\n\nEvan Flaks\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nBuilding and analyzing an automated system for a bank extending credit\n\n\n\n\n\nMar 5, 2025\n\n\nEvan Flaks\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Classification\n\n\n\n\n\nMachine Learning model to predict penguin species\n\n\n\n\n\nFeb 18, 2025\n\n\nEvan Flaks\n\n\n\n\n\n\nNo matching items"
  }
]