[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Evan Flaks and this is my Machine Learning blog."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/penguins/penguins.html",
    "href": "posts/penguins/penguins.html",
    "title": "Palmer Penguins Classification",
    "section": "",
    "text": "Abstract\nThis goal of this project was to develop a machine learning model to classify penguin species based on quantitative and qualititave characteristics using the Palmer Penguins dataset. After accessing the data and splitting it up into training and testing sets, I created visualizations on the training data to better understand which species exhibited particular traits. Then, I used a feature selection system to determine which features (two quantitative and one qualitiative) would be the best predictor of a penguin’s species. From there, I used these features to train a logistic regression model. The model’s performance is assessed using training and test accuracy and decision region visualizations. The goal of this assignment was to take a penguin with unknown species, analyze three of its given features, and with 100% accuracy, predict its species.\n\n\nData Preparation\nFirst I must access and read the data from the source.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nNow, I must prepare the dataset for machine learning by encoding the target variable (Species) into numerical values, dropping unnecessary columns, removing missing values and invalid entries, converting categorical columns into numerical representations via one-hot encoding, and splitting up training and testing sets.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\n# Initialize LabelEncoder for the target variable\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n# Function to preprocess data\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]  # Remove rows with invalid 'Sex' values\n    df = df.dropna()  # Remove any remaining missing values\n    y = le.transform(df[\"Species\"])  # Encode the target variable\n    df = df.drop([\"Species\"], axis=1)  # Remove the target column from features\n    df = pd.get_dummies(df)  # One-hot encode categorical features\n    return df, y\n\n# Prepare the dataset\nX, y = prepare_data(train)\n\n# Split the dataset into 80% training and 20% testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Convert training data back into a DataFrame for visualization purposes\ntrain_data = X_train.copy()\ntrain_data[\"Species\"] = le.inverse_transform(y_train)  # Convert encoded labels back to species names\n\n# Reconstruct the 'Island' column from one-hot encoded values\nisland_columns = ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\ntrain_data[\"Island\"] = train_data[island_columns].idxmax(axis=1).str.replace(\"Island_\", \"\")\n\n# Check the sizes of the splits\nprint(f\"Training Set Size: {X_train.shape[0]}\")\nprint(f\"Test Set Size: {X_test.shape[0]}\")\n\nTraining Set Size: 204\nTest Set Size: 52\n\n\n\n\nVisualizations\nNow, to visualize the dataset, I have created a scatter plot that plots flipper length vs. body mass of the three species and a bar chart showing the distribution of Penguins found on the three different islands. Finally, I created a summary table that shows each species’ average Culmen Length and Culmen Depth.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=train_data, x='Flipper Length (mm)', y='Body Mass (g)', hue='Species', style='Species')\n\n# Labels and title\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Body Mass (g)')\nplt.title('Flipper Length vs. Body Mass by Species (Training Data Only)')\nplt.legend(title='Species')\nplt.show()\n\n\n\n\n\n\n\n\nAbove, I have created a scatter plot that plots the flipper length and body mass of each penguin observed in the training set. I created a key with different symbols to plot each species so I could visualize the physical differences between each species. This visualization gave me valuable insight, informing me that Gentoo penguins are much larger in terms of body mass and flipper length than both Adelie and Chinstrap penguins. Adelie and Chinstrap penguins, as represented by the blue circles and green squares, share very similar sizes in these metrics. From this, we can conclude that body mass and flipper length would be excellent features with which to train our model in order to distinguish between Gentoo or not-gentoo, but these features would not help our model distinguish between Adelie and Chinstrap.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create a count plot for species distribution by island (training data only)\nplt.figure(figsize=(8, 6))\nsns.countplot(data=train_data, x='Island', hue='Species')\n\n# Labels and title\nplt.xlabel('Island')\nplt.ylabel('Count')\nplt.title('Penguin Species Distribution by Island (Training Data Only)')\nplt.legend(title='Species')\nplt.show()\n\n\n\n\n\n\n\n\nFor this visualization, I created a bar chart to see the island where each observed penguin was found. Here, we can see that Gentoo penguins are only found on Biscoe Island and Chinstrap penguins are only found on Dream Island. Adelie penguins, on the other hand, are found on all three islands. This bar chart tells me that Island location could help train my model to distinguish between Adelie and Chinstrap because Chinstrap penguins seem to only be found on Dream Island. So, if we see a penguin with quantitaive measurables that could fall under Adelie or Chinstrap, but the penguin was found on Torgersen Island, the model would correctly predict that the penguin is Adelie. This is especially helpful considering the quantitative features observed in my first visualization had a lot of overlap between Adelie and Chinstrap.\n\nimport pandas as pd\n\n# Compute the average Culmen Length & Depth in the training set only\nsummary_table = train_data.groupby(\"Species\")[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].mean()\n\n# Display the summary table\nprint(summary_table)\n\n\n                                           Culmen Length (mm)  \\\nSpecies                                                         \nAdelie Penguin (Pygoscelis adeliae)                 39.048837   \nChinstrap penguin (Pygoscelis antarctica)           48.697778   \nGentoo penguin (Pygoscelis papua)                   47.031507   \n\n                                           Culmen Depth (mm)  \nSpecies                                                       \nAdelie Penguin (Pygoscelis adeliae)                18.412791  \nChinstrap penguin (Pygoscelis antarctica)          18.406667  \nGentoo penguin (Pygoscelis papua)                  14.957534  \n\n\nHere, I created a table to see the average culmen length and culmen depth of each penguin species from the data set. From this, we can see that culmen length would be a great feature to distinguish Adelie penguins from the other two species because they have considerably shorter culmens. However, culmen length would not be very helpful in distinguishing Gentoo from Chinstrap because they have very similar measurements. As for culmen depth, we see that Adelie and Chinstrap have very similar average measurements while Gentoo penguins have considerably smaller culmen depths. This means culmen depth would be a good feature for my model to distinguish Gentoo penguins from the other two species.\n\n\nData Visualization Conclusions\nAfter creating three data visualizations, I have intuition for some of the features that would help train a successful model. From my scatter plot, I saw that body mass and flipper length would both be great features to help my model distinguish between Gentoo and Adelie or Chinstrap, but would not be very helpful in distinguishing Adelie from Chinstrap. Then, from the summary table, we could see that Adelie penguins, on average, have around a 20% shorter culmen length than Chinstrap penguins, so that would be a great feature to help my model distinguish between those two species. Finally, since Chinstrap penguins were only found on Dream Island and Gentoo Penguins were only found on Biscoe Island as represented by my bar chart, I figured this would be a nice qualitative feature with which to train my model.\n\n\nFeature Selection\nNow we must choose which three features we want to use to predict the penguin species. I gained some valuable insight from my visualizations but now want to use a systematic approach to select the three best features. To do this, I used a Random Forest Classifier by measuring how much each feature contributes to making accurate predictions. When splitting a node in a decision tree, features that provide better separation between classes are preferred. The model tracks how often a feature is used in important splits and how much it improves classification accuracy. The importance of each feature is calculated as the total reduction in prediction error (impurity) it provides across all trees. Features with higher scores contribute more to the model’s decision-making.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train a random forest to assess feature importance\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Get feature importance scores\nfeature_importance = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': model.feature_importances_\n})\n\n# Sort by importance\nfeature_importance = feature_importance.sort_values(by='Importance', ascending=False)\nprint(feature_importance)\n\n                     Feature  Importance\n0         Culmen Length (mm)    0.234636\n2        Flipper Length (mm)    0.179364\n5          Delta 13 C (o/oo)    0.161621\n1          Culmen Depth (mm)    0.148071\n4          Delta 15 N (o/oo)    0.075136\n7               Island_Dream    0.063007\n6              Island_Biscoe    0.059049\n3              Body Mass (g)    0.058736\n8           Island_Torgersen    0.012969\n10      Clutch Completion_No    0.002622\n12                Sex_FEMALE    0.002250\n13                  Sex_MALE    0.001822\n11     Clutch Completion_Yes    0.000717\n9   Stage_Adult, 1 Egg Stage    0.000000\n\n\nFrom this, we can conclude that the best qualitative category is Island and the best quantitative categories are Culmen Length and Flipper Length. So, these are the features we will train our model on.\n\n\nLogistic Regression Model Testing\nThe model below uses logistic regression to classify data based on flipper length, culmen length, and island. First, it standardizes the numerical features using StandardScaler() to ensure consistent scaling. Then, it trains a logistic regression model with increased iterations (1000) to ensure convergence. Finally, it evaluates performance using training and test accuracy, helping assess how well the model generalizes.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# Define selected features\ncols = ['Flipper Length (mm)', 'Culmen Length (mm)','Island_Biscoe', 'Island_Dream', 'Island_Torgersen',]\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[cols]), columns=cols)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test[cols]), columns=cols)\n\n\n# Initialize and train logistic regression with increased iterations\nLR = LogisticRegression(max_iter=1000)  # Increased max_iter to ensure convergence\nLR.fit(X_train_scaled, y_train)\n\n# Evaluate model\nprint(\"Training Accuracy:\", LR.score(X_train_scaled, y_train))\nprint(\"Test Accuracy:\", LR.score(X_test_scaled, y_test))\n\n\n\nTraining Accuracy: 0.9754901960784313\nTest Accuracy: 1.0\n\n\n\n\nPlotting Decision Regions\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, scaler):\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(7, 3))\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n\n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({X.columns[0]: XX, X.columns[1]: YY})\n\n        # Initialize categorical features as zeros\n        for j in qual_features:\n            XY[j] = 0\n\n        # Set the specific category feature to 1\n        XY[qual_features[i]] = 1\n\n        # Standardize XY to match model input\n        XY_scaled = pd.DataFrame(scaler.transform(XY), columns=X.columns)\n\n        # Predict decision boundary\n        p = model.predict(XY_scaled)\n        p = p.reshape(xx.shape)\n\n        # Use contour plot to visualize the predictions\n        decision_cmap = ListedColormap([\"blue\", \"green\", \"orange\"])  # Match species colors\n        axarr[i].contourf(xx, yy, p, cmap=decision_cmap, alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n\n        # Plot the actual training data points\n        species_cmap = ListedColormap([\"blue\", \"green\", \"orange\"])  # blue = Adelie, Green = Chinstrap, orange = Gentoo\n        axarr[i].scatter(x0[ix], x1[ix], c=y[ix], cmap=species_cmap, vmin=0, vmax=2)\n\n        axarr[i].set(\n            xlabel=X.columns[0],\n            ylabel=X.columns[1],\n            title=qual_features[i]\n        )\n\n        patches = []\n        for color, spec in zip([\"blue\", \"green\", \"orange\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color=color, label=spec))\n\n        axarr[i].legend(handles=patches, title=\"Species\", loc=\"best\")\n\n    plt.tight_layout()\n\n# Call the function with scaler applied\nplot_regions(LR, X_train[cols], y_train, scaler)\nplot_regions(LR, X_test[cols], y_test, scaler)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbove, I have plotted my model’s decision regions on both the training and test sets, showing the thresholds of prediction for each species split up by each island.\nFor both the training and test set of penguins on Torgersen Island, my model predicted with 100% success as there were no other penguin found on this island besides Adelies.\nDream Island contained both Adelie and Chinstrap penguins, which both have very similar flipper length, so the decision regions for that island are based almost entirely by culmen length, as can be seen by the near-horizontal divide between the green and blue regions. My model did have some error on the training set for Dream Island as there were some outliers of the decision regions – Chinstraps with below average culmen lengths and Adelies with above average culmen lengths. The test set did not have as many outliers and therefore still predicted with 100% accuracy on Dream Island.\nAs for Biscoe Island, there were both Gentoo and Adelie penguins. As observed in my data visualizations, Gentoo penguins are considerably larger than Adelie in both flipper length and culmen length so the regions are divided diagonally. There were no overlaps in the decision regions on Biscoe in either the test or training set.\nIt is interesting to me that even though no Gentoo or Chinstrap penguins were found on Torgersen in the entire data set, there are still orange and green regions. Similarly, no Gentoos are found on Dream or Torgerson, but my model still has an orange decision region for both of those islands. This makes sense to me because, for instance, if the model found a penguin with a 230 mm flipper length and 60 mm culmen depth on Torgersen, it would be a reasonable prediction for that penguin to be a Gentoo, even if there are no other Gentoos on the island.\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test_scaled)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[22,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 19]])\n\n\nAbove is my models confusion matrix for the test set. The rows represent the actual species of a penguin while the columns represent the predicted species. As we can see, my model predicted all 22 Adelies to be Adelie, all 11 Chinstraps to be Chinstraps, and all 19 Gentoos to be gentoos. Therefore, my model correctly predicted the species of all 52 penguins in the test set with no errors.\n\n\nDiscussion\nThroughout this project, I gained valuable insights into the entire machine learning process. First, it was essential to prepare the data into training and test sections, encode qualitative regions into quantitave values, drop unessecary columns, and simplify column names. Then, the data visualizations helped me uncover trends in the dataset. This allowed me to get a foundational understanding of the variance between the species’ characteristics. A key takeaway was the importance of feature selection. I initially just analyzed my data visualizations and chose features I thought would be strong predictors. Even though this was somewhat accurate, when I automated my feature selection with a RandomForestClassifier and chose the most statistically important feautures, I was able to bring my model’s prediction success up to 100% for the test set. I also gained experience with model selection and scaling. After trying out a few options, I chose the Logistic Regression model because it was consistently high performing. During this process, I also realized that I had to standardize the numerical features to ensure consistent scaling. Perhaps the most interesting discovery of the entire process was plotting the decision regions to see where my model numerically distinguished between the species’ and to see the entire regions where a particular species would fall."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Evan Flaks’ Machine Learning Blog",
    "section": "",
    "text": "Palmer Penguins Classification\n\n\n\n\n\nMachine Learning model to predict penguin species\n\n\n\n\n\nFeb 18, 2024\n\n\nEvan Flaks\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]